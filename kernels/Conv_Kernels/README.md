# Tested Convolution Kernels in Triton
Unfortunately, doesn't seem like any of these kernels are able to hold up to native torch performace, trident and implicit gemm kernels are ~40x slower at sdxl conv sizes and torch inductor kernel doesn't seem to even run. This is the lowest hanging fruit for this repo since there are a fair amount of convolutions in the U-Net. If you want to push for this as well please lend your support to this issue (https://github.com/openai/triton/issues/2380) on the triton repo to let Phil Tillet and other maintainers know that this is a high priority issue. Please be respectful and understanding to them since they are extremely dedicated to this project and it is a great help to the OS community