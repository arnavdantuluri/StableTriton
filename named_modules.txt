UNet2DConditionModel(
  (time_embedding): Module(
    (linear_1): Linear(in_features=320, out_features=1280, bias=True)
    (act): SiLU()
    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (add_embedding): Module(
    (linear_1): Linear(in_features=2816, out_features=1280, bias=True)
    (act): SiLU()
    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (down_blocks): Module(
    (0): Module(
      (resnets): Module(
        (0): Module(
          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (1): Module(
          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (downsamplers): Module(
        (0): Module(
          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (1): Module(
      (resnets): Module(
        (0): Module(
          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Module(
          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (attentions): Module(
        (0): Module(
          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=640, out_features=640, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=640, out_features=640, bias=True)
        )
        (1): Module(
          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=640, out_features=640, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=640, out_features=640, bias=True)
        )
      )
      (downsamplers): Module(
        (0): Module(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
      )
    )
    (2): Module(
      (resnets): Module(
        (0): Module(
          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Module(
          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (attentions): Module(
        (0): Module(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (2): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (3): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (4): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (5): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (6): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (7): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (8): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (9): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (1): Module(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (2): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (3): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (4): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (5): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (6): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (7): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (8): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (9): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
      )
    )
  )
  (mid_block): Module(
    (resnets): Module(
      (0): Module(
        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): Module(
        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (attentions): Module(
      (0): Module(
        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (2): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (3): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (4): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (5): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (6): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (7): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (8): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (9): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
      )
    )
  )
  (up_blocks): Module(
    (0): Module(
      (resnets): Module(
        (0): Module(
          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Module(
          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Module(
          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attentions): Module(
        (0): Module(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (2): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (3): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (4): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (5): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (6): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (7): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (8): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (9): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (1): Module(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (2): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (3): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (4): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (5): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (6): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (7): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (8): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (9): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (2): Module(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (2): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (3): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (4): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (5): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (6): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (7): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (8): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
            (9): Module(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=2048, out_features=1280, bias=False)
                (to_v): Linear(in_features=2048, out_features=1280, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
        )
      )
      (upsamplers): Module(
        (0): Module(
          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (1): Module(
      (resnets): Module(
        (0): Module(
          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Module(
          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Module(
          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attentions): Module(
        (0): Module(
          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=640, out_features=640, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=640, out_features=640, bias=True)
        )
        (1): Module(
          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=640, out_features=640, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=640, out_features=640, bias=True)
        )
        (2): Module(
          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
          (proj_in): Linear(in_features=640, out_features=640, bias=True)
          (transformer_blocks): Module(
            (0): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
            (1): Module(
              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn1): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=640, out_features=640, bias=False)
                (to_v): Linear(in_features=640, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (attn2): Module(
                (to_q): Linear(in_features=640, out_features=640, bias=False)
                (to_k): Linear(in_features=2048, out_features=640, bias=False)
                (to_v): Linear(in_features=2048, out_features=640, bias=False)
                (to_out): Module(
                  (0): Linear(in_features=640, out_features=640, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
              (ff): Module(
                (net): Module(
                  (0): Module(
                    (proj): Linear(in_features=640, out_features=5120, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): Linear(in_features=2560, out_features=640, bias=True)
                )
              )
            )
          )
          (proj_out): Linear(in_features=640, out_features=640, bias=True)
        )
      )
      (upsamplers): Module(
        (0): Module(
          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (2): Module(
      (resnets): Module(
        (0): Module(
          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Module(
          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Module(
          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
          (nonlinearity): SiLU()
          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)
  (conv_act): SiLU()
  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)



def forward(self, sample, timesteps, encoder_hidden_states, added_cond_kwargs, **kwargs):
    _kwargs = kwargs
    getattr_1 = sample.shape
    getitem = getattr_1[0];  getattr_1 = None
    expand = timesteps.expand(getitem);  timesteps = getitem = None
    getattr_2 = expand.shape
    _tensor_constant0 = self._tensor_constant0
    getattr_3 = expand.device
    to = _tensor_constant0.to(getattr_3);  _tensor_constant0 = getattr_3 = None
    mul = -9.210340371976184 * to;  to = None
    truediv = mul / 160.0;  mul = None
    exp = torch.exp(truediv);  truediv = None
    getitem_1 = expand[(slice(None, None, None), None)];  expand = None
    float_1 = getitem_1.float();  getitem_1 = None
    getitem_2 = exp[(None, slice(None, None, None))];  exp = None
    mul_1 = float_1 * getitem_2;  float_1 = getitem_2 = None
    sin = torch.sin(mul_1)
    cos = torch.cos(mul_1);  mul_1 = None
    cat = torch.cat([cos, sin], dim = -1);  cos = sin = None
    getattr_4 = sample.dtype
    to_1 = cat.to(dtype = getattr_4);  cat = getattr_4 = None
    time_embedding_linear_1 = self.time_embedding.linear_1(to_1);  to_1 = None
    time_embedding_act = self.time_embedding.act(time_embedding_linear_1);  time_embedding_linear_1 = None
    time_embedding_linear_2 = self.time_embedding.linear_2(time_embedding_act);  time_embedding_act = None
    get = added_cond_kwargs.get('text_embeds')
    get_1 = added_cond_kwargs.get('time_ids');  added_cond_kwargs = None
    flatten = get_1.flatten();  get_1 = None
    getattr_5 = flatten.shape
    _tensor_constant1 = self._tensor_constant1
    getattr_6 = flatten.device
    to_2 = _tensor_constant1.to(getattr_6);  _tensor_constant1 = getattr_6 = None
    mul_2 = -9.210340371976184 * to_2;  to_2 = None
    truediv_1 = mul_2 / 128.0;  mul_2 = None
    exp_1 = torch.exp(truediv_1);  truediv_1 = None
    getitem_3 = flatten[(slice(None, None, None), None)];  flatten = None
    float_2 = getitem_3.float();  getitem_3 = None
    getitem_4 = exp_1[(None, slice(None, None, None))];  exp_1 = None
    mul_3 = float_2 * getitem_4;  float_2 = getitem_4 = None
    sin_1 = torch.sin(mul_3)
    cos_1 = torch.cos(mul_3);  mul_3 = None
    cat_1 = torch.cat([cos_1, sin_1], dim = -1);  cos_1 = sin_1 = None
    getattr_7 = get.shape
    getitem_5 = getattr_7[0];  getattr_7 = None
    reshape = cat_1.reshape((getitem_5, -1));  cat_1 = getitem_5 = None
    concat = torch.concat([get, reshape], dim = -1);  get = reshape = None
    getattr_8 = time_embedding_linear_2.dtype
    to_3 = concat.to(getattr_8);  concat = getattr_8 = None
    add_embedding_linear_1 = self.add_embedding.linear_1(to_3);  to_3 = None
    add_embedding_act = self.add_embedding.act(add_embedding_linear_1);  add_embedding_linear_1 = None
    add_embedding_linear_2 = self.add_embedding.linear_2(add_embedding_act);  add_embedding_act = None
    add = time_embedding_linear_2 + add_embedding_linear_2;  time_embedding_linear_2 = add_embedding_linear_2 = None
    conv_in = self.conv_in(sample);  sample = None
    down_blocks_0_resnets_0_norm1 = getattr(getattr(self.down_blocks, "0").resnets, "0").norm1(conv_in)
    down_blocks_0_resnets_0_nonlinearity = getattr(getattr(self.down_blocks, "0").resnets, "0").nonlinearity(down_blocks_0_resnets_0_norm1);  down_blocks_0_resnets_0_norm1 = None
    down_blocks_0_resnets_0_conv1 = getattr(getattr(self.down_blocks, "0").resnets, "0").conv1(down_blocks_0_resnets_0_nonlinearity);  down_blocks_0_resnets_0_nonlinearity = None
    down_blocks_0_resnets_0_nonlinearity_1 = getattr(getattr(self.down_blocks, "0").resnets, "0").nonlinearity(add)
    down_blocks_0_resnets_0_time_emb_proj = getattr(getattr(self.down_blocks, "0").resnets, "0").time_emb_proj(down_blocks_0_resnets_0_nonlinearity_1);  down_blocks_0_resnets_0_nonlinearity_1 = None
    getitem_6 = down_blocks_0_resnets_0_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  down_blocks_0_resnets_0_time_emb_proj = None
    add_1 = down_blocks_0_resnets_0_conv1 + getitem_6;  down_blocks_0_resnets_0_conv1 = getitem_6 = None
    down_blocks_0_resnets_0_norm2 = getattr(getattr(self.down_blocks, "0").resnets, "0").norm2(add_1);  add_1 = None
    down_blocks_0_resnets_0_nonlinearity_2 = getattr(getattr(self.down_blocks, "0").resnets, "0").nonlinearity(down_blocks_0_resnets_0_norm2);  down_blocks_0_resnets_0_norm2 = None
    down_blocks_0_resnets_0_dropout = getattr(getattr(self.down_blocks, "0").resnets, "0").dropout(down_blocks_0_resnets_0_nonlinearity_2);  down_blocks_0_resnets_0_nonlinearity_2 = None
    down_blocks_0_resnets_0_conv2 = getattr(getattr(self.down_blocks, "0").resnets, "0").conv2(down_blocks_0_resnets_0_dropout);  down_blocks_0_resnets_0_dropout = None
    add_2 = conv_in + down_blocks_0_resnets_0_conv2;  down_blocks_0_resnets_0_conv2 = None
    down_blocks_0_resnets_1_norm1 = getattr(getattr(self.down_blocks, "0").resnets, "1").norm1(add_2)
    down_blocks_0_resnets_1_nonlinearity = getattr(getattr(self.down_blocks, "0").resnets, "1").nonlinearity(down_blocks_0_resnets_1_norm1);  down_blocks_0_resnets_1_norm1 = None
    down_blocks_0_resnets_1_conv1 = getattr(getattr(self.down_blocks, "0").resnets, "1").conv1(down_blocks_0_resnets_1_nonlinearity);  down_blocks_0_resnets_1_nonlinearity = None
    down_blocks_0_resnets_1_nonlinearity_1 = getattr(getattr(self.down_blocks, "0").resnets, "1").nonlinearity(add)
    down_blocks_0_resnets_1_time_emb_proj = getattr(getattr(self.down_blocks, "0").resnets, "1").time_emb_proj(down_blocks_0_resnets_1_nonlinearity_1);  down_blocks_0_resnets_1_nonlinearity_1 = None
    getitem_7 = down_blocks_0_resnets_1_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  down_blocks_0_resnets_1_time_emb_proj = None
    add_3 = down_blocks_0_resnets_1_conv1 + getitem_7;  down_blocks_0_resnets_1_conv1 = getitem_7 = None
    down_blocks_0_resnets_1_norm2 = getattr(getattr(self.down_blocks, "0").resnets, "1").norm2(add_3);  add_3 = None
    down_blocks_0_resnets_1_nonlinearity_2 = getattr(getattr(self.down_blocks, "0").resnets, "1").nonlinearity(down_blocks_0_resnets_1_norm2);  down_blocks_0_resnets_1_norm2 = None
    down_blocks_0_resnets_1_dropout = getattr(getattr(self.down_blocks, "0").resnets, "1").dropout(down_blocks_0_resnets_1_nonlinearity_2);  down_blocks_0_resnets_1_nonlinearity_2 = None
    down_blocks_0_resnets_1_conv2 = getattr(getattr(self.down_blocks, "0").resnets, "1").conv2(down_blocks_0_resnets_1_dropout);  down_blocks_0_resnets_1_dropout = None
    add_4 = add_2 + down_blocks_0_resnets_1_conv2;  down_blocks_0_resnets_1_conv2 = None
    down_blocks_0_downsamplers_0_conv = getattr(getattr(self.down_blocks, "0").downsamplers, "0").conv(add_4)
    down_blocks_1_resnets_0_norm1 = getattr(getattr(self.down_blocks, "1").resnets, "0").norm1(down_blocks_0_downsamplers_0_conv)
    down_blocks_1_resnets_0_nonlinearity = getattr(getattr(self.down_blocks, "1").resnets, "0").nonlinearity(down_blocks_1_resnets_0_norm1);  down_blocks_1_resnets_0_norm1 = None
    down_blocks_1_resnets_0_conv1 = getattr(getattr(self.down_blocks, "1").resnets, "0").conv1(down_blocks_1_resnets_0_nonlinearity);  down_blocks_1_resnets_0_nonlinearity = None
    down_blocks_1_resnets_0_nonlinearity_1 = getattr(getattr(self.down_blocks, "1").resnets, "0").nonlinearity(add)
    down_blocks_1_resnets_0_time_emb_proj = getattr(getattr(self.down_blocks, "1").resnets, "0").time_emb_proj(down_blocks_1_resnets_0_nonlinearity_1);  down_blocks_1_resnets_0_nonlinearity_1 = None
    getitem_8 = down_blocks_1_resnets_0_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  down_blocks_1_resnets_0_time_emb_proj = None
    add_5 = down_blocks_1_resnets_0_conv1 + getitem_8;  down_blocks_1_resnets_0_conv1 = getitem_8 = None
    down_blocks_1_resnets_0_norm2 = getattr(getattr(self.down_blocks, "1").resnets, "0").norm2(add_5);  add_5 = None
    down_blocks_1_resnets_0_nonlinearity_2 = getattr(getattr(self.down_blocks, "1").resnets, "0").nonlinearity(down_blocks_1_resnets_0_norm2);  down_blocks_1_resnets_0_norm2 = None
    down_blocks_1_resnets_0_dropout = getattr(getattr(self.down_blocks, "1").resnets, "0").dropout(down_blocks_1_resnets_0_nonlinearity_2);  down_blocks_1_resnets_0_nonlinearity_2 = None
    down_blocks_1_resnets_0_conv2 = getattr(getattr(self.down_blocks, "1").resnets, "0").conv2(down_blocks_1_resnets_0_dropout);  down_blocks_1_resnets_0_dropout = None
    down_blocks_1_resnets_0_conv_shortcut = getattr(getattr(self.down_blocks, "1").resnets, "0").conv_shortcut(down_blocks_0_downsamplers_0_conv)
    add_6 = down_blocks_1_resnets_0_conv_shortcut + down_blocks_1_resnets_0_conv2;  down_blocks_1_resnets_0_conv_shortcut = down_blocks_1_resnets_0_conv2 = None
    getattr_9 = add_6.shape
    getitem_9 = getattr_9[0]
    getitem_10 = getattr_9[1]
    getitem_11 = getattr_9[2]
    getitem_12 = getattr_9[3];  getattr_9 = None
    down_blocks_1_attentions_0_norm = getattr(getattr(self.down_blocks, "1").attentions, "0").norm(add_6)
    getattr_10 = down_blocks_1_attentions_0_norm.shape
    getitem_13 = getattr_10[1];  getattr_10 = None
    permute = down_blocks_1_attentions_0_norm.permute(0, 2, 3, 1);  down_blocks_1_attentions_0_norm = None
    mul_4 = getitem_11 * getitem_12
    reshape_1 = permute.reshape(getitem_9, mul_4, getitem_13);  permute = mul_4 = None
    down_blocks_1_attentions_0_proj_in = getattr(getattr(self.down_blocks, "1").attentions, "0").proj_in(reshape_1);  reshape_1 = None
    down_blocks_1_attentions_0_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").norm1(down_blocks_1_attentions_0_proj_in)
    down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_q(down_blocks_1_attentions_0_transformer_blocks_0_norm1)
    down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_k(down_blocks_1_attentions_0_transformer_blocks_0_norm1)
    down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_v(down_blocks_1_attentions_0_transformer_blocks_0_norm1);  down_blocks_1_attentions_0_transformer_blocks_0_norm1 = None
    size = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.size()
    getitem_14 = size[0]
    getitem_15 = size[1]
    getitem_16 = size[2];  size = None
    size_1 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.size(0)
    size_2 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.size(1)
    view = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.view(size_1, size_2, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q = size_1 = size_2 = None
    transpose = view.transpose(1, 2);  view = None
    size_3 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.size(0)
    size_4 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.size(1)
    view_1 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.view(size_3, size_4, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k = size_3 = size_4 = None
    transpose_1 = view_1.transpose(1, 2);  view_1 = None
    size_5 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.size(0)
    size_6 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.size(1)
    view_2 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.view(size_5, size_6, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v = size_5 = size_6 = None
    transpose_2 = view_2.transpose(1, 2);  view_2 = None
    transpose_3 = transpose_1.transpose(-2, -1);  transpose_1 = None
    matmul = torch.matmul(transpose, transpose_3);  transpose = transpose_3 = None
    mul_5 = matmul * 0.125;  matmul = None
    softmax = torch.softmax(mul_5, dim = -1);  mul_5 = None
    matmul_1 = torch.matmul(softmax, transpose_2);  softmax = transpose_2 = None
    transpose_4 = matmul_1.transpose(1, 2);  matmul_1 = None
    contiguous = transpose_4.contiguous();  transpose_4 = None
    view_3 = contiguous.view(getitem_14, getitem_15, getitem_16);  contiguous = getitem_14 = getitem_15 = getitem_16 = None
    down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_out, "0")(view_3);  view_3 = None
    down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_out, "1")(down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0);  down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 = None
    add_7 = down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_1 + down_blocks_1_attentions_0_proj_in;  down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_1 = down_blocks_1_attentions_0_proj_in = None
    down_blocks_1_attentions_0_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").norm2(add_7)
    down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_q(down_blocks_1_attentions_0_transformer_blocks_0_norm2)
    down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_k(down_blocks_1_attentions_0_transformer_blocks_0_norm2)
    down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_v(down_blocks_1_attentions_0_transformer_blocks_0_norm2);  down_blocks_1_attentions_0_transformer_blocks_0_norm2 = None
    size_7 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.size()
    getitem_17 = size_7[0]
    getitem_18 = size_7[1]
    getitem_19 = size_7[2];  size_7 = None
    size_8 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.size(0)
    size_9 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.size(1)
    view_4 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.view(size_8, size_9, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q = size_8 = size_9 = None
    transpose_5 = view_4.transpose(1, 2);  view_4 = None
    size_10 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.size(0)
    size_11 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.size(1)
    view_5 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.view(size_10, size_11, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k = size_10 = size_11 = None
    transpose_6 = view_5.transpose(1, 2);  view_5 = None
    size_12 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.size(0)
    size_13 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.size(1)
    view_6 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.view(size_12, size_13, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v = size_12 = size_13 = None
    transpose_7 = view_6.transpose(1, 2);  view_6 = None
    transpose_8 = transpose_6.transpose(-2, -1);  transpose_6 = None
    matmul_2 = torch.matmul(transpose_5, transpose_8);  transpose_5 = transpose_8 = None
    mul_6 = matmul_2 * 0.125;  matmul_2 = None
    softmax_1 = torch.softmax(mul_6, dim = -1);  mul_6 = None
    matmul_3 = torch.matmul(softmax_1, transpose_7);  softmax_1 = transpose_7 = None
    transpose_9 = matmul_3.transpose(1, 2);  matmul_3 = None
    contiguous_1 = transpose_9.contiguous();  transpose_9 = None
    view_7 = contiguous_1.view(getitem_17, getitem_18, getitem_19);  contiguous_1 = getitem_17 = getitem_18 = getitem_19 = None
    down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_out, "0")(view_7);  view_7 = None
    down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_out, "1")(down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0);  down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 = None
    add_8 = down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_1 + add_7;  down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_1 = add_7 = None
    down_blocks_1_attentions_0_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").norm3(add_8)
    down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").ff.net, "0").proj(down_blocks_1_attentions_0_transformer_blocks_0_norm3);  down_blocks_1_attentions_0_transformer_blocks_0_norm3 = None
    chunk = down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj = None
    getitem_20 = chunk[0]
    getitem_21 = chunk[1];  chunk = None
    gelu = torch._C._nn.gelu(getitem_21);  getitem_21 = None
    mul_7 = getitem_20 * gelu;  getitem_20 = gelu = None
    down_blocks_1_attentions_0_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").ff.net, "1")(mul_7);  mul_7 = None
    down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "0").ff.net, "2")(down_blocks_1_attentions_0_transformer_blocks_0_ff_net_1);  down_blocks_1_attentions_0_transformer_blocks_0_ff_net_1 = None
    add_9 = down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 + add_8;  down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 = add_8 = None
    down_blocks_1_attentions_0_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").norm1(add_9)
    down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_q(down_blocks_1_attentions_0_transformer_blocks_1_norm1)
    down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_k(down_blocks_1_attentions_0_transformer_blocks_1_norm1)
    down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_v(down_blocks_1_attentions_0_transformer_blocks_1_norm1);  down_blocks_1_attentions_0_transformer_blocks_1_norm1 = None
    size_14 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.size()
    getitem_22 = size_14[0]
    getitem_23 = size_14[1]
    getitem_24 = size_14[2];  size_14 = None
    size_15 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.size(0)
    size_16 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.size(1)
    view_8 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.view(size_15, size_16, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q = size_15 = size_16 = None
    transpose_10 = view_8.transpose(1, 2);  view_8 = None
    size_17 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k.size(0)
    size_18 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k.size(1)
    view_9 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k.view(size_17, size_18, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k = size_17 = size_18 = None
    transpose_11 = view_9.transpose(1, 2);  view_9 = None
    size_19 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v.size(0)
    size_20 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v.size(1)
    view_10 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v.view(size_19, size_20, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v = size_19 = size_20 = None
    transpose_12 = view_10.transpose(1, 2);  view_10 = None
    transpose_13 = transpose_11.transpose(-2, -1);  transpose_11 = None
    matmul_4 = torch.matmul(transpose_10, transpose_13);  transpose_10 = transpose_13 = None
    mul_8 = matmul_4 * 0.125;  matmul_4 = None
    softmax_2 = torch.softmax(mul_8, dim = -1);  mul_8 = None
    matmul_5 = torch.matmul(softmax_2, transpose_12);  softmax_2 = transpose_12 = None
    transpose_14 = matmul_5.transpose(1, 2);  matmul_5 = None
    contiguous_2 = transpose_14.contiguous();  transpose_14 = None
    view_11 = contiguous_2.view(getitem_22, getitem_23, getitem_24);  contiguous_2 = getitem_22 = getitem_23 = getitem_24 = None
    down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_out, "0")(view_11);  view_11 = None
    down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_out, "1")(down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_0);  down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_0 = None
    add_10 = down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_1 + add_9;  down_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_1 = add_9 = None
    down_blocks_1_attentions_0_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").norm2(add_10)
    down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_q(down_blocks_1_attentions_0_transformer_blocks_1_norm2)
    down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_k(down_blocks_1_attentions_0_transformer_blocks_1_norm2)
    down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_v(down_blocks_1_attentions_0_transformer_blocks_1_norm2);  down_blocks_1_attentions_0_transformer_blocks_1_norm2 = None
    size_21 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.size()
    getitem_25 = size_21[0]
    getitem_26 = size_21[1]
    getitem_27 = size_21[2];  size_21 = None
    size_22 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.size(0)
    size_23 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.size(1)
    view_12 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.view(size_22, size_23, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q = size_22 = size_23 = None
    transpose_15 = view_12.transpose(1, 2);  view_12 = None
    size_24 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k.size(0)
    size_25 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k.size(1)
    view_13 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k.view(size_24, size_25, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k = size_24 = size_25 = None
    transpose_16 = view_13.transpose(1, 2);  view_13 = None
    size_26 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v.size(0)
    size_27 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v.size(1)
    view_14 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v.view(size_26, size_27, 10, 64);  down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v = size_26 = size_27 = None
    transpose_17 = view_14.transpose(1, 2);  view_14 = None
    transpose_18 = transpose_16.transpose(-2, -1);  transpose_16 = None
    matmul_6 = torch.matmul(transpose_15, transpose_18);  transpose_15 = transpose_18 = None
    mul_9 = matmul_6 * 0.125;  matmul_6 = None
    softmax_3 = torch.softmax(mul_9, dim = -1);  mul_9 = None
    matmul_7 = torch.matmul(softmax_3, transpose_17);  softmax_3 = transpose_17 = None
    transpose_19 = matmul_7.transpose(1, 2);  matmul_7 = None
    contiguous_3 = transpose_19.contiguous();  transpose_19 = None
    view_15 = contiguous_3.view(getitem_25, getitem_26, getitem_27);  contiguous_3 = getitem_25 = getitem_26 = getitem_27 = None
    down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_out, "0")(view_15);  view_15 = None
    down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_out, "1")(down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_0);  down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_0 = None
    add_11 = down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_1 + add_10;  down_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_1 = add_10 = None
    down_blocks_1_attentions_0_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").norm3(add_11)
    down_blocks_1_attentions_0_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").ff.net, "0").proj(down_blocks_1_attentions_0_transformer_blocks_1_norm3);  down_blocks_1_attentions_0_transformer_blocks_1_norm3 = None
    chunk_1 = down_blocks_1_attentions_0_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_1_attentions_0_transformer_blocks_1_ff_net_0_proj = None
    getitem_28 = chunk_1[0]
    getitem_29 = chunk_1[1];  chunk_1 = None
    gelu_1 = torch._C._nn.gelu(getitem_29);  getitem_29 = None
    mul_10 = getitem_28 * gelu_1;  getitem_28 = gelu_1 = None
    down_blocks_1_attentions_0_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").ff.net, "1")(mul_10);  mul_10 = None
    down_blocks_1_attentions_0_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "0").transformer_blocks, "1").ff.net, "2")(down_blocks_1_attentions_0_transformer_blocks_1_ff_net_1);  down_blocks_1_attentions_0_transformer_blocks_1_ff_net_1 = None
    add_12 = down_blocks_1_attentions_0_transformer_blocks_1_ff_net_2 + add_11;  down_blocks_1_attentions_0_transformer_blocks_1_ff_net_2 = add_11 = None
    down_blocks_1_attentions_0_proj_out = getattr(getattr(self.down_blocks, "1").attentions, "0").proj_out(add_12);  add_12 = None
    reshape_2 = down_blocks_1_attentions_0_proj_out.reshape(getitem_9, getitem_11, getitem_12, getitem_13);  down_blocks_1_attentions_0_proj_out = getitem_9 = getitem_11 = getitem_12 = getitem_13 = None
    permute_1 = reshape_2.permute(0, 3, 1, 2);  reshape_2 = None
    contiguous_4 = permute_1.contiguous();  permute_1 = None
    add_13 = contiguous_4 + add_6;  contiguous_4 = add_6 = None
    down_blocks_1_resnets_1_norm1 = getattr(getattr(self.down_blocks, "1").resnets, "1").norm1(add_13)
    down_blocks_1_resnets_1_nonlinearity = getattr(getattr(self.down_blocks, "1").resnets, "1").nonlinearity(down_blocks_1_resnets_1_norm1);  down_blocks_1_resnets_1_norm1 = None
    down_blocks_1_resnets_1_conv1 = getattr(getattr(self.down_blocks, "1").resnets, "1").conv1(down_blocks_1_resnets_1_nonlinearity);  down_blocks_1_resnets_1_nonlinearity = None
    down_blocks_1_resnets_1_nonlinearity_1 = getattr(getattr(self.down_blocks, "1").resnets, "1").nonlinearity(add)
    down_blocks_1_resnets_1_time_emb_proj = getattr(getattr(self.down_blocks, "1").resnets, "1").time_emb_proj(down_blocks_1_resnets_1_nonlinearity_1);  down_blocks_1_resnets_1_nonlinearity_1 = None
    getitem_30 = down_blocks_1_resnets_1_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  down_blocks_1_resnets_1_time_emb_proj = None
    add_14 = down_blocks_1_resnets_1_conv1 + getitem_30;  down_blocks_1_resnets_1_conv1 = getitem_30 = None
    down_blocks_1_resnets_1_norm2 = getattr(getattr(self.down_blocks, "1").resnets, "1").norm2(add_14);  add_14 = None
    down_blocks_1_resnets_1_nonlinearity_2 = getattr(getattr(self.down_blocks, "1").resnets, "1").nonlinearity(down_blocks_1_resnets_1_norm2);  down_blocks_1_resnets_1_norm2 = None
    down_blocks_1_resnets_1_dropout = getattr(getattr(self.down_blocks, "1").resnets, "1").dropout(down_blocks_1_resnets_1_nonlinearity_2);  down_blocks_1_resnets_1_nonlinearity_2 = None
    down_blocks_1_resnets_1_conv2 = getattr(getattr(self.down_blocks, "1").resnets, "1").conv2(down_blocks_1_resnets_1_dropout);  down_blocks_1_resnets_1_dropout = None
    add_15 = add_13 + down_blocks_1_resnets_1_conv2;  down_blocks_1_resnets_1_conv2 = None
    getattr_11 = add_15.shape
    getitem_31 = getattr_11[0]
    getitem_32 = getattr_11[1]
    getitem_33 = getattr_11[2]
    getitem_34 = getattr_11[3];  getattr_11 = None
    down_blocks_1_attentions_1_norm = getattr(getattr(self.down_blocks, "1").attentions, "1").norm(add_15)
    getattr_12 = down_blocks_1_attentions_1_norm.shape
    getitem_35 = getattr_12[1];  getattr_12 = None
    permute_2 = down_blocks_1_attentions_1_norm.permute(0, 2, 3, 1);  down_blocks_1_attentions_1_norm = None
    mul_11 = getitem_33 * getitem_34
    reshape_3 = permute_2.reshape(getitem_31, mul_11, getitem_35);  permute_2 = mul_11 = None
    down_blocks_1_attentions_1_proj_in = getattr(getattr(self.down_blocks, "1").attentions, "1").proj_in(reshape_3);  reshape_3 = None
    down_blocks_1_attentions_1_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").norm1(down_blocks_1_attentions_1_proj_in)
    down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_q(down_blocks_1_attentions_1_transformer_blocks_0_norm1)
    down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_k(down_blocks_1_attentions_1_transformer_blocks_0_norm1)
    down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_v(down_blocks_1_attentions_1_transformer_blocks_0_norm1);  down_blocks_1_attentions_1_transformer_blocks_0_norm1 = None
    size_28 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.size()
    getitem_36 = size_28[0]
    getitem_37 = size_28[1]
    getitem_38 = size_28[2];  size_28 = None
    size_29 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.size(0)
    size_30 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.size(1)
    view_16 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.view(size_29, size_30, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q = size_29 = size_30 = None
    transpose_20 = view_16.transpose(1, 2);  view_16 = None
    size_31 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.size(0)
    size_32 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.size(1)
    view_17 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.view(size_31, size_32, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k = size_31 = size_32 = None
    transpose_21 = view_17.transpose(1, 2);  view_17 = None
    size_33 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.size(0)
    size_34 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.size(1)
    view_18 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.view(size_33, size_34, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v = size_33 = size_34 = None
    transpose_22 = view_18.transpose(1, 2);  view_18 = None
    transpose_23 = transpose_21.transpose(-2, -1);  transpose_21 = None
    matmul_8 = torch.matmul(transpose_20, transpose_23);  transpose_20 = transpose_23 = None
    mul_12 = matmul_8 * 0.125;  matmul_8 = None
    softmax_4 = torch.softmax(mul_12, dim = -1);  mul_12 = None
    matmul_9 = torch.matmul(softmax_4, transpose_22);  softmax_4 = transpose_22 = None
    transpose_24 = matmul_9.transpose(1, 2);  matmul_9 = None
    contiguous_5 = transpose_24.contiguous();  transpose_24 = None
    view_19 = contiguous_5.view(getitem_36, getitem_37, getitem_38);  contiguous_5 = getitem_36 = getitem_37 = getitem_38 = None
    down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_out, "0")(view_19);  view_19 = None
    down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_out, "1")(down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0);  down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 = None
    add_16 = down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_1 + down_blocks_1_attentions_1_proj_in;  down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_1 = down_blocks_1_attentions_1_proj_in = None
    down_blocks_1_attentions_1_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").norm2(add_16)
    down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_q(down_blocks_1_attentions_1_transformer_blocks_0_norm2)
    down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_k(down_blocks_1_attentions_1_transformer_blocks_0_norm2)
    down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_v(down_blocks_1_attentions_1_transformer_blocks_0_norm2);  down_blocks_1_attentions_1_transformer_blocks_0_norm2 = None
    size_35 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.size()
    getitem_39 = size_35[0]
    getitem_40 = size_35[1]
    getitem_41 = size_35[2];  size_35 = None
    size_36 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.size(0)
    size_37 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.size(1)
    view_20 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.view(size_36, size_37, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q = size_36 = size_37 = None
    transpose_25 = view_20.transpose(1, 2);  view_20 = None
    size_38 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.size(0)
    size_39 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.size(1)
    view_21 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.view(size_38, size_39, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k = size_38 = size_39 = None
    transpose_26 = view_21.transpose(1, 2);  view_21 = None
    size_40 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.size(0)
    size_41 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.size(1)
    view_22 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.view(size_40, size_41, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v = size_40 = size_41 = None
    transpose_27 = view_22.transpose(1, 2);  view_22 = None
    transpose_28 = transpose_26.transpose(-2, -1);  transpose_26 = None
    matmul_10 = torch.matmul(transpose_25, transpose_28);  transpose_25 = transpose_28 = None
    mul_13 = matmul_10 * 0.125;  matmul_10 = None
    softmax_5 = torch.softmax(mul_13, dim = -1);  mul_13 = None
    matmul_11 = torch.matmul(softmax_5, transpose_27);  softmax_5 = transpose_27 = None
    transpose_29 = matmul_11.transpose(1, 2);  matmul_11 = None
    contiguous_6 = transpose_29.contiguous();  transpose_29 = None
    view_23 = contiguous_6.view(getitem_39, getitem_40, getitem_41);  contiguous_6 = getitem_39 = getitem_40 = getitem_41 = None
    down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_out, "0")(view_23);  view_23 = None
    down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_out, "1")(down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0);  down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 = None
    add_17 = down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_1 + add_16;  down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_1 = add_16 = None
    down_blocks_1_attentions_1_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").norm3(add_17)
    down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").ff.net, "0").proj(down_blocks_1_attentions_1_transformer_blocks_0_norm3);  down_blocks_1_attentions_1_transformer_blocks_0_norm3 = None
    chunk_2 = down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj = None
    getitem_42 = chunk_2[0]
    getitem_43 = chunk_2[1];  chunk_2 = None
    gelu_2 = torch._C._nn.gelu(getitem_43);  getitem_43 = None
    mul_14 = getitem_42 * gelu_2;  getitem_42 = gelu_2 = None
    down_blocks_1_attentions_1_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").ff.net, "1")(mul_14);  mul_14 = None
    down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "0").ff.net, "2")(down_blocks_1_attentions_1_transformer_blocks_0_ff_net_1);  down_blocks_1_attentions_1_transformer_blocks_0_ff_net_1 = None
    add_18 = down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 + add_17;  down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 = add_17 = None
    down_blocks_1_attentions_1_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").norm1(add_18)
    down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_q(down_blocks_1_attentions_1_transformer_blocks_1_norm1)
    down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_k(down_blocks_1_attentions_1_transformer_blocks_1_norm1)
    down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_v(down_blocks_1_attentions_1_transformer_blocks_1_norm1);  down_blocks_1_attentions_1_transformer_blocks_1_norm1 = None
    size_42 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.size()
    getitem_44 = size_42[0]
    getitem_45 = size_42[1]
    getitem_46 = size_42[2];  size_42 = None
    size_43 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.size(0)
    size_44 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.size(1)
    view_24 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.view(size_43, size_44, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q = size_43 = size_44 = None
    transpose_30 = view_24.transpose(1, 2);  view_24 = None
    size_45 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k.size(0)
    size_46 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k.size(1)
    view_25 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k.view(size_45, size_46, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k = size_45 = size_46 = None
    transpose_31 = view_25.transpose(1, 2);  view_25 = None
    size_47 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v.size(0)
    size_48 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v.size(1)
    view_26 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v.view(size_47, size_48, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v = size_47 = size_48 = None
    transpose_32 = view_26.transpose(1, 2);  view_26 = None
    transpose_33 = transpose_31.transpose(-2, -1);  transpose_31 = None
    matmul_12 = torch.matmul(transpose_30, transpose_33);  transpose_30 = transpose_33 = None
    mul_15 = matmul_12 * 0.125;  matmul_12 = None
    softmax_6 = torch.softmax(mul_15, dim = -1);  mul_15 = None
    matmul_13 = torch.matmul(softmax_6, transpose_32);  softmax_6 = transpose_32 = None
    transpose_34 = matmul_13.transpose(1, 2);  matmul_13 = None
    contiguous_7 = transpose_34.contiguous();  transpose_34 = None
    view_27 = contiguous_7.view(getitem_44, getitem_45, getitem_46);  contiguous_7 = getitem_44 = getitem_45 = getitem_46 = None
    down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_out, "0")(view_27);  view_27 = None
    down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_out, "1")(down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_0);  down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_0 = None
    add_19 = down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_1 + add_18;  down_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_1 = add_18 = None
    down_blocks_1_attentions_1_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").norm2(add_19)
    down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_q(down_blocks_1_attentions_1_transformer_blocks_1_norm2)
    down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_k(down_blocks_1_attentions_1_transformer_blocks_1_norm2)
    down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_v(down_blocks_1_attentions_1_transformer_blocks_1_norm2);  down_blocks_1_attentions_1_transformer_blocks_1_norm2 = None
    size_49 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.size()
    getitem_47 = size_49[0]
    getitem_48 = size_49[1]
    getitem_49 = size_49[2];  size_49 = None
    size_50 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.size(0)
    size_51 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.size(1)
    view_28 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.view(size_50, size_51, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q = size_50 = size_51 = None
    transpose_35 = view_28.transpose(1, 2);  view_28 = None
    size_52 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k.size(0)
    size_53 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k.size(1)
    view_29 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k.view(size_52, size_53, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k = size_52 = size_53 = None
    transpose_36 = view_29.transpose(1, 2);  view_29 = None
    size_54 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v.size(0)
    size_55 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v.size(1)
    view_30 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v.view(size_54, size_55, 10, 64);  down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v = size_54 = size_55 = None
    transpose_37 = view_30.transpose(1, 2);  view_30 = None
    transpose_38 = transpose_36.transpose(-2, -1);  transpose_36 = None
    matmul_14 = torch.matmul(transpose_35, transpose_38);  transpose_35 = transpose_38 = None
    mul_16 = matmul_14 * 0.125;  matmul_14 = None
    softmax_7 = torch.softmax(mul_16, dim = -1);  mul_16 = None
    matmul_15 = torch.matmul(softmax_7, transpose_37);  softmax_7 = transpose_37 = None
    transpose_39 = matmul_15.transpose(1, 2);  matmul_15 = None
    contiguous_8 = transpose_39.contiguous();  transpose_39 = None
    view_31 = contiguous_8.view(getitem_47, getitem_48, getitem_49);  contiguous_8 = getitem_47 = getitem_48 = getitem_49 = None
    down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_out, "0")(view_31);  view_31 = None
    down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_out, "1")(down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_0);  down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_0 = None
    add_20 = down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_1 + add_19;  down_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_1 = add_19 = None
    down_blocks_1_attentions_1_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").norm3(add_20)
    down_blocks_1_attentions_1_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").ff.net, "0").proj(down_blocks_1_attentions_1_transformer_blocks_1_norm3);  down_blocks_1_attentions_1_transformer_blocks_1_norm3 = None
    chunk_3 = down_blocks_1_attentions_1_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_1_attentions_1_transformer_blocks_1_ff_net_0_proj = None
    getitem_50 = chunk_3[0]
    getitem_51 = chunk_3[1];  chunk_3 = None
    gelu_3 = torch._C._nn.gelu(getitem_51);  getitem_51 = None
    mul_17 = getitem_50 * gelu_3;  getitem_50 = gelu_3 = None
    down_blocks_1_attentions_1_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").ff.net, "1")(mul_17);  mul_17 = None
    down_blocks_1_attentions_1_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "1").attentions, "1").transformer_blocks, "1").ff.net, "2")(down_blocks_1_attentions_1_transformer_blocks_1_ff_net_1);  down_blocks_1_attentions_1_transformer_blocks_1_ff_net_1 = None
    add_21 = down_blocks_1_attentions_1_transformer_blocks_1_ff_net_2 + add_20;  down_blocks_1_attentions_1_transformer_blocks_1_ff_net_2 = add_20 = None
    down_blocks_1_attentions_1_proj_out = getattr(getattr(self.down_blocks, "1").attentions, "1").proj_out(add_21);  add_21 = None
    reshape_4 = down_blocks_1_attentions_1_proj_out.reshape(getitem_31, getitem_33, getitem_34, getitem_35);  down_blocks_1_attentions_1_proj_out = getitem_31 = getitem_33 = getitem_34 = getitem_35 = None
    permute_3 = reshape_4.permute(0, 3, 1, 2);  reshape_4 = None
    contiguous_9 = permute_3.contiguous();  permute_3 = None
    add_22 = contiguous_9 + add_15;  contiguous_9 = add_15 = None
    down_blocks_1_downsamplers_0_conv = getattr(getattr(self.down_blocks, "1").downsamplers, "0").conv(add_22)
    down_blocks_2_resnets_0_norm1 = getattr(getattr(self.down_blocks, "2").resnets, "0").norm1(down_blocks_1_downsamplers_0_conv)
    down_blocks_2_resnets_0_nonlinearity = getattr(getattr(self.down_blocks, "2").resnets, "0").nonlinearity(down_blocks_2_resnets_0_norm1);  down_blocks_2_resnets_0_norm1 = None
    down_blocks_2_resnets_0_conv1 = getattr(getattr(self.down_blocks, "2").resnets, "0").conv1(down_blocks_2_resnets_0_nonlinearity);  down_blocks_2_resnets_0_nonlinearity = None
    down_blocks_2_resnets_0_nonlinearity_1 = getattr(getattr(self.down_blocks, "2").resnets, "0").nonlinearity(add)
    down_blocks_2_resnets_0_time_emb_proj = getattr(getattr(self.down_blocks, "2").resnets, "0").time_emb_proj(down_blocks_2_resnets_0_nonlinearity_1);  down_blocks_2_resnets_0_nonlinearity_1 = None
    getitem_52 = down_blocks_2_resnets_0_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  down_blocks_2_resnets_0_time_emb_proj = None
    add_23 = down_blocks_2_resnets_0_conv1 + getitem_52;  down_blocks_2_resnets_0_conv1 = getitem_52 = None
    down_blocks_2_resnets_0_norm2 = getattr(getattr(self.down_blocks, "2").resnets, "0").norm2(add_23);  add_23 = None
    down_blocks_2_resnets_0_nonlinearity_2 = getattr(getattr(self.down_blocks, "2").resnets, "0").nonlinearity(down_blocks_2_resnets_0_norm2);  down_blocks_2_resnets_0_norm2 = None
    down_blocks_2_resnets_0_dropout = getattr(getattr(self.down_blocks, "2").resnets, "0").dropout(down_blocks_2_resnets_0_nonlinearity_2);  down_blocks_2_resnets_0_nonlinearity_2 = None
    down_blocks_2_resnets_0_conv2 = getattr(getattr(self.down_blocks, "2").resnets, "0").conv2(down_blocks_2_resnets_0_dropout);  down_blocks_2_resnets_0_dropout = None
    down_blocks_2_resnets_0_conv_shortcut = getattr(getattr(self.down_blocks, "2").resnets, "0").conv_shortcut(down_blocks_1_downsamplers_0_conv)
    add_24 = down_blocks_2_resnets_0_conv_shortcut + down_blocks_2_resnets_0_conv2;  down_blocks_2_resnets_0_conv_shortcut = down_blocks_2_resnets_0_conv2 = None
    getattr_13 = add_24.shape
    getitem_53 = getattr_13[0]
    getitem_54 = getattr_13[1]
    getitem_55 = getattr_13[2]
    getitem_56 = getattr_13[3];  getattr_13 = None
    down_blocks_2_attentions_0_norm = getattr(getattr(self.down_blocks, "2").attentions, "0").norm(add_24)
    getattr_14 = down_blocks_2_attentions_0_norm.shape
    getitem_57 = getattr_14[1];  getattr_14 = None
    permute_4 = down_blocks_2_attentions_0_norm.permute(0, 2, 3, 1);  down_blocks_2_attentions_0_norm = None
    mul_18 = getitem_55 * getitem_56
    reshape_5 = permute_4.reshape(getitem_53, mul_18, getitem_57);  permute_4 = mul_18 = None
    down_blocks_2_attentions_0_proj_in = getattr(getattr(self.down_blocks, "2").attentions, "0").proj_in(reshape_5);  reshape_5 = None
    down_blocks_2_attentions_0_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").norm1(down_blocks_2_attentions_0_proj_in)
    down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_0_norm1)
    down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_0_norm1)
    down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_0_norm1);  down_blocks_2_attentions_0_transformer_blocks_0_norm1 = None
    size_56 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.size()
    getitem_58 = size_56[0]
    getitem_59 = size_56[1]
    getitem_60 = size_56[2];  size_56 = None
    size_57 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.size(0)
    size_58 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.size(1)
    view_32 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.view(size_57, size_58, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q = size_57 = size_58 = None
    transpose_40 = view_32.transpose(1, 2);  view_32 = None
    size_59 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.size(0)
    size_60 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.size(1)
    view_33 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.view(size_59, size_60, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k = size_59 = size_60 = None
    transpose_41 = view_33.transpose(1, 2);  view_33 = None
    size_61 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.size(0)
    size_62 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.size(1)
    view_34 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.view(size_61, size_62, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v = size_61 = size_62 = None
    transpose_42 = view_34.transpose(1, 2);  view_34 = None
    transpose_43 = transpose_41.transpose(-2, -1);  transpose_41 = None
    matmul_16 = torch.matmul(transpose_40, transpose_43);  transpose_40 = transpose_43 = None
    mul_19 = matmul_16 * 0.125;  matmul_16 = None
    softmax_8 = torch.softmax(mul_19, dim = -1);  mul_19 = None
    matmul_17 = torch.matmul(softmax_8, transpose_42);  softmax_8 = transpose_42 = None
    transpose_44 = matmul_17.transpose(1, 2);  matmul_17 = None
    contiguous_10 = transpose_44.contiguous();  transpose_44 = None
    view_35 = contiguous_10.view(getitem_58, getitem_59, getitem_60);  contiguous_10 = getitem_58 = getitem_59 = getitem_60 = None
    down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn1.to_out, "0")(view_35);  view_35 = None
    down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0 = None
    add_25 = down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_1 + down_blocks_2_attentions_0_proj_in;  down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_1 = down_blocks_2_attentions_0_proj_in = None
    down_blocks_2_attentions_0_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").norm2(add_25)
    down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_0_norm2)
    down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_0_norm2)
    down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_0_norm2);  down_blocks_2_attentions_0_transformer_blocks_0_norm2 = None
    size_63 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.size()
    getitem_61 = size_63[0]
    getitem_62 = size_63[1]
    getitem_63 = size_63[2];  size_63 = None
    size_64 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.size(0)
    size_65 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.size(1)
    view_36 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.view(size_64, size_65, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q = size_64 = size_65 = None
    transpose_45 = view_36.transpose(1, 2);  view_36 = None
    size_66 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.size(0)
    size_67 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.size(1)
    view_37 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.view(size_66, size_67, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k = size_66 = size_67 = None
    transpose_46 = view_37.transpose(1, 2);  view_37 = None
    size_68 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.size(0)
    size_69 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.size(1)
    view_38 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.view(size_68, size_69, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v = size_68 = size_69 = None
    transpose_47 = view_38.transpose(1, 2);  view_38 = None
    transpose_48 = transpose_46.transpose(-2, -1);  transpose_46 = None
    matmul_18 = torch.matmul(transpose_45, transpose_48);  transpose_45 = transpose_48 = None
    mul_20 = matmul_18 * 0.125;  matmul_18 = None
    softmax_9 = torch.softmax(mul_20, dim = -1);  mul_20 = None
    matmul_19 = torch.matmul(softmax_9, transpose_47);  softmax_9 = transpose_47 = None
    transpose_49 = matmul_19.transpose(1, 2);  matmul_19 = None
    contiguous_11 = transpose_49.contiguous();  transpose_49 = None
    view_39 = contiguous_11.view(getitem_61, getitem_62, getitem_63);  contiguous_11 = getitem_61 = getitem_62 = getitem_63 = None
    down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn2.to_out, "0")(view_39);  view_39 = None
    down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0 = None
    add_26 = down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_1 + add_25;  down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_1 = add_25 = None
    down_blocks_2_attentions_0_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").norm3(add_26)
    down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_0_norm3);  down_blocks_2_attentions_0_transformer_blocks_0_norm3 = None
    chunk_4 = down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj = None
    getitem_64 = chunk_4[0]
    getitem_65 = chunk_4[1];  chunk_4 = None
    gelu_4 = torch._C._nn.gelu(getitem_65);  getitem_65 = None
    mul_21 = getitem_64 * gelu_4;  getitem_64 = gelu_4 = None
    down_blocks_2_attentions_0_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").ff.net, "1")(mul_21);  mul_21 = None
    down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "0").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_0_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_0_ff_net_1 = None
    add_27 = down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 + add_26;  down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 = add_26 = None
    down_blocks_2_attentions_0_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").norm1(add_27)
    down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_1_norm1)
    down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_1_norm1)
    down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_1_norm1);  down_blocks_2_attentions_0_transformer_blocks_1_norm1 = None
    size_70 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_q.size()
    getitem_66 = size_70[0]
    getitem_67 = size_70[1]
    getitem_68 = size_70[2];  size_70 = None
    size_71 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_q.size(0)
    size_72 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_q.size(1)
    view_40 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_q.view(size_71, size_72, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_q = size_71 = size_72 = None
    transpose_50 = view_40.transpose(1, 2);  view_40 = None
    size_73 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_k.size(0)
    size_74 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_k.size(1)
    view_41 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_k.view(size_73, size_74, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_k = size_73 = size_74 = None
    transpose_51 = view_41.transpose(1, 2);  view_41 = None
    size_75 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_v.size(0)
    size_76 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_v.size(1)
    view_42 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_v.view(size_75, size_76, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_v = size_75 = size_76 = None
    transpose_52 = view_42.transpose(1, 2);  view_42 = None
    transpose_53 = transpose_51.transpose(-2, -1);  transpose_51 = None
    matmul_20 = torch.matmul(transpose_50, transpose_53);  transpose_50 = transpose_53 = None
    mul_22 = matmul_20 * 0.125;  matmul_20 = None
    softmax_10 = torch.softmax(mul_22, dim = -1);  mul_22 = None
    matmul_21 = torch.matmul(softmax_10, transpose_52);  softmax_10 = transpose_52 = None
    transpose_54 = matmul_21.transpose(1, 2);  matmul_21 = None
    contiguous_12 = transpose_54.contiguous();  transpose_54 = None
    view_43 = contiguous_12.view(getitem_66, getitem_67, getitem_68);  contiguous_12 = getitem_66 = getitem_67 = getitem_68 = None
    down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn1.to_out, "0")(view_43);  view_43 = None
    down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_out_0 = None
    add_28 = down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_out_1 + add_27;  down_blocks_2_attentions_0_transformer_blocks_1_attn1_to_out_1 = add_27 = None
    down_blocks_2_attentions_0_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").norm2(add_28)
    down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_1_norm2)
    down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_1_norm2)
    down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_1_norm2);  down_blocks_2_attentions_0_transformer_blocks_1_norm2 = None
    size_77 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_q.size()
    getitem_69 = size_77[0]
    getitem_70 = size_77[1]
    getitem_71 = size_77[2];  size_77 = None
    size_78 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_q.size(0)
    size_79 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_q.size(1)
    view_44 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_q.view(size_78, size_79, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_q = size_78 = size_79 = None
    transpose_55 = view_44.transpose(1, 2);  view_44 = None
    size_80 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_k.size(0)
    size_81 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_k.size(1)
    view_45 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_k.view(size_80, size_81, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_k = size_80 = size_81 = None
    transpose_56 = view_45.transpose(1, 2);  view_45 = None
    size_82 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_v.size(0)
    size_83 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_v.size(1)
    view_46 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_v.view(size_82, size_83, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_v = size_82 = size_83 = None
    transpose_57 = view_46.transpose(1, 2);  view_46 = None
    transpose_58 = transpose_56.transpose(-2, -1);  transpose_56 = None
    matmul_22 = torch.matmul(transpose_55, transpose_58);  transpose_55 = transpose_58 = None
    mul_23 = matmul_22 * 0.125;  matmul_22 = None
    softmax_11 = torch.softmax(mul_23, dim = -1);  mul_23 = None
    matmul_23 = torch.matmul(softmax_11, transpose_57);  softmax_11 = transpose_57 = None
    transpose_59 = matmul_23.transpose(1, 2);  matmul_23 = None
    contiguous_13 = transpose_59.contiguous();  transpose_59 = None
    view_47 = contiguous_13.view(getitem_69, getitem_70, getitem_71);  contiguous_13 = getitem_69 = getitem_70 = getitem_71 = None
    down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn2.to_out, "0")(view_47);  view_47 = None
    down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_out_0 = None
    add_29 = down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_out_1 + add_28;  down_blocks_2_attentions_0_transformer_blocks_1_attn2_to_out_1 = add_28 = None
    down_blocks_2_attentions_0_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").norm3(add_29)
    down_blocks_2_attentions_0_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_1_norm3);  down_blocks_2_attentions_0_transformer_blocks_1_norm3 = None
    chunk_5 = down_blocks_2_attentions_0_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_1_ff_net_0_proj = None
    getitem_72 = chunk_5[0]
    getitem_73 = chunk_5[1];  chunk_5 = None
    gelu_5 = torch._C._nn.gelu(getitem_73);  getitem_73 = None
    mul_24 = getitem_72 * gelu_5;  getitem_72 = gelu_5 = None
    down_blocks_2_attentions_0_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").ff.net, "1")(mul_24);  mul_24 = None
    down_blocks_2_attentions_0_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "1").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_1_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_1_ff_net_1 = None
    add_30 = down_blocks_2_attentions_0_transformer_blocks_1_ff_net_2 + add_29;  down_blocks_2_attentions_0_transformer_blocks_1_ff_net_2 = add_29 = None
    down_blocks_2_attentions_0_transformer_blocks_2_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").norm1(add_30)
    down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_2_norm1)
    down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_2_norm1)
    down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_2_norm1);  down_blocks_2_attentions_0_transformer_blocks_2_norm1 = None
    size_84 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_q.size()
    getitem_74 = size_84[0]
    getitem_75 = size_84[1]
    getitem_76 = size_84[2];  size_84 = None
    size_85 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_q.size(0)
    size_86 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_q.size(1)
    view_48 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_q.view(size_85, size_86, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_q = size_85 = size_86 = None
    transpose_60 = view_48.transpose(1, 2);  view_48 = None
    size_87 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_k.size(0)
    size_88 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_k.size(1)
    view_49 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_k.view(size_87, size_88, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_k = size_87 = size_88 = None
    transpose_61 = view_49.transpose(1, 2);  view_49 = None
    size_89 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_v.size(0)
    size_90 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_v.size(1)
    view_50 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_v.view(size_89, size_90, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_v = size_89 = size_90 = None
    transpose_62 = view_50.transpose(1, 2);  view_50 = None
    transpose_63 = transpose_61.transpose(-2, -1);  transpose_61 = None
    matmul_24 = torch.matmul(transpose_60, transpose_63);  transpose_60 = transpose_63 = None
    mul_25 = matmul_24 * 0.125;  matmul_24 = None
    softmax_12 = torch.softmax(mul_25, dim = -1);  mul_25 = None
    matmul_25 = torch.matmul(softmax_12, transpose_62);  softmax_12 = transpose_62 = None
    transpose_64 = matmul_25.transpose(1, 2);  matmul_25 = None
    contiguous_14 = transpose_64.contiguous();  transpose_64 = None
    view_51 = contiguous_14.view(getitem_74, getitem_75, getitem_76);  contiguous_14 = getitem_74 = getitem_75 = getitem_76 = None
    down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn1.to_out, "0")(view_51);  view_51 = None
    down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_out_0 = None
    add_31 = down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_out_1 + add_30;  down_blocks_2_attentions_0_transformer_blocks_2_attn1_to_out_1 = add_30 = None
    down_blocks_2_attentions_0_transformer_blocks_2_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").norm2(add_31)
    down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_2_norm2)
    down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_2_norm2)
    down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_2_norm2);  down_blocks_2_attentions_0_transformer_blocks_2_norm2 = None
    size_91 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_q.size()
    getitem_77 = size_91[0]
    getitem_78 = size_91[1]
    getitem_79 = size_91[2];  size_91 = None
    size_92 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_q.size(0)
    size_93 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_q.size(1)
    view_52 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_q.view(size_92, size_93, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_q = size_92 = size_93 = None
    transpose_65 = view_52.transpose(1, 2);  view_52 = None
    size_94 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_k.size(0)
    size_95 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_k.size(1)
    view_53 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_k.view(size_94, size_95, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_k = size_94 = size_95 = None
    transpose_66 = view_53.transpose(1, 2);  view_53 = None
    size_96 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_v.size(0)
    size_97 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_v.size(1)
    view_54 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_v.view(size_96, size_97, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_v = size_96 = size_97 = None
    transpose_67 = view_54.transpose(1, 2);  view_54 = None
    transpose_68 = transpose_66.transpose(-2, -1);  transpose_66 = None
    matmul_26 = torch.matmul(transpose_65, transpose_68);  transpose_65 = transpose_68 = None
    mul_26 = matmul_26 * 0.125;  matmul_26 = None
    softmax_13 = torch.softmax(mul_26, dim = -1);  mul_26 = None
    matmul_27 = torch.matmul(softmax_13, transpose_67);  softmax_13 = transpose_67 = None
    transpose_69 = matmul_27.transpose(1, 2);  matmul_27 = None
    contiguous_15 = transpose_69.contiguous();  transpose_69 = None
    view_55 = contiguous_15.view(getitem_77, getitem_78, getitem_79);  contiguous_15 = getitem_77 = getitem_78 = getitem_79 = None
    down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn2.to_out, "0")(view_55);  view_55 = None
    down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_out_0 = None
    add_32 = down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_out_1 + add_31;  down_blocks_2_attentions_0_transformer_blocks_2_attn2_to_out_1 = add_31 = None
    down_blocks_2_attentions_0_transformer_blocks_2_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").norm3(add_32)
    down_blocks_2_attentions_0_transformer_blocks_2_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_2_norm3);  down_blocks_2_attentions_0_transformer_blocks_2_norm3 = None
    chunk_6 = down_blocks_2_attentions_0_transformer_blocks_2_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_2_ff_net_0_proj = None
    getitem_80 = chunk_6[0]
    getitem_81 = chunk_6[1];  chunk_6 = None
    gelu_6 = torch._C._nn.gelu(getitem_81);  getitem_81 = None
    mul_27 = getitem_80 * gelu_6;  getitem_80 = gelu_6 = None
    down_blocks_2_attentions_0_transformer_blocks_2_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").ff.net, "1")(mul_27);  mul_27 = None
    down_blocks_2_attentions_0_transformer_blocks_2_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "2").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_2_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_2_ff_net_1 = None
    add_33 = down_blocks_2_attentions_0_transformer_blocks_2_ff_net_2 + add_32;  down_blocks_2_attentions_0_transformer_blocks_2_ff_net_2 = add_32 = None
    down_blocks_2_attentions_0_transformer_blocks_3_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").norm1(add_33)
    down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_3_norm1)
    down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_3_norm1)
    down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_3_norm1);  down_blocks_2_attentions_0_transformer_blocks_3_norm1 = None
    size_98 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_q.size()
    getitem_82 = size_98[0]
    getitem_83 = size_98[1]
    getitem_84 = size_98[2];  size_98 = None
    size_99 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_q.size(0)
    size_100 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_q.size(1)
    view_56 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_q.view(size_99, size_100, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_q = size_99 = size_100 = None
    transpose_70 = view_56.transpose(1, 2);  view_56 = None
    size_101 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_k.size(0)
    size_102 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_k.size(1)
    view_57 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_k.view(size_101, size_102, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_k = size_101 = size_102 = None
    transpose_71 = view_57.transpose(1, 2);  view_57 = None
    size_103 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_v.size(0)
    size_104 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_v.size(1)
    view_58 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_v.view(size_103, size_104, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_v = size_103 = size_104 = None
    transpose_72 = view_58.transpose(1, 2);  view_58 = None
    transpose_73 = transpose_71.transpose(-2, -1);  transpose_71 = None
    matmul_28 = torch.matmul(transpose_70, transpose_73);  transpose_70 = transpose_73 = None
    mul_28 = matmul_28 * 0.125;  matmul_28 = None
    softmax_14 = torch.softmax(mul_28, dim = -1);  mul_28 = None
    matmul_29 = torch.matmul(softmax_14, transpose_72);  softmax_14 = transpose_72 = None
    transpose_74 = matmul_29.transpose(1, 2);  matmul_29 = None
    contiguous_16 = transpose_74.contiguous();  transpose_74 = None
    view_59 = contiguous_16.view(getitem_82, getitem_83, getitem_84);  contiguous_16 = getitem_82 = getitem_83 = getitem_84 = None
    down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn1.to_out, "0")(view_59);  view_59 = None
    down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_out_0 = None
    add_34 = down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_out_1 + add_33;  down_blocks_2_attentions_0_transformer_blocks_3_attn1_to_out_1 = add_33 = None
    down_blocks_2_attentions_0_transformer_blocks_3_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").norm2(add_34)
    down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_3_norm2)
    down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_3_norm2)
    down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_3_norm2);  down_blocks_2_attentions_0_transformer_blocks_3_norm2 = None
    size_105 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_q.size()
    getitem_85 = size_105[0]
    getitem_86 = size_105[1]
    getitem_87 = size_105[2];  size_105 = None
    size_106 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_q.size(0)
    size_107 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_q.size(1)
    view_60 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_q.view(size_106, size_107, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_q = size_106 = size_107 = None
    transpose_75 = view_60.transpose(1, 2);  view_60 = None
    size_108 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_k.size(0)
    size_109 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_k.size(1)
    view_61 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_k.view(size_108, size_109, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_k = size_108 = size_109 = None
    transpose_76 = view_61.transpose(1, 2);  view_61 = None
    size_110 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_v.size(0)
    size_111 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_v.size(1)
    view_62 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_v.view(size_110, size_111, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_v = size_110 = size_111 = None
    transpose_77 = view_62.transpose(1, 2);  view_62 = None
    transpose_78 = transpose_76.transpose(-2, -1);  transpose_76 = None
    matmul_30 = torch.matmul(transpose_75, transpose_78);  transpose_75 = transpose_78 = None
    mul_29 = matmul_30 * 0.125;  matmul_30 = None
    softmax_15 = torch.softmax(mul_29, dim = -1);  mul_29 = None
    matmul_31 = torch.matmul(softmax_15, transpose_77);  softmax_15 = transpose_77 = None
    transpose_79 = matmul_31.transpose(1, 2);  matmul_31 = None
    contiguous_17 = transpose_79.contiguous();  transpose_79 = None
    view_63 = contiguous_17.view(getitem_85, getitem_86, getitem_87);  contiguous_17 = getitem_85 = getitem_86 = getitem_87 = None
    down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn2.to_out, "0")(view_63);  view_63 = None
    down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_out_0 = None
    add_35 = down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_out_1 + add_34;  down_blocks_2_attentions_0_transformer_blocks_3_attn2_to_out_1 = add_34 = None
    down_blocks_2_attentions_0_transformer_blocks_3_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").norm3(add_35)
    down_blocks_2_attentions_0_transformer_blocks_3_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_3_norm3);  down_blocks_2_attentions_0_transformer_blocks_3_norm3 = None
    chunk_7 = down_blocks_2_attentions_0_transformer_blocks_3_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_3_ff_net_0_proj = None
    getitem_88 = chunk_7[0]
    getitem_89 = chunk_7[1];  chunk_7 = None
    gelu_7 = torch._C._nn.gelu(getitem_89);  getitem_89 = None
    mul_30 = getitem_88 * gelu_7;  getitem_88 = gelu_7 = None
    down_blocks_2_attentions_0_transformer_blocks_3_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").ff.net, "1")(mul_30);  mul_30 = None
    down_blocks_2_attentions_0_transformer_blocks_3_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "3").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_3_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_3_ff_net_1 = None
    add_36 = down_blocks_2_attentions_0_transformer_blocks_3_ff_net_2 + add_35;  down_blocks_2_attentions_0_transformer_blocks_3_ff_net_2 = add_35 = None
    down_blocks_2_attentions_0_transformer_blocks_4_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").norm1(add_36)
    down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_4_norm1)
    down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_4_norm1)
    down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_4_norm1);  down_blocks_2_attentions_0_transformer_blocks_4_norm1 = None
    size_112 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_q.size()
    getitem_90 = size_112[0]
    getitem_91 = size_112[1]
    getitem_92 = size_112[2];  size_112 = None
    size_113 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_q.size(0)
    size_114 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_q.size(1)
    view_64 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_q.view(size_113, size_114, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_q = size_113 = size_114 = None
    transpose_80 = view_64.transpose(1, 2);  view_64 = None
    size_115 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_k.size(0)
    size_116 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_k.size(1)
    view_65 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_k.view(size_115, size_116, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_k = size_115 = size_116 = None
    transpose_81 = view_65.transpose(1, 2);  view_65 = None
    size_117 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_v.size(0)
    size_118 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_v.size(1)
    view_66 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_v.view(size_117, size_118, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_v = size_117 = size_118 = None
    transpose_82 = view_66.transpose(1, 2);  view_66 = None
    transpose_83 = transpose_81.transpose(-2, -1);  transpose_81 = None
    matmul_32 = torch.matmul(transpose_80, transpose_83);  transpose_80 = transpose_83 = None
    mul_31 = matmul_32 * 0.125;  matmul_32 = None
    softmax_16 = torch.softmax(mul_31, dim = -1);  mul_31 = None
    matmul_33 = torch.matmul(softmax_16, transpose_82);  softmax_16 = transpose_82 = None
    transpose_84 = matmul_33.transpose(1, 2);  matmul_33 = None
    contiguous_18 = transpose_84.contiguous();  transpose_84 = None
    view_67 = contiguous_18.view(getitem_90, getitem_91, getitem_92);  contiguous_18 = getitem_90 = getitem_91 = getitem_92 = None
    down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn1.to_out, "0")(view_67);  view_67 = None
    down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_out_0 = None
    add_37 = down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_out_1 + add_36;  down_blocks_2_attentions_0_transformer_blocks_4_attn1_to_out_1 = add_36 = None
    down_blocks_2_attentions_0_transformer_blocks_4_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").norm2(add_37)
    down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_4_norm2)
    down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_4_norm2)
    down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_4_norm2);  down_blocks_2_attentions_0_transformer_blocks_4_norm2 = None
    size_119 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_q.size()
    getitem_93 = size_119[0]
    getitem_94 = size_119[1]
    getitem_95 = size_119[2];  size_119 = None
    size_120 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_q.size(0)
    size_121 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_q.size(1)
    view_68 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_q.view(size_120, size_121, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_q = size_120 = size_121 = None
    transpose_85 = view_68.transpose(1, 2);  view_68 = None
    size_122 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_k.size(0)
    size_123 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_k.size(1)
    view_69 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_k.view(size_122, size_123, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_k = size_122 = size_123 = None
    transpose_86 = view_69.transpose(1, 2);  view_69 = None
    size_124 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_v.size(0)
    size_125 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_v.size(1)
    view_70 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_v.view(size_124, size_125, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_v = size_124 = size_125 = None
    transpose_87 = view_70.transpose(1, 2);  view_70 = None
    transpose_88 = transpose_86.transpose(-2, -1);  transpose_86 = None
    matmul_34 = torch.matmul(transpose_85, transpose_88);  transpose_85 = transpose_88 = None
    mul_32 = matmul_34 * 0.125;  matmul_34 = None
    softmax_17 = torch.softmax(mul_32, dim = -1);  mul_32 = None
    matmul_35 = torch.matmul(softmax_17, transpose_87);  softmax_17 = transpose_87 = None
    transpose_89 = matmul_35.transpose(1, 2);  matmul_35 = None
    contiguous_19 = transpose_89.contiguous();  transpose_89 = None
    view_71 = contiguous_19.view(getitem_93, getitem_94, getitem_95);  contiguous_19 = getitem_93 = getitem_94 = getitem_95 = None
    down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn2.to_out, "0")(view_71);  view_71 = None
    down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_out_0 = None
    add_38 = down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_out_1 + add_37;  down_blocks_2_attentions_0_transformer_blocks_4_attn2_to_out_1 = add_37 = None
    down_blocks_2_attentions_0_transformer_blocks_4_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").norm3(add_38)
    down_blocks_2_attentions_0_transformer_blocks_4_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_4_norm3);  down_blocks_2_attentions_0_transformer_blocks_4_norm3 = None
    chunk_8 = down_blocks_2_attentions_0_transformer_blocks_4_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_4_ff_net_0_proj = None
    getitem_96 = chunk_8[0]
    getitem_97 = chunk_8[1];  chunk_8 = None
    gelu_8 = torch._C._nn.gelu(getitem_97);  getitem_97 = None
    mul_33 = getitem_96 * gelu_8;  getitem_96 = gelu_8 = None
    down_blocks_2_attentions_0_transformer_blocks_4_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").ff.net, "1")(mul_33);  mul_33 = None
    down_blocks_2_attentions_0_transformer_blocks_4_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "4").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_4_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_4_ff_net_1 = None
    add_39 = down_blocks_2_attentions_0_transformer_blocks_4_ff_net_2 + add_38;  down_blocks_2_attentions_0_transformer_blocks_4_ff_net_2 = add_38 = None
    down_blocks_2_attentions_0_transformer_blocks_5_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").norm1(add_39)
    down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_5_norm1)
    down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_5_norm1)
    down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_5_norm1);  down_blocks_2_attentions_0_transformer_blocks_5_norm1 = None
    size_126 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_q.size()
    getitem_98 = size_126[0]
    getitem_99 = size_126[1]
    getitem_100 = size_126[2];  size_126 = None
    size_127 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_q.size(0)
    size_128 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_q.size(1)
    view_72 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_q.view(size_127, size_128, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_q = size_127 = size_128 = None
    transpose_90 = view_72.transpose(1, 2);  view_72 = None
    size_129 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_k.size(0)
    size_130 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_k.size(1)
    view_73 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_k.view(size_129, size_130, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_k = size_129 = size_130 = None
    transpose_91 = view_73.transpose(1, 2);  view_73 = None
    size_131 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_v.size(0)
    size_132 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_v.size(1)
    view_74 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_v.view(size_131, size_132, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_v = size_131 = size_132 = None
    transpose_92 = view_74.transpose(1, 2);  view_74 = None
    transpose_93 = transpose_91.transpose(-2, -1);  transpose_91 = None
    matmul_36 = torch.matmul(transpose_90, transpose_93);  transpose_90 = transpose_93 = None
    mul_34 = matmul_36 * 0.125;  matmul_36 = None
    softmax_18 = torch.softmax(mul_34, dim = -1);  mul_34 = None
    matmul_37 = torch.matmul(softmax_18, transpose_92);  softmax_18 = transpose_92 = None
    transpose_94 = matmul_37.transpose(1, 2);  matmul_37 = None
    contiguous_20 = transpose_94.contiguous();  transpose_94 = None
    view_75 = contiguous_20.view(getitem_98, getitem_99, getitem_100);  contiguous_20 = getitem_98 = getitem_99 = getitem_100 = None
    down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn1.to_out, "0")(view_75);  view_75 = None
    down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_out_0 = None
    add_40 = down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_out_1 + add_39;  down_blocks_2_attentions_0_transformer_blocks_5_attn1_to_out_1 = add_39 = None
    down_blocks_2_attentions_0_transformer_blocks_5_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").norm2(add_40)
    down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_5_norm2)
    down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_5_norm2)
    down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_5_norm2);  down_blocks_2_attentions_0_transformer_blocks_5_norm2 = None
    size_133 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_q.size()
    getitem_101 = size_133[0]
    getitem_102 = size_133[1]
    getitem_103 = size_133[2];  size_133 = None
    size_134 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_q.size(0)
    size_135 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_q.size(1)
    view_76 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_q.view(size_134, size_135, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_q = size_134 = size_135 = None
    transpose_95 = view_76.transpose(1, 2);  view_76 = None
    size_136 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_k.size(0)
    size_137 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_k.size(1)
    view_77 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_k.view(size_136, size_137, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_k = size_136 = size_137 = None
    transpose_96 = view_77.transpose(1, 2);  view_77 = None
    size_138 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_v.size(0)
    size_139 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_v.size(1)
    view_78 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_v.view(size_138, size_139, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_v = size_138 = size_139 = None
    transpose_97 = view_78.transpose(1, 2);  view_78 = None
    transpose_98 = transpose_96.transpose(-2, -1);  transpose_96 = None
    matmul_38 = torch.matmul(transpose_95, transpose_98);  transpose_95 = transpose_98 = None
    mul_35 = matmul_38 * 0.125;  matmul_38 = None
    softmax_19 = torch.softmax(mul_35, dim = -1);  mul_35 = None
    matmul_39 = torch.matmul(softmax_19, transpose_97);  softmax_19 = transpose_97 = None
    transpose_99 = matmul_39.transpose(1, 2);  matmul_39 = None
    contiguous_21 = transpose_99.contiguous();  transpose_99 = None
    view_79 = contiguous_21.view(getitem_101, getitem_102, getitem_103);  contiguous_21 = getitem_101 = getitem_102 = getitem_103 = None
    down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn2.to_out, "0")(view_79);  view_79 = None
    down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_out_0 = None
    add_41 = down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_out_1 + add_40;  down_blocks_2_attentions_0_transformer_blocks_5_attn2_to_out_1 = add_40 = None
    down_blocks_2_attentions_0_transformer_blocks_5_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").norm3(add_41)
    down_blocks_2_attentions_0_transformer_blocks_5_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_5_norm3);  down_blocks_2_attentions_0_transformer_blocks_5_norm3 = None
    chunk_9 = down_blocks_2_attentions_0_transformer_blocks_5_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_5_ff_net_0_proj = None
    getitem_104 = chunk_9[0]
    getitem_105 = chunk_9[1];  chunk_9 = None
    gelu_9 = torch._C._nn.gelu(getitem_105);  getitem_105 = None
    mul_36 = getitem_104 * gelu_9;  getitem_104 = gelu_9 = None
    down_blocks_2_attentions_0_transformer_blocks_5_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").ff.net, "1")(mul_36);  mul_36 = None
    down_blocks_2_attentions_0_transformer_blocks_5_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "5").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_5_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_5_ff_net_1 = None
    add_42 = down_blocks_2_attentions_0_transformer_blocks_5_ff_net_2 + add_41;  down_blocks_2_attentions_0_transformer_blocks_5_ff_net_2 = add_41 = None
    down_blocks_2_attentions_0_transformer_blocks_6_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").norm1(add_42)
    down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_6_norm1)
    down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_6_norm1)
    down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_6_norm1);  down_blocks_2_attentions_0_transformer_blocks_6_norm1 = None
    size_140 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_q.size()
    getitem_106 = size_140[0]
    getitem_107 = size_140[1]
    getitem_108 = size_140[2];  size_140 = None
    size_141 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_q.size(0)
    size_142 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_q.size(1)
    view_80 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_q.view(size_141, size_142, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_q = size_141 = size_142 = None
    transpose_100 = view_80.transpose(1, 2);  view_80 = None
    size_143 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_k.size(0)
    size_144 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_k.size(1)
    view_81 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_k.view(size_143, size_144, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_k = size_143 = size_144 = None
    transpose_101 = view_81.transpose(1, 2);  view_81 = None
    size_145 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_v.size(0)
    size_146 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_v.size(1)
    view_82 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_v.view(size_145, size_146, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_v = size_145 = size_146 = None
    transpose_102 = view_82.transpose(1, 2);  view_82 = None
    transpose_103 = transpose_101.transpose(-2, -1);  transpose_101 = None
    matmul_40 = torch.matmul(transpose_100, transpose_103);  transpose_100 = transpose_103 = None
    mul_37 = matmul_40 * 0.125;  matmul_40 = None
    softmax_20 = torch.softmax(mul_37, dim = -1);  mul_37 = None
    matmul_41 = torch.matmul(softmax_20, transpose_102);  softmax_20 = transpose_102 = None
    transpose_104 = matmul_41.transpose(1, 2);  matmul_41 = None
    contiguous_22 = transpose_104.contiguous();  transpose_104 = None
    view_83 = contiguous_22.view(getitem_106, getitem_107, getitem_108);  contiguous_22 = getitem_106 = getitem_107 = getitem_108 = None
    down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn1.to_out, "0")(view_83);  view_83 = None
    down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_out_0 = None
    add_43 = down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_out_1 + add_42;  down_blocks_2_attentions_0_transformer_blocks_6_attn1_to_out_1 = add_42 = None
    down_blocks_2_attentions_0_transformer_blocks_6_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").norm2(add_43)
    down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_6_norm2)
    down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_6_norm2)
    down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_6_norm2);  down_blocks_2_attentions_0_transformer_blocks_6_norm2 = None
    size_147 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_q.size()
    getitem_109 = size_147[0]
    getitem_110 = size_147[1]
    getitem_111 = size_147[2];  size_147 = None
    size_148 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_q.size(0)
    size_149 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_q.size(1)
    view_84 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_q.view(size_148, size_149, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_q = size_148 = size_149 = None
    transpose_105 = view_84.transpose(1, 2);  view_84 = None
    size_150 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_k.size(0)
    size_151 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_k.size(1)
    view_85 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_k.view(size_150, size_151, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_k = size_150 = size_151 = None
    transpose_106 = view_85.transpose(1, 2);  view_85 = None
    size_152 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_v.size(0)
    size_153 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_v.size(1)
    view_86 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_v.view(size_152, size_153, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_v = size_152 = size_153 = None
    transpose_107 = view_86.transpose(1, 2);  view_86 = None
    transpose_108 = transpose_106.transpose(-2, -1);  transpose_106 = None
    matmul_42 = torch.matmul(transpose_105, transpose_108);  transpose_105 = transpose_108 = None
    mul_38 = matmul_42 * 0.125;  matmul_42 = None
    softmax_21 = torch.softmax(mul_38, dim = -1);  mul_38 = None
    matmul_43 = torch.matmul(softmax_21, transpose_107);  softmax_21 = transpose_107 = None
    transpose_109 = matmul_43.transpose(1, 2);  matmul_43 = None
    contiguous_23 = transpose_109.contiguous();  transpose_109 = None
    view_87 = contiguous_23.view(getitem_109, getitem_110, getitem_111);  contiguous_23 = getitem_109 = getitem_110 = getitem_111 = None
    down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn2.to_out, "0")(view_87);  view_87 = None
    down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_out_0 = None
    add_44 = down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_out_1 + add_43;  down_blocks_2_attentions_0_transformer_blocks_6_attn2_to_out_1 = add_43 = None
    down_blocks_2_attentions_0_transformer_blocks_6_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").norm3(add_44)
    down_blocks_2_attentions_0_transformer_blocks_6_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_6_norm3);  down_blocks_2_attentions_0_transformer_blocks_6_norm3 = None
    chunk_10 = down_blocks_2_attentions_0_transformer_blocks_6_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_6_ff_net_0_proj = None
    getitem_112 = chunk_10[0]
    getitem_113 = chunk_10[1];  chunk_10 = None
    gelu_10 = torch._C._nn.gelu(getitem_113);  getitem_113 = None
    mul_39 = getitem_112 * gelu_10;  getitem_112 = gelu_10 = None
    down_blocks_2_attentions_0_transformer_blocks_6_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").ff.net, "1")(mul_39);  mul_39 = None
    down_blocks_2_attentions_0_transformer_blocks_6_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "6").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_6_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_6_ff_net_1 = None
    add_45 = down_blocks_2_attentions_0_transformer_blocks_6_ff_net_2 + add_44;  down_blocks_2_attentions_0_transformer_blocks_6_ff_net_2 = add_44 = None
    down_blocks_2_attentions_0_transformer_blocks_7_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").norm1(add_45)
    down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_7_norm1)
    down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_7_norm1)
    down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_7_norm1);  down_blocks_2_attentions_0_transformer_blocks_7_norm1 = None
    size_154 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_q.size()
    getitem_114 = size_154[0]
    getitem_115 = size_154[1]
    getitem_116 = size_154[2];  size_154 = None
    size_155 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_q.size(0)
    size_156 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_q.size(1)
    view_88 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_q.view(size_155, size_156, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_q = size_155 = size_156 = None
    transpose_110 = view_88.transpose(1, 2);  view_88 = None
    size_157 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_k.size(0)
    size_158 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_k.size(1)
    view_89 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_k.view(size_157, size_158, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_k = size_157 = size_158 = None
    transpose_111 = view_89.transpose(1, 2);  view_89 = None
    size_159 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_v.size(0)
    size_160 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_v.size(1)
    view_90 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_v.view(size_159, size_160, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_v = size_159 = size_160 = None
    transpose_112 = view_90.transpose(1, 2);  view_90 = None
    transpose_113 = transpose_111.transpose(-2, -1);  transpose_111 = None
    matmul_44 = torch.matmul(transpose_110, transpose_113);  transpose_110 = transpose_113 = None
    mul_40 = matmul_44 * 0.125;  matmul_44 = None
    softmax_22 = torch.softmax(mul_40, dim = -1);  mul_40 = None
    matmul_45 = torch.matmul(softmax_22, transpose_112);  softmax_22 = transpose_112 = None
    transpose_114 = matmul_45.transpose(1, 2);  matmul_45 = None
    contiguous_24 = transpose_114.contiguous();  transpose_114 = None
    view_91 = contiguous_24.view(getitem_114, getitem_115, getitem_116);  contiguous_24 = getitem_114 = getitem_115 = getitem_116 = None
    down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn1.to_out, "0")(view_91);  view_91 = None
    down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_out_0 = None
    add_46 = down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_out_1 + add_45;  down_blocks_2_attentions_0_transformer_blocks_7_attn1_to_out_1 = add_45 = None
    down_blocks_2_attentions_0_transformer_blocks_7_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").norm2(add_46)
    down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_7_norm2)
    down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_7_norm2)
    down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_7_norm2);  down_blocks_2_attentions_0_transformer_blocks_7_norm2 = None
    size_161 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_q.size()
    getitem_117 = size_161[0]
    getitem_118 = size_161[1]
    getitem_119 = size_161[2];  size_161 = None
    size_162 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_q.size(0)
    size_163 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_q.size(1)
    view_92 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_q.view(size_162, size_163, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_q = size_162 = size_163 = None
    transpose_115 = view_92.transpose(1, 2);  view_92 = None
    size_164 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_k.size(0)
    size_165 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_k.size(1)
    view_93 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_k.view(size_164, size_165, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_k = size_164 = size_165 = None
    transpose_116 = view_93.transpose(1, 2);  view_93 = None
    size_166 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_v.size(0)
    size_167 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_v.size(1)
    view_94 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_v.view(size_166, size_167, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_v = size_166 = size_167 = None
    transpose_117 = view_94.transpose(1, 2);  view_94 = None
    transpose_118 = transpose_116.transpose(-2, -1);  transpose_116 = None
    matmul_46 = torch.matmul(transpose_115, transpose_118);  transpose_115 = transpose_118 = None
    mul_41 = matmul_46 * 0.125;  matmul_46 = None
    softmax_23 = torch.softmax(mul_41, dim = -1);  mul_41 = None
    matmul_47 = torch.matmul(softmax_23, transpose_117);  softmax_23 = transpose_117 = None
    transpose_119 = matmul_47.transpose(1, 2);  matmul_47 = None
    contiguous_25 = transpose_119.contiguous();  transpose_119 = None
    view_95 = contiguous_25.view(getitem_117, getitem_118, getitem_119);  contiguous_25 = getitem_117 = getitem_118 = getitem_119 = None
    down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn2.to_out, "0")(view_95);  view_95 = None
    down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_out_0 = None
    add_47 = down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_out_1 + add_46;  down_blocks_2_attentions_0_transformer_blocks_7_attn2_to_out_1 = add_46 = None
    down_blocks_2_attentions_0_transformer_blocks_7_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").norm3(add_47)
    down_blocks_2_attentions_0_transformer_blocks_7_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_7_norm3);  down_blocks_2_attentions_0_transformer_blocks_7_norm3 = None
    chunk_11 = down_blocks_2_attentions_0_transformer_blocks_7_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_7_ff_net_0_proj = None
    getitem_120 = chunk_11[0]
    getitem_121 = chunk_11[1];  chunk_11 = None
    gelu_11 = torch._C._nn.gelu(getitem_121);  getitem_121 = None
    mul_42 = getitem_120 * gelu_11;  getitem_120 = gelu_11 = None
    down_blocks_2_attentions_0_transformer_blocks_7_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").ff.net, "1")(mul_42);  mul_42 = None
    down_blocks_2_attentions_0_transformer_blocks_7_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "7").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_7_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_7_ff_net_1 = None
    add_48 = down_blocks_2_attentions_0_transformer_blocks_7_ff_net_2 + add_47;  down_blocks_2_attentions_0_transformer_blocks_7_ff_net_2 = add_47 = None
    down_blocks_2_attentions_0_transformer_blocks_8_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").norm1(add_48)
    down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_8_norm1)
    down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_8_norm1)
    down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_8_norm1);  down_blocks_2_attentions_0_transformer_blocks_8_norm1 = None
    size_168 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_q.size()
    getitem_122 = size_168[0]
    getitem_123 = size_168[1]
    getitem_124 = size_168[2];  size_168 = None
    size_169 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_q.size(0)
    size_170 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_q.size(1)
    view_96 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_q.view(size_169, size_170, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_q = size_169 = size_170 = None
    transpose_120 = view_96.transpose(1, 2);  view_96 = None
    size_171 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_k.size(0)
    size_172 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_k.size(1)
    view_97 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_k.view(size_171, size_172, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_k = size_171 = size_172 = None
    transpose_121 = view_97.transpose(1, 2);  view_97 = None
    size_173 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_v.size(0)
    size_174 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_v.size(1)
    view_98 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_v.view(size_173, size_174, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_v = size_173 = size_174 = None
    transpose_122 = view_98.transpose(1, 2);  view_98 = None
    transpose_123 = transpose_121.transpose(-2, -1);  transpose_121 = None
    matmul_48 = torch.matmul(transpose_120, transpose_123);  transpose_120 = transpose_123 = None
    mul_43 = matmul_48 * 0.125;  matmul_48 = None
    softmax_24 = torch.softmax(mul_43, dim = -1);  mul_43 = None
    matmul_49 = torch.matmul(softmax_24, transpose_122);  softmax_24 = transpose_122 = None
    transpose_124 = matmul_49.transpose(1, 2);  matmul_49 = None
    contiguous_26 = transpose_124.contiguous();  transpose_124 = None
    view_99 = contiguous_26.view(getitem_122, getitem_123, getitem_124);  contiguous_26 = getitem_122 = getitem_123 = getitem_124 = None
    down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn1.to_out, "0")(view_99);  view_99 = None
    down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_out_0 = None
    add_49 = down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_out_1 + add_48;  down_blocks_2_attentions_0_transformer_blocks_8_attn1_to_out_1 = add_48 = None
    down_blocks_2_attentions_0_transformer_blocks_8_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").norm2(add_49)
    down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_8_norm2)
    down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_8_norm2)
    down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_8_norm2);  down_blocks_2_attentions_0_transformer_blocks_8_norm2 = None
    size_175 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_q.size()
    getitem_125 = size_175[0]
    getitem_126 = size_175[1]
    getitem_127 = size_175[2];  size_175 = None
    size_176 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_q.size(0)
    size_177 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_q.size(1)
    view_100 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_q.view(size_176, size_177, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_q = size_176 = size_177 = None
    transpose_125 = view_100.transpose(1, 2);  view_100 = None
    size_178 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_k.size(0)
    size_179 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_k.size(1)
    view_101 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_k.view(size_178, size_179, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_k = size_178 = size_179 = None
    transpose_126 = view_101.transpose(1, 2);  view_101 = None
    size_180 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_v.size(0)
    size_181 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_v.size(1)
    view_102 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_v.view(size_180, size_181, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_v = size_180 = size_181 = None
    transpose_127 = view_102.transpose(1, 2);  view_102 = None
    transpose_128 = transpose_126.transpose(-2, -1);  transpose_126 = None
    matmul_50 = torch.matmul(transpose_125, transpose_128);  transpose_125 = transpose_128 = None
    mul_44 = matmul_50 * 0.125;  matmul_50 = None
    softmax_25 = torch.softmax(mul_44, dim = -1);  mul_44 = None
    matmul_51 = torch.matmul(softmax_25, transpose_127);  softmax_25 = transpose_127 = None
    transpose_129 = matmul_51.transpose(1, 2);  matmul_51 = None
    contiguous_27 = transpose_129.contiguous();  transpose_129 = None
    view_103 = contiguous_27.view(getitem_125, getitem_126, getitem_127);  contiguous_27 = getitem_125 = getitem_126 = getitem_127 = None
    down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn2.to_out, "0")(view_103);  view_103 = None
    down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_out_0 = None
    add_50 = down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_out_1 + add_49;  down_blocks_2_attentions_0_transformer_blocks_8_attn2_to_out_1 = add_49 = None
    down_blocks_2_attentions_0_transformer_blocks_8_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").norm3(add_50)
    down_blocks_2_attentions_0_transformer_blocks_8_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_8_norm3);  down_blocks_2_attentions_0_transformer_blocks_8_norm3 = None
    chunk_12 = down_blocks_2_attentions_0_transformer_blocks_8_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_8_ff_net_0_proj = None
    getitem_128 = chunk_12[0]
    getitem_129 = chunk_12[1];  chunk_12 = None
    gelu_12 = torch._C._nn.gelu(getitem_129);  getitem_129 = None
    mul_45 = getitem_128 * gelu_12;  getitem_128 = gelu_12 = None
    down_blocks_2_attentions_0_transformer_blocks_8_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").ff.net, "1")(mul_45);  mul_45 = None
    down_blocks_2_attentions_0_transformer_blocks_8_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "8").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_8_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_8_ff_net_1 = None
    add_51 = down_blocks_2_attentions_0_transformer_blocks_8_ff_net_2 + add_50;  down_blocks_2_attentions_0_transformer_blocks_8_ff_net_2 = add_50 = None
    down_blocks_2_attentions_0_transformer_blocks_9_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").norm1(add_51)
    down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn1.to_q(down_blocks_2_attentions_0_transformer_blocks_9_norm1)
    down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn1.to_k(down_blocks_2_attentions_0_transformer_blocks_9_norm1)
    down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn1.to_v(down_blocks_2_attentions_0_transformer_blocks_9_norm1);  down_blocks_2_attentions_0_transformer_blocks_9_norm1 = None
    size_182 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_q.size()
    getitem_130 = size_182[0]
    getitem_131 = size_182[1]
    getitem_132 = size_182[2];  size_182 = None
    size_183 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_q.size(0)
    size_184 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_q.size(1)
    view_104 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_q.view(size_183, size_184, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_q = size_183 = size_184 = None
    transpose_130 = view_104.transpose(1, 2);  view_104 = None
    size_185 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_k.size(0)
    size_186 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_k.size(1)
    view_105 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_k.view(size_185, size_186, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_k = size_185 = size_186 = None
    transpose_131 = view_105.transpose(1, 2);  view_105 = None
    size_187 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_v.size(0)
    size_188 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_v.size(1)
    view_106 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_v.view(size_187, size_188, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_v = size_187 = size_188 = None
    transpose_132 = view_106.transpose(1, 2);  view_106 = None
    transpose_133 = transpose_131.transpose(-2, -1);  transpose_131 = None
    matmul_52 = torch.matmul(transpose_130, transpose_133);  transpose_130 = transpose_133 = None
    mul_46 = matmul_52 * 0.125;  matmul_52 = None
    softmax_26 = torch.softmax(mul_46, dim = -1);  mul_46 = None
    matmul_53 = torch.matmul(softmax_26, transpose_132);  softmax_26 = transpose_132 = None
    transpose_134 = matmul_53.transpose(1, 2);  matmul_53 = None
    contiguous_28 = transpose_134.contiguous();  transpose_134 = None
    view_107 = contiguous_28.view(getitem_130, getitem_131, getitem_132);  contiguous_28 = getitem_130 = getitem_131 = getitem_132 = None
    down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn1.to_out, "0")(view_107);  view_107 = None
    down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn1.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_out_0 = None
    add_52 = down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_out_1 + add_51;  down_blocks_2_attentions_0_transformer_blocks_9_attn1_to_out_1 = add_51 = None
    down_blocks_2_attentions_0_transformer_blocks_9_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").norm2(add_52)
    down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn2.to_q(down_blocks_2_attentions_0_transformer_blocks_9_norm2)
    down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn2.to_k(down_blocks_2_attentions_0_transformer_blocks_9_norm2)
    down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn2.to_v(down_blocks_2_attentions_0_transformer_blocks_9_norm2);  down_blocks_2_attentions_0_transformer_blocks_9_norm2 = None
    size_189 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_q.size()
    getitem_133 = size_189[0]
    getitem_134 = size_189[1]
    getitem_135 = size_189[2];  size_189 = None
    size_190 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_q.size(0)
    size_191 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_q.size(1)
    view_108 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_q.view(size_190, size_191, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_q = size_190 = size_191 = None
    transpose_135 = view_108.transpose(1, 2);  view_108 = None
    size_192 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_k.size(0)
    size_193 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_k.size(1)
    view_109 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_k.view(size_192, size_193, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_k = size_192 = size_193 = None
    transpose_136 = view_109.transpose(1, 2);  view_109 = None
    size_194 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_v.size(0)
    size_195 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_v.size(1)
    view_110 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_v.view(size_194, size_195, 20, 64);  down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_v = size_194 = size_195 = None
    transpose_137 = view_110.transpose(1, 2);  view_110 = None
    transpose_138 = transpose_136.transpose(-2, -1);  transpose_136 = None
    matmul_54 = torch.matmul(transpose_135, transpose_138);  transpose_135 = transpose_138 = None
    mul_47 = matmul_54 * 0.125;  matmul_54 = None
    softmax_27 = torch.softmax(mul_47, dim = -1);  mul_47 = None
    matmul_55 = torch.matmul(softmax_27, transpose_137);  softmax_27 = transpose_137 = None
    transpose_139 = matmul_55.transpose(1, 2);  matmul_55 = None
    contiguous_29 = transpose_139.contiguous();  transpose_139 = None
    view_111 = contiguous_29.view(getitem_133, getitem_134, getitem_135);  contiguous_29 = getitem_133 = getitem_134 = getitem_135 = None
    down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn2.to_out, "0")(view_111);  view_111 = None
    down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").attn2.to_out, "1")(down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_out_0);  down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_out_0 = None
    add_53 = down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_out_1 + add_52;  down_blocks_2_attentions_0_transformer_blocks_9_attn2_to_out_1 = add_52 = None
    down_blocks_2_attentions_0_transformer_blocks_9_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").norm3(add_53)
    down_blocks_2_attentions_0_transformer_blocks_9_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").ff.net, "0").proj(down_blocks_2_attentions_0_transformer_blocks_9_norm3);  down_blocks_2_attentions_0_transformer_blocks_9_norm3 = None
    chunk_13 = down_blocks_2_attentions_0_transformer_blocks_9_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_0_transformer_blocks_9_ff_net_0_proj = None
    getitem_136 = chunk_13[0]
    getitem_137 = chunk_13[1];  chunk_13 = None
    gelu_13 = torch._C._nn.gelu(getitem_137);  getitem_137 = None
    mul_48 = getitem_136 * gelu_13;  getitem_136 = gelu_13 = None
    down_blocks_2_attentions_0_transformer_blocks_9_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").ff.net, "1")(mul_48);  mul_48 = None
    down_blocks_2_attentions_0_transformer_blocks_9_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "0").transformer_blocks, "9").ff.net, "2")(down_blocks_2_attentions_0_transformer_blocks_9_ff_net_1);  down_blocks_2_attentions_0_transformer_blocks_9_ff_net_1 = None
    add_54 = down_blocks_2_attentions_0_transformer_blocks_9_ff_net_2 + add_53;  down_blocks_2_attentions_0_transformer_blocks_9_ff_net_2 = add_53 = None
    down_blocks_2_attentions_0_proj_out = getattr(getattr(self.down_blocks, "2").attentions, "0").proj_out(add_54);  add_54 = None
    reshape_6 = down_blocks_2_attentions_0_proj_out.reshape(getitem_53, getitem_55, getitem_56, getitem_57);  down_blocks_2_attentions_0_proj_out = getitem_53 = getitem_55 = getitem_56 = getitem_57 = None
    permute_5 = reshape_6.permute(0, 3, 1, 2);  reshape_6 = None
    contiguous_30 = permute_5.contiguous();  permute_5 = None
    add_55 = contiguous_30 + add_24;  contiguous_30 = add_24 = None
    down_blocks_2_resnets_1_norm1 = getattr(getattr(self.down_blocks, "2").resnets, "1").norm1(add_55)
    down_blocks_2_resnets_1_nonlinearity = getattr(getattr(self.down_blocks, "2").resnets, "1").nonlinearity(down_blocks_2_resnets_1_norm1);  down_blocks_2_resnets_1_norm1 = None
    down_blocks_2_resnets_1_conv1 = getattr(getattr(self.down_blocks, "2").resnets, "1").conv1(down_blocks_2_resnets_1_nonlinearity);  down_blocks_2_resnets_1_nonlinearity = None
    down_blocks_2_resnets_1_nonlinearity_1 = getattr(getattr(self.down_blocks, "2").resnets, "1").nonlinearity(add)
    down_blocks_2_resnets_1_time_emb_proj = getattr(getattr(self.down_blocks, "2").resnets, "1").time_emb_proj(down_blocks_2_resnets_1_nonlinearity_1);  down_blocks_2_resnets_1_nonlinearity_1 = None
    getitem_138 = down_blocks_2_resnets_1_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  down_blocks_2_resnets_1_time_emb_proj = None
    add_56 = down_blocks_2_resnets_1_conv1 + getitem_138;  down_blocks_2_resnets_1_conv1 = getitem_138 = None
    down_blocks_2_resnets_1_norm2 = getattr(getattr(self.down_blocks, "2").resnets, "1").norm2(add_56);  add_56 = None
    down_blocks_2_resnets_1_nonlinearity_2 = getattr(getattr(self.down_blocks, "2").resnets, "1").nonlinearity(down_blocks_2_resnets_1_norm2);  down_blocks_2_resnets_1_norm2 = None
    down_blocks_2_resnets_1_dropout = getattr(getattr(self.down_blocks, "2").resnets, "1").dropout(down_blocks_2_resnets_1_nonlinearity_2);  down_blocks_2_resnets_1_nonlinearity_2 = None
    down_blocks_2_resnets_1_conv2 = getattr(getattr(self.down_blocks, "2").resnets, "1").conv2(down_blocks_2_resnets_1_dropout);  down_blocks_2_resnets_1_dropout = None
    add_57 = add_55 + down_blocks_2_resnets_1_conv2;  down_blocks_2_resnets_1_conv2 = None
    getattr_15 = add_57.shape
    getitem_139 = getattr_15[0]
    getitem_140 = getattr_15[1]
    getitem_141 = getattr_15[2]
    getitem_142 = getattr_15[3];  getattr_15 = None
    down_blocks_2_attentions_1_norm = getattr(getattr(self.down_blocks, "2").attentions, "1").norm(add_57)
    getattr_16 = down_blocks_2_attentions_1_norm.shape
    getitem_143 = getattr_16[1];  getattr_16 = None
    permute_6 = down_blocks_2_attentions_1_norm.permute(0, 2, 3, 1);  down_blocks_2_attentions_1_norm = None
    mul_49 = getitem_141 * getitem_142
    reshape_7 = permute_6.reshape(getitem_139, mul_49, getitem_143);  permute_6 = mul_49 = None
    down_blocks_2_attentions_1_proj_in = getattr(getattr(self.down_blocks, "2").attentions, "1").proj_in(reshape_7);  reshape_7 = None
    down_blocks_2_attentions_1_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").norm1(down_blocks_2_attentions_1_proj_in)
    down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_0_norm1)
    down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_0_norm1)
    down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_0_norm1);  down_blocks_2_attentions_1_transformer_blocks_0_norm1 = None
    size_196 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.size()
    getitem_144 = size_196[0]
    getitem_145 = size_196[1]
    getitem_146 = size_196[2];  size_196 = None
    size_197 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.size(0)
    size_198 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.size(1)
    view_112 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.view(size_197, size_198, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q = size_197 = size_198 = None
    transpose_140 = view_112.transpose(1, 2);  view_112 = None
    size_199 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.size(0)
    size_200 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.size(1)
    view_113 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.view(size_199, size_200, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k = size_199 = size_200 = None
    transpose_141 = view_113.transpose(1, 2);  view_113 = None
    size_201 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.size(0)
    size_202 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.size(1)
    view_114 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.view(size_201, size_202, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v = size_201 = size_202 = None
    transpose_142 = view_114.transpose(1, 2);  view_114 = None
    transpose_143 = transpose_141.transpose(-2, -1);  transpose_141 = None
    matmul_56 = torch.matmul(transpose_140, transpose_143);  transpose_140 = transpose_143 = None
    mul_50 = matmul_56 * 0.125;  matmul_56 = None
    softmax_28 = torch.softmax(mul_50, dim = -1);  mul_50 = None
    matmul_57 = torch.matmul(softmax_28, transpose_142);  softmax_28 = transpose_142 = None
    transpose_144 = matmul_57.transpose(1, 2);  matmul_57 = None
    contiguous_31 = transpose_144.contiguous();  transpose_144 = None
    view_115 = contiguous_31.view(getitem_144, getitem_145, getitem_146);  contiguous_31 = getitem_144 = getitem_145 = getitem_146 = None
    down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn1.to_out, "0")(view_115);  view_115 = None
    down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0 = None
    add_58 = down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_1 + down_blocks_2_attentions_1_proj_in;  down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_1 = down_blocks_2_attentions_1_proj_in = None
    down_blocks_2_attentions_1_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").norm2(add_58)
    down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_0_norm2)
    down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_0_norm2)
    down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_0_norm2);  down_blocks_2_attentions_1_transformer_blocks_0_norm2 = None
    size_203 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.size()
    getitem_147 = size_203[0]
    getitem_148 = size_203[1]
    getitem_149 = size_203[2];  size_203 = None
    size_204 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.size(0)
    size_205 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.size(1)
    view_116 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.view(size_204, size_205, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q = size_204 = size_205 = None
    transpose_145 = view_116.transpose(1, 2);  view_116 = None
    size_206 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.size(0)
    size_207 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.size(1)
    view_117 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.view(size_206, size_207, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k = size_206 = size_207 = None
    transpose_146 = view_117.transpose(1, 2);  view_117 = None
    size_208 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.size(0)
    size_209 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.size(1)
    view_118 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.view(size_208, size_209, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v = size_208 = size_209 = None
    transpose_147 = view_118.transpose(1, 2);  view_118 = None
    transpose_148 = transpose_146.transpose(-2, -1);  transpose_146 = None
    matmul_58 = torch.matmul(transpose_145, transpose_148);  transpose_145 = transpose_148 = None
    mul_51 = matmul_58 * 0.125;  matmul_58 = None
    softmax_29 = torch.softmax(mul_51, dim = -1);  mul_51 = None
    matmul_59 = torch.matmul(softmax_29, transpose_147);  softmax_29 = transpose_147 = None
    transpose_149 = matmul_59.transpose(1, 2);  matmul_59 = None
    contiguous_32 = transpose_149.contiguous();  transpose_149 = None
    view_119 = contiguous_32.view(getitem_147, getitem_148, getitem_149);  contiguous_32 = getitem_147 = getitem_148 = getitem_149 = None
    down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn2.to_out, "0")(view_119);  view_119 = None
    down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0 = None
    add_59 = down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_1 + add_58;  down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_1 = add_58 = None
    down_blocks_2_attentions_1_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").norm3(add_59)
    down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_0_norm3);  down_blocks_2_attentions_1_transformer_blocks_0_norm3 = None
    chunk_14 = down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj = None
    getitem_150 = chunk_14[0]
    getitem_151 = chunk_14[1];  chunk_14 = None
    gelu_14 = torch._C._nn.gelu(getitem_151);  getitem_151 = None
    mul_52 = getitem_150 * gelu_14;  getitem_150 = gelu_14 = None
    down_blocks_2_attentions_1_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").ff.net, "1")(mul_52);  mul_52 = None
    down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "0").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_0_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_0_ff_net_1 = None
    add_60 = down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 + add_59;  down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 = add_59 = None
    down_blocks_2_attentions_1_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").norm1(add_60)
    down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_1_norm1)
    down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_1_norm1)
    down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_1_norm1);  down_blocks_2_attentions_1_transformer_blocks_1_norm1 = None
    size_210 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_q.size()
    getitem_152 = size_210[0]
    getitem_153 = size_210[1]
    getitem_154 = size_210[2];  size_210 = None
    size_211 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_q.size(0)
    size_212 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_q.size(1)
    view_120 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_q.view(size_211, size_212, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_q = size_211 = size_212 = None
    transpose_150 = view_120.transpose(1, 2);  view_120 = None
    size_213 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_k.size(0)
    size_214 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_k.size(1)
    view_121 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_k.view(size_213, size_214, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_k = size_213 = size_214 = None
    transpose_151 = view_121.transpose(1, 2);  view_121 = None
    size_215 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_v.size(0)
    size_216 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_v.size(1)
    view_122 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_v.view(size_215, size_216, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_v = size_215 = size_216 = None
    transpose_152 = view_122.transpose(1, 2);  view_122 = None
    transpose_153 = transpose_151.transpose(-2, -1);  transpose_151 = None
    matmul_60 = torch.matmul(transpose_150, transpose_153);  transpose_150 = transpose_153 = None
    mul_53 = matmul_60 * 0.125;  matmul_60 = None
    softmax_30 = torch.softmax(mul_53, dim = -1);  mul_53 = None
    matmul_61 = torch.matmul(softmax_30, transpose_152);  softmax_30 = transpose_152 = None
    transpose_154 = matmul_61.transpose(1, 2);  matmul_61 = None
    contiguous_33 = transpose_154.contiguous();  transpose_154 = None
    view_123 = contiguous_33.view(getitem_152, getitem_153, getitem_154);  contiguous_33 = getitem_152 = getitem_153 = getitem_154 = None
    down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn1.to_out, "0")(view_123);  view_123 = None
    down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_out_0 = None
    add_61 = down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_out_1 + add_60;  down_blocks_2_attentions_1_transformer_blocks_1_attn1_to_out_1 = add_60 = None
    down_blocks_2_attentions_1_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").norm2(add_61)
    down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_1_norm2)
    down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_1_norm2)
    down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_1_norm2);  down_blocks_2_attentions_1_transformer_blocks_1_norm2 = None
    size_217 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_q.size()
    getitem_155 = size_217[0]
    getitem_156 = size_217[1]
    getitem_157 = size_217[2];  size_217 = None
    size_218 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_q.size(0)
    size_219 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_q.size(1)
    view_124 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_q.view(size_218, size_219, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_q = size_218 = size_219 = None
    transpose_155 = view_124.transpose(1, 2);  view_124 = None
    size_220 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_k.size(0)
    size_221 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_k.size(1)
    view_125 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_k.view(size_220, size_221, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_k = size_220 = size_221 = None
    transpose_156 = view_125.transpose(1, 2);  view_125 = None
    size_222 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_v.size(0)
    size_223 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_v.size(1)
    view_126 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_v.view(size_222, size_223, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_v = size_222 = size_223 = None
    transpose_157 = view_126.transpose(1, 2);  view_126 = None
    transpose_158 = transpose_156.transpose(-2, -1);  transpose_156 = None
    matmul_62 = torch.matmul(transpose_155, transpose_158);  transpose_155 = transpose_158 = None
    mul_54 = matmul_62 * 0.125;  matmul_62 = None
    softmax_31 = torch.softmax(mul_54, dim = -1);  mul_54 = None
    matmul_63 = torch.matmul(softmax_31, transpose_157);  softmax_31 = transpose_157 = None
    transpose_159 = matmul_63.transpose(1, 2);  matmul_63 = None
    contiguous_34 = transpose_159.contiguous();  transpose_159 = None
    view_127 = contiguous_34.view(getitem_155, getitem_156, getitem_157);  contiguous_34 = getitem_155 = getitem_156 = getitem_157 = None
    down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn2.to_out, "0")(view_127);  view_127 = None
    down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_out_0 = None
    add_62 = down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_out_1 + add_61;  down_blocks_2_attentions_1_transformer_blocks_1_attn2_to_out_1 = add_61 = None
    down_blocks_2_attentions_1_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").norm3(add_62)
    down_blocks_2_attentions_1_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_1_norm3);  down_blocks_2_attentions_1_transformer_blocks_1_norm3 = None
    chunk_15 = down_blocks_2_attentions_1_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_1_ff_net_0_proj = None
    getitem_158 = chunk_15[0]
    getitem_159 = chunk_15[1];  chunk_15 = None
    gelu_15 = torch._C._nn.gelu(getitem_159);  getitem_159 = None
    mul_55 = getitem_158 * gelu_15;  getitem_158 = gelu_15 = None
    down_blocks_2_attentions_1_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").ff.net, "1")(mul_55);  mul_55 = None
    down_blocks_2_attentions_1_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "1").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_1_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_1_ff_net_1 = None
    add_63 = down_blocks_2_attentions_1_transformer_blocks_1_ff_net_2 + add_62;  down_blocks_2_attentions_1_transformer_blocks_1_ff_net_2 = add_62 = None
    down_blocks_2_attentions_1_transformer_blocks_2_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").norm1(add_63)
    down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_2_norm1)
    down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_2_norm1)
    down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_2_norm1);  down_blocks_2_attentions_1_transformer_blocks_2_norm1 = None
    size_224 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_q.size()
    getitem_160 = size_224[0]
    getitem_161 = size_224[1]
    getitem_162 = size_224[2];  size_224 = None
    size_225 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_q.size(0)
    size_226 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_q.size(1)
    view_128 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_q.view(size_225, size_226, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_q = size_225 = size_226 = None
    transpose_160 = view_128.transpose(1, 2);  view_128 = None
    size_227 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_k.size(0)
    size_228 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_k.size(1)
    view_129 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_k.view(size_227, size_228, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_k = size_227 = size_228 = None
    transpose_161 = view_129.transpose(1, 2);  view_129 = None
    size_229 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_v.size(0)
    size_230 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_v.size(1)
    view_130 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_v.view(size_229, size_230, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_v = size_229 = size_230 = None
    transpose_162 = view_130.transpose(1, 2);  view_130 = None
    transpose_163 = transpose_161.transpose(-2, -1);  transpose_161 = None
    matmul_64 = torch.matmul(transpose_160, transpose_163);  transpose_160 = transpose_163 = None
    mul_56 = matmul_64 * 0.125;  matmul_64 = None
    softmax_32 = torch.softmax(mul_56, dim = -1);  mul_56 = None
    matmul_65 = torch.matmul(softmax_32, transpose_162);  softmax_32 = transpose_162 = None
    transpose_164 = matmul_65.transpose(1, 2);  matmul_65 = None
    contiguous_35 = transpose_164.contiguous();  transpose_164 = None
    view_131 = contiguous_35.view(getitem_160, getitem_161, getitem_162);  contiguous_35 = getitem_160 = getitem_161 = getitem_162 = None
    down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn1.to_out, "0")(view_131);  view_131 = None
    down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_out_0 = None
    add_64 = down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_out_1 + add_63;  down_blocks_2_attentions_1_transformer_blocks_2_attn1_to_out_1 = add_63 = None
    down_blocks_2_attentions_1_transformer_blocks_2_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").norm2(add_64)
    down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_2_norm2)
    down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_2_norm2)
    down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_2_norm2);  down_blocks_2_attentions_1_transformer_blocks_2_norm2 = None
    size_231 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_q.size()
    getitem_163 = size_231[0]
    getitem_164 = size_231[1]
    getitem_165 = size_231[2];  size_231 = None
    size_232 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_q.size(0)
    size_233 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_q.size(1)
    view_132 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_q.view(size_232, size_233, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_q = size_232 = size_233 = None
    transpose_165 = view_132.transpose(1, 2);  view_132 = None
    size_234 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_k.size(0)
    size_235 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_k.size(1)
    view_133 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_k.view(size_234, size_235, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_k = size_234 = size_235 = None
    transpose_166 = view_133.transpose(1, 2);  view_133 = None
    size_236 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_v.size(0)
    size_237 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_v.size(1)
    view_134 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_v.view(size_236, size_237, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_v = size_236 = size_237 = None
    transpose_167 = view_134.transpose(1, 2);  view_134 = None
    transpose_168 = transpose_166.transpose(-2, -1);  transpose_166 = None
    matmul_66 = torch.matmul(transpose_165, transpose_168);  transpose_165 = transpose_168 = None
    mul_57 = matmul_66 * 0.125;  matmul_66 = None
    softmax_33 = torch.softmax(mul_57, dim = -1);  mul_57 = None
    matmul_67 = torch.matmul(softmax_33, transpose_167);  softmax_33 = transpose_167 = None
    transpose_169 = matmul_67.transpose(1, 2);  matmul_67 = None
    contiguous_36 = transpose_169.contiguous();  transpose_169 = None
    view_135 = contiguous_36.view(getitem_163, getitem_164, getitem_165);  contiguous_36 = getitem_163 = getitem_164 = getitem_165 = None
    down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn2.to_out, "0")(view_135);  view_135 = None
    down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_out_0 = None
    add_65 = down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_out_1 + add_64;  down_blocks_2_attentions_1_transformer_blocks_2_attn2_to_out_1 = add_64 = None
    down_blocks_2_attentions_1_transformer_blocks_2_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").norm3(add_65)
    down_blocks_2_attentions_1_transformer_blocks_2_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_2_norm3);  down_blocks_2_attentions_1_transformer_blocks_2_norm3 = None
    chunk_16 = down_blocks_2_attentions_1_transformer_blocks_2_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_2_ff_net_0_proj = None
    getitem_166 = chunk_16[0]
    getitem_167 = chunk_16[1];  chunk_16 = None
    gelu_16 = torch._C._nn.gelu(getitem_167);  getitem_167 = None
    mul_58 = getitem_166 * gelu_16;  getitem_166 = gelu_16 = None
    down_blocks_2_attentions_1_transformer_blocks_2_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").ff.net, "1")(mul_58);  mul_58 = None
    down_blocks_2_attentions_1_transformer_blocks_2_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "2").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_2_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_2_ff_net_1 = None
    add_66 = down_blocks_2_attentions_1_transformer_blocks_2_ff_net_2 + add_65;  down_blocks_2_attentions_1_transformer_blocks_2_ff_net_2 = add_65 = None
    down_blocks_2_attentions_1_transformer_blocks_3_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").norm1(add_66)
    down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_3_norm1)
    down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_3_norm1)
    down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_3_norm1);  down_blocks_2_attentions_1_transformer_blocks_3_norm1 = None
    size_238 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_q.size()
    getitem_168 = size_238[0]
    getitem_169 = size_238[1]
    getitem_170 = size_238[2];  size_238 = None
    size_239 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_q.size(0)
    size_240 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_q.size(1)
    view_136 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_q.view(size_239, size_240, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_q = size_239 = size_240 = None
    transpose_170 = view_136.transpose(1, 2);  view_136 = None
    size_241 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_k.size(0)
    size_242 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_k.size(1)
    view_137 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_k.view(size_241, size_242, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_k = size_241 = size_242 = None
    transpose_171 = view_137.transpose(1, 2);  view_137 = None
    size_243 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_v.size(0)
    size_244 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_v.size(1)
    view_138 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_v.view(size_243, size_244, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_v = size_243 = size_244 = None
    transpose_172 = view_138.transpose(1, 2);  view_138 = None
    transpose_173 = transpose_171.transpose(-2, -1);  transpose_171 = None
    matmul_68 = torch.matmul(transpose_170, transpose_173);  transpose_170 = transpose_173 = None
    mul_59 = matmul_68 * 0.125;  matmul_68 = None
    softmax_34 = torch.softmax(mul_59, dim = -1);  mul_59 = None
    matmul_69 = torch.matmul(softmax_34, transpose_172);  softmax_34 = transpose_172 = None
    transpose_174 = matmul_69.transpose(1, 2);  matmul_69 = None
    contiguous_37 = transpose_174.contiguous();  transpose_174 = None
    view_139 = contiguous_37.view(getitem_168, getitem_169, getitem_170);  contiguous_37 = getitem_168 = getitem_169 = getitem_170 = None
    down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn1.to_out, "0")(view_139);  view_139 = None
    down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_out_0 = None
    add_67 = down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_out_1 + add_66;  down_blocks_2_attentions_1_transformer_blocks_3_attn1_to_out_1 = add_66 = None
    down_blocks_2_attentions_1_transformer_blocks_3_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").norm2(add_67)
    down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_3_norm2)
    down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_3_norm2)
    down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_3_norm2);  down_blocks_2_attentions_1_transformer_blocks_3_norm2 = None
    size_245 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_q.size()
    getitem_171 = size_245[0]
    getitem_172 = size_245[1]
    getitem_173 = size_245[2];  size_245 = None
    size_246 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_q.size(0)
    size_247 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_q.size(1)
    view_140 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_q.view(size_246, size_247, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_q = size_246 = size_247 = None
    transpose_175 = view_140.transpose(1, 2);  view_140 = None
    size_248 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_k.size(0)
    size_249 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_k.size(1)
    view_141 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_k.view(size_248, size_249, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_k = size_248 = size_249 = None
    transpose_176 = view_141.transpose(1, 2);  view_141 = None
    size_250 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_v.size(0)
    size_251 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_v.size(1)
    view_142 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_v.view(size_250, size_251, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_v = size_250 = size_251 = None
    transpose_177 = view_142.transpose(1, 2);  view_142 = None
    transpose_178 = transpose_176.transpose(-2, -1);  transpose_176 = None
    matmul_70 = torch.matmul(transpose_175, transpose_178);  transpose_175 = transpose_178 = None
    mul_60 = matmul_70 * 0.125;  matmul_70 = None
    softmax_35 = torch.softmax(mul_60, dim = -1);  mul_60 = None
    matmul_71 = torch.matmul(softmax_35, transpose_177);  softmax_35 = transpose_177 = None
    transpose_179 = matmul_71.transpose(1, 2);  matmul_71 = None
    contiguous_38 = transpose_179.contiguous();  transpose_179 = None
    view_143 = contiguous_38.view(getitem_171, getitem_172, getitem_173);  contiguous_38 = getitem_171 = getitem_172 = getitem_173 = None
    down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn2.to_out, "0")(view_143);  view_143 = None
    down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_out_0 = None
    add_68 = down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_out_1 + add_67;  down_blocks_2_attentions_1_transformer_blocks_3_attn2_to_out_1 = add_67 = None
    down_blocks_2_attentions_1_transformer_blocks_3_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").norm3(add_68)
    down_blocks_2_attentions_1_transformer_blocks_3_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_3_norm3);  down_blocks_2_attentions_1_transformer_blocks_3_norm3 = None
    chunk_17 = down_blocks_2_attentions_1_transformer_blocks_3_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_3_ff_net_0_proj = None
    getitem_174 = chunk_17[0]
    getitem_175 = chunk_17[1];  chunk_17 = None
    gelu_17 = torch._C._nn.gelu(getitem_175);  getitem_175 = None
    mul_61 = getitem_174 * gelu_17;  getitem_174 = gelu_17 = None
    down_blocks_2_attentions_1_transformer_blocks_3_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").ff.net, "1")(mul_61);  mul_61 = None
    down_blocks_2_attentions_1_transformer_blocks_3_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "3").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_3_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_3_ff_net_1 = None
    add_69 = down_blocks_2_attentions_1_transformer_blocks_3_ff_net_2 + add_68;  down_blocks_2_attentions_1_transformer_blocks_3_ff_net_2 = add_68 = None
    down_blocks_2_attentions_1_transformer_blocks_4_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").norm1(add_69)
    down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_4_norm1)
    down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_4_norm1)
    down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_4_norm1);  down_blocks_2_attentions_1_transformer_blocks_4_norm1 = None
    size_252 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_q.size()
    getitem_176 = size_252[0]
    getitem_177 = size_252[1]
    getitem_178 = size_252[2];  size_252 = None
    size_253 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_q.size(0)
    size_254 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_q.size(1)
    view_144 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_q.view(size_253, size_254, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_q = size_253 = size_254 = None
    transpose_180 = view_144.transpose(1, 2);  view_144 = None
    size_255 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_k.size(0)
    size_256 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_k.size(1)
    view_145 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_k.view(size_255, size_256, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_k = size_255 = size_256 = None
    transpose_181 = view_145.transpose(1, 2);  view_145 = None
    size_257 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_v.size(0)
    size_258 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_v.size(1)
    view_146 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_v.view(size_257, size_258, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_v = size_257 = size_258 = None
    transpose_182 = view_146.transpose(1, 2);  view_146 = None
    transpose_183 = transpose_181.transpose(-2, -1);  transpose_181 = None
    matmul_72 = torch.matmul(transpose_180, transpose_183);  transpose_180 = transpose_183 = None
    mul_62 = matmul_72 * 0.125;  matmul_72 = None
    softmax_36 = torch.softmax(mul_62, dim = -1);  mul_62 = None
    matmul_73 = torch.matmul(softmax_36, transpose_182);  softmax_36 = transpose_182 = None
    transpose_184 = matmul_73.transpose(1, 2);  matmul_73 = None
    contiguous_39 = transpose_184.contiguous();  transpose_184 = None
    view_147 = contiguous_39.view(getitem_176, getitem_177, getitem_178);  contiguous_39 = getitem_176 = getitem_177 = getitem_178 = None
    down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn1.to_out, "0")(view_147);  view_147 = None
    down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_out_0 = None
    add_70 = down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_out_1 + add_69;  down_blocks_2_attentions_1_transformer_blocks_4_attn1_to_out_1 = add_69 = None
    down_blocks_2_attentions_1_transformer_blocks_4_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").norm2(add_70)
    down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_4_norm2)
    down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_4_norm2)
    down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_4_norm2);  down_blocks_2_attentions_1_transformer_blocks_4_norm2 = None
    size_259 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_q.size()
    getitem_179 = size_259[0]
    getitem_180 = size_259[1]
    getitem_181 = size_259[2];  size_259 = None
    size_260 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_q.size(0)
    size_261 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_q.size(1)
    view_148 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_q.view(size_260, size_261, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_q = size_260 = size_261 = None
    transpose_185 = view_148.transpose(1, 2);  view_148 = None
    size_262 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_k.size(0)
    size_263 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_k.size(1)
    view_149 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_k.view(size_262, size_263, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_k = size_262 = size_263 = None
    transpose_186 = view_149.transpose(1, 2);  view_149 = None
    size_264 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_v.size(0)
    size_265 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_v.size(1)
    view_150 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_v.view(size_264, size_265, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_v = size_264 = size_265 = None
    transpose_187 = view_150.transpose(1, 2);  view_150 = None
    transpose_188 = transpose_186.transpose(-2, -1);  transpose_186 = None
    matmul_74 = torch.matmul(transpose_185, transpose_188);  transpose_185 = transpose_188 = None
    mul_63 = matmul_74 * 0.125;  matmul_74 = None
    softmax_37 = torch.softmax(mul_63, dim = -1);  mul_63 = None
    matmul_75 = torch.matmul(softmax_37, transpose_187);  softmax_37 = transpose_187 = None
    transpose_189 = matmul_75.transpose(1, 2);  matmul_75 = None
    contiguous_40 = transpose_189.contiguous();  transpose_189 = None
    view_151 = contiguous_40.view(getitem_179, getitem_180, getitem_181);  contiguous_40 = getitem_179 = getitem_180 = getitem_181 = None
    down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn2.to_out, "0")(view_151);  view_151 = None
    down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_out_0 = None
    add_71 = down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_out_1 + add_70;  down_blocks_2_attentions_1_transformer_blocks_4_attn2_to_out_1 = add_70 = None
    down_blocks_2_attentions_1_transformer_blocks_4_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").norm3(add_71)
    down_blocks_2_attentions_1_transformer_blocks_4_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_4_norm3);  down_blocks_2_attentions_1_transformer_blocks_4_norm3 = None
    chunk_18 = down_blocks_2_attentions_1_transformer_blocks_4_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_4_ff_net_0_proj = None
    getitem_182 = chunk_18[0]
    getitem_183 = chunk_18[1];  chunk_18 = None
    gelu_18 = torch._C._nn.gelu(getitem_183);  getitem_183 = None
    mul_64 = getitem_182 * gelu_18;  getitem_182 = gelu_18 = None
    down_blocks_2_attentions_1_transformer_blocks_4_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").ff.net, "1")(mul_64);  mul_64 = None
    down_blocks_2_attentions_1_transformer_blocks_4_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "4").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_4_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_4_ff_net_1 = None
    add_72 = down_blocks_2_attentions_1_transformer_blocks_4_ff_net_2 + add_71;  down_blocks_2_attentions_1_transformer_blocks_4_ff_net_2 = add_71 = None
    down_blocks_2_attentions_1_transformer_blocks_5_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").norm1(add_72)
    down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_5_norm1)
    down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_5_norm1)
    down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_5_norm1);  down_blocks_2_attentions_1_transformer_blocks_5_norm1 = None
    size_266 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_q.size()
    getitem_184 = size_266[0]
    getitem_185 = size_266[1]
    getitem_186 = size_266[2];  size_266 = None
    size_267 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_q.size(0)
    size_268 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_q.size(1)
    view_152 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_q.view(size_267, size_268, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_q = size_267 = size_268 = None
    transpose_190 = view_152.transpose(1, 2);  view_152 = None
    size_269 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_k.size(0)
    size_270 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_k.size(1)
    view_153 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_k.view(size_269, size_270, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_k = size_269 = size_270 = None
    transpose_191 = view_153.transpose(1, 2);  view_153 = None
    size_271 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_v.size(0)
    size_272 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_v.size(1)
    view_154 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_v.view(size_271, size_272, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_v = size_271 = size_272 = None
    transpose_192 = view_154.transpose(1, 2);  view_154 = None
    transpose_193 = transpose_191.transpose(-2, -1);  transpose_191 = None
    matmul_76 = torch.matmul(transpose_190, transpose_193);  transpose_190 = transpose_193 = None
    mul_65 = matmul_76 * 0.125;  matmul_76 = None
    softmax_38 = torch.softmax(mul_65, dim = -1);  mul_65 = None
    matmul_77 = torch.matmul(softmax_38, transpose_192);  softmax_38 = transpose_192 = None
    transpose_194 = matmul_77.transpose(1, 2);  matmul_77 = None
    contiguous_41 = transpose_194.contiguous();  transpose_194 = None
    view_155 = contiguous_41.view(getitem_184, getitem_185, getitem_186);  contiguous_41 = getitem_184 = getitem_185 = getitem_186 = None
    down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn1.to_out, "0")(view_155);  view_155 = None
    down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_out_0 = None
    add_73 = down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_out_1 + add_72;  down_blocks_2_attentions_1_transformer_blocks_5_attn1_to_out_1 = add_72 = None
    down_blocks_2_attentions_1_transformer_blocks_5_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").norm2(add_73)
    down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_5_norm2)
    down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_5_norm2)
    down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_5_norm2);  down_blocks_2_attentions_1_transformer_blocks_5_norm2 = None
    size_273 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_q.size()
    getitem_187 = size_273[0]
    getitem_188 = size_273[1]
    getitem_189 = size_273[2];  size_273 = None
    size_274 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_q.size(0)
    size_275 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_q.size(1)
    view_156 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_q.view(size_274, size_275, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_q = size_274 = size_275 = None
    transpose_195 = view_156.transpose(1, 2);  view_156 = None
    size_276 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_k.size(0)
    size_277 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_k.size(1)
    view_157 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_k.view(size_276, size_277, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_k = size_276 = size_277 = None
    transpose_196 = view_157.transpose(1, 2);  view_157 = None
    size_278 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_v.size(0)
    size_279 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_v.size(1)
    view_158 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_v.view(size_278, size_279, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_v = size_278 = size_279 = None
    transpose_197 = view_158.transpose(1, 2);  view_158 = None
    transpose_198 = transpose_196.transpose(-2, -1);  transpose_196 = None
    matmul_78 = torch.matmul(transpose_195, transpose_198);  transpose_195 = transpose_198 = None
    mul_66 = matmul_78 * 0.125;  matmul_78 = None
    softmax_39 = torch.softmax(mul_66, dim = -1);  mul_66 = None
    matmul_79 = torch.matmul(softmax_39, transpose_197);  softmax_39 = transpose_197 = None
    transpose_199 = matmul_79.transpose(1, 2);  matmul_79 = None
    contiguous_42 = transpose_199.contiguous();  transpose_199 = None
    view_159 = contiguous_42.view(getitem_187, getitem_188, getitem_189);  contiguous_42 = getitem_187 = getitem_188 = getitem_189 = None
    down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn2.to_out, "0")(view_159);  view_159 = None
    down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_out_0 = None
    add_74 = down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_out_1 + add_73;  down_blocks_2_attentions_1_transformer_blocks_5_attn2_to_out_1 = add_73 = None
    down_blocks_2_attentions_1_transformer_blocks_5_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").norm3(add_74)
    down_blocks_2_attentions_1_transformer_blocks_5_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_5_norm3);  down_blocks_2_attentions_1_transformer_blocks_5_norm3 = None
    chunk_19 = down_blocks_2_attentions_1_transformer_blocks_5_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_5_ff_net_0_proj = None
    getitem_190 = chunk_19[0]
    getitem_191 = chunk_19[1];  chunk_19 = None
    gelu_19 = torch._C._nn.gelu(getitem_191);  getitem_191 = None
    mul_67 = getitem_190 * gelu_19;  getitem_190 = gelu_19 = None
    down_blocks_2_attentions_1_transformer_blocks_5_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").ff.net, "1")(mul_67);  mul_67 = None
    down_blocks_2_attentions_1_transformer_blocks_5_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "5").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_5_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_5_ff_net_1 = None
    add_75 = down_blocks_2_attentions_1_transformer_blocks_5_ff_net_2 + add_74;  down_blocks_2_attentions_1_transformer_blocks_5_ff_net_2 = add_74 = None
    down_blocks_2_attentions_1_transformer_blocks_6_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").norm1(add_75)
    down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_6_norm1)
    down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_6_norm1)
    down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_6_norm1);  down_blocks_2_attentions_1_transformer_blocks_6_norm1 = None
    size_280 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_q.size()
    getitem_192 = size_280[0]
    getitem_193 = size_280[1]
    getitem_194 = size_280[2];  size_280 = None
    size_281 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_q.size(0)
    size_282 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_q.size(1)
    view_160 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_q.view(size_281, size_282, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_q = size_281 = size_282 = None
    transpose_200 = view_160.transpose(1, 2);  view_160 = None
    size_283 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_k.size(0)
    size_284 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_k.size(1)
    view_161 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_k.view(size_283, size_284, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_k = size_283 = size_284 = None
    transpose_201 = view_161.transpose(1, 2);  view_161 = None
    size_285 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_v.size(0)
    size_286 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_v.size(1)
    view_162 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_v.view(size_285, size_286, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_v = size_285 = size_286 = None
    transpose_202 = view_162.transpose(1, 2);  view_162 = None
    transpose_203 = transpose_201.transpose(-2, -1);  transpose_201 = None
    matmul_80 = torch.matmul(transpose_200, transpose_203);  transpose_200 = transpose_203 = None
    mul_68 = matmul_80 * 0.125;  matmul_80 = None
    softmax_40 = torch.softmax(mul_68, dim = -1);  mul_68 = None
    matmul_81 = torch.matmul(softmax_40, transpose_202);  softmax_40 = transpose_202 = None
    transpose_204 = matmul_81.transpose(1, 2);  matmul_81 = None
    contiguous_43 = transpose_204.contiguous();  transpose_204 = None
    view_163 = contiguous_43.view(getitem_192, getitem_193, getitem_194);  contiguous_43 = getitem_192 = getitem_193 = getitem_194 = None
    down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn1.to_out, "0")(view_163);  view_163 = None
    down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_out_0 = None
    add_76 = down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_out_1 + add_75;  down_blocks_2_attentions_1_transformer_blocks_6_attn1_to_out_1 = add_75 = None
    down_blocks_2_attentions_1_transformer_blocks_6_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").norm2(add_76)
    down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_6_norm2)
    down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_6_norm2)
    down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_6_norm2);  down_blocks_2_attentions_1_transformer_blocks_6_norm2 = None
    size_287 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_q.size()
    getitem_195 = size_287[0]
    getitem_196 = size_287[1]
    getitem_197 = size_287[2];  size_287 = None
    size_288 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_q.size(0)
    size_289 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_q.size(1)
    view_164 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_q.view(size_288, size_289, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_q = size_288 = size_289 = None
    transpose_205 = view_164.transpose(1, 2);  view_164 = None
    size_290 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_k.size(0)
    size_291 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_k.size(1)
    view_165 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_k.view(size_290, size_291, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_k = size_290 = size_291 = None
    transpose_206 = view_165.transpose(1, 2);  view_165 = None
    size_292 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_v.size(0)
    size_293 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_v.size(1)
    view_166 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_v.view(size_292, size_293, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_v = size_292 = size_293 = None
    transpose_207 = view_166.transpose(1, 2);  view_166 = None
    transpose_208 = transpose_206.transpose(-2, -1);  transpose_206 = None
    matmul_82 = torch.matmul(transpose_205, transpose_208);  transpose_205 = transpose_208 = None
    mul_69 = matmul_82 * 0.125;  matmul_82 = None
    softmax_41 = torch.softmax(mul_69, dim = -1);  mul_69 = None
    matmul_83 = torch.matmul(softmax_41, transpose_207);  softmax_41 = transpose_207 = None
    transpose_209 = matmul_83.transpose(1, 2);  matmul_83 = None
    contiguous_44 = transpose_209.contiguous();  transpose_209 = None
    view_167 = contiguous_44.view(getitem_195, getitem_196, getitem_197);  contiguous_44 = getitem_195 = getitem_196 = getitem_197 = None
    down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn2.to_out, "0")(view_167);  view_167 = None
    down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_out_0 = None
    add_77 = down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_out_1 + add_76;  down_blocks_2_attentions_1_transformer_blocks_6_attn2_to_out_1 = add_76 = None
    down_blocks_2_attentions_1_transformer_blocks_6_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").norm3(add_77)
    down_blocks_2_attentions_1_transformer_blocks_6_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_6_norm3);  down_blocks_2_attentions_1_transformer_blocks_6_norm3 = None
    chunk_20 = down_blocks_2_attentions_1_transformer_blocks_6_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_6_ff_net_0_proj = None
    getitem_198 = chunk_20[0]
    getitem_199 = chunk_20[1];  chunk_20 = None
    gelu_20 = torch._C._nn.gelu(getitem_199);  getitem_199 = None
    mul_70 = getitem_198 * gelu_20;  getitem_198 = gelu_20 = None
    down_blocks_2_attentions_1_transformer_blocks_6_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").ff.net, "1")(mul_70);  mul_70 = None
    down_blocks_2_attentions_1_transformer_blocks_6_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "6").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_6_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_6_ff_net_1 = None
    add_78 = down_blocks_2_attentions_1_transformer_blocks_6_ff_net_2 + add_77;  down_blocks_2_attentions_1_transformer_blocks_6_ff_net_2 = add_77 = None
    down_blocks_2_attentions_1_transformer_blocks_7_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").norm1(add_78)
    down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_7_norm1)
    down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_7_norm1)
    down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_7_norm1);  down_blocks_2_attentions_1_transformer_blocks_7_norm1 = None
    size_294 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_q.size()
    getitem_200 = size_294[0]
    getitem_201 = size_294[1]
    getitem_202 = size_294[2];  size_294 = None
    size_295 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_q.size(0)
    size_296 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_q.size(1)
    view_168 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_q.view(size_295, size_296, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_q = size_295 = size_296 = None
    transpose_210 = view_168.transpose(1, 2);  view_168 = None
    size_297 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_k.size(0)
    size_298 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_k.size(1)
    view_169 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_k.view(size_297, size_298, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_k = size_297 = size_298 = None
    transpose_211 = view_169.transpose(1, 2);  view_169 = None
    size_299 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_v.size(0)
    size_300 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_v.size(1)
    view_170 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_v.view(size_299, size_300, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_v = size_299 = size_300 = None
    transpose_212 = view_170.transpose(1, 2);  view_170 = None
    transpose_213 = transpose_211.transpose(-2, -1);  transpose_211 = None
    matmul_84 = torch.matmul(transpose_210, transpose_213);  transpose_210 = transpose_213 = None
    mul_71 = matmul_84 * 0.125;  matmul_84 = None
    softmax_42 = torch.softmax(mul_71, dim = -1);  mul_71 = None
    matmul_85 = torch.matmul(softmax_42, transpose_212);  softmax_42 = transpose_212 = None
    transpose_214 = matmul_85.transpose(1, 2);  matmul_85 = None
    contiguous_45 = transpose_214.contiguous();  transpose_214 = None
    view_171 = contiguous_45.view(getitem_200, getitem_201, getitem_202);  contiguous_45 = getitem_200 = getitem_201 = getitem_202 = None
    down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn1.to_out, "0")(view_171);  view_171 = None
    down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_out_0 = None
    add_79 = down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_out_1 + add_78;  down_blocks_2_attentions_1_transformer_blocks_7_attn1_to_out_1 = add_78 = None
    down_blocks_2_attentions_1_transformer_blocks_7_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").norm2(add_79)
    down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_7_norm2)
    down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_7_norm2)
    down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_7_norm2);  down_blocks_2_attentions_1_transformer_blocks_7_norm2 = None
    size_301 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_q.size()
    getitem_203 = size_301[0]
    getitem_204 = size_301[1]
    getitem_205 = size_301[2];  size_301 = None
    size_302 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_q.size(0)
    size_303 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_q.size(1)
    view_172 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_q.view(size_302, size_303, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_q = size_302 = size_303 = None
    transpose_215 = view_172.transpose(1, 2);  view_172 = None
    size_304 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_k.size(0)
    size_305 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_k.size(1)
    view_173 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_k.view(size_304, size_305, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_k = size_304 = size_305 = None
    transpose_216 = view_173.transpose(1, 2);  view_173 = None
    size_306 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_v.size(0)
    size_307 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_v.size(1)
    view_174 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_v.view(size_306, size_307, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_v = size_306 = size_307 = None
    transpose_217 = view_174.transpose(1, 2);  view_174 = None
    transpose_218 = transpose_216.transpose(-2, -1);  transpose_216 = None
    matmul_86 = torch.matmul(transpose_215, transpose_218);  transpose_215 = transpose_218 = None
    mul_72 = matmul_86 * 0.125;  matmul_86 = None
    softmax_43 = torch.softmax(mul_72, dim = -1);  mul_72 = None
    matmul_87 = torch.matmul(softmax_43, transpose_217);  softmax_43 = transpose_217 = None
    transpose_219 = matmul_87.transpose(1, 2);  matmul_87 = None
    contiguous_46 = transpose_219.contiguous();  transpose_219 = None
    view_175 = contiguous_46.view(getitem_203, getitem_204, getitem_205);  contiguous_46 = getitem_203 = getitem_204 = getitem_205 = None
    down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn2.to_out, "0")(view_175);  view_175 = None
    down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_out_0 = None
    add_80 = down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_out_1 + add_79;  down_blocks_2_attentions_1_transformer_blocks_7_attn2_to_out_1 = add_79 = None
    down_blocks_2_attentions_1_transformer_blocks_7_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").norm3(add_80)
    down_blocks_2_attentions_1_transformer_blocks_7_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_7_norm3);  down_blocks_2_attentions_1_transformer_blocks_7_norm3 = None
    chunk_21 = down_blocks_2_attentions_1_transformer_blocks_7_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_7_ff_net_0_proj = None
    getitem_206 = chunk_21[0]
    getitem_207 = chunk_21[1];  chunk_21 = None
    gelu_21 = torch._C._nn.gelu(getitem_207);  getitem_207 = None
    mul_73 = getitem_206 * gelu_21;  getitem_206 = gelu_21 = None
    down_blocks_2_attentions_1_transformer_blocks_7_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").ff.net, "1")(mul_73);  mul_73 = None
    down_blocks_2_attentions_1_transformer_blocks_7_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "7").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_7_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_7_ff_net_1 = None
    add_81 = down_blocks_2_attentions_1_transformer_blocks_7_ff_net_2 + add_80;  down_blocks_2_attentions_1_transformer_blocks_7_ff_net_2 = add_80 = None
    down_blocks_2_attentions_1_transformer_blocks_8_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").norm1(add_81)
    down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_8_norm1)
    down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_8_norm1)
    down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_8_norm1);  down_blocks_2_attentions_1_transformer_blocks_8_norm1 = None
    size_308 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_q.size()
    getitem_208 = size_308[0]
    getitem_209 = size_308[1]
    getitem_210 = size_308[2];  size_308 = None
    size_309 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_q.size(0)
    size_310 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_q.size(1)
    view_176 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_q.view(size_309, size_310, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_q = size_309 = size_310 = None
    transpose_220 = view_176.transpose(1, 2);  view_176 = None
    size_311 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_k.size(0)
    size_312 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_k.size(1)
    view_177 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_k.view(size_311, size_312, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_k = size_311 = size_312 = None
    transpose_221 = view_177.transpose(1, 2);  view_177 = None
    size_313 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_v.size(0)
    size_314 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_v.size(1)
    view_178 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_v.view(size_313, size_314, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_v = size_313 = size_314 = None
    transpose_222 = view_178.transpose(1, 2);  view_178 = None
    transpose_223 = transpose_221.transpose(-2, -1);  transpose_221 = None
    matmul_88 = torch.matmul(transpose_220, transpose_223);  transpose_220 = transpose_223 = None
    mul_74 = matmul_88 * 0.125;  matmul_88 = None
    softmax_44 = torch.softmax(mul_74, dim = -1);  mul_74 = None
    matmul_89 = torch.matmul(softmax_44, transpose_222);  softmax_44 = transpose_222 = None
    transpose_224 = matmul_89.transpose(1, 2);  matmul_89 = None
    contiguous_47 = transpose_224.contiguous();  transpose_224 = None
    view_179 = contiguous_47.view(getitem_208, getitem_209, getitem_210);  contiguous_47 = getitem_208 = getitem_209 = getitem_210 = None
    down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn1.to_out, "0")(view_179);  view_179 = None
    down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_out_0 = None
    add_82 = down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_out_1 + add_81;  down_blocks_2_attentions_1_transformer_blocks_8_attn1_to_out_1 = add_81 = None
    down_blocks_2_attentions_1_transformer_blocks_8_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").norm2(add_82)
    down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_8_norm2)
    down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_8_norm2)
    down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_8_norm2);  down_blocks_2_attentions_1_transformer_blocks_8_norm2 = None
    size_315 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_q.size()
    getitem_211 = size_315[0]
    getitem_212 = size_315[1]
    getitem_213 = size_315[2];  size_315 = None
    size_316 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_q.size(0)
    size_317 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_q.size(1)
    view_180 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_q.view(size_316, size_317, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_q = size_316 = size_317 = None
    transpose_225 = view_180.transpose(1, 2);  view_180 = None
    size_318 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_k.size(0)
    size_319 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_k.size(1)
    view_181 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_k.view(size_318, size_319, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_k = size_318 = size_319 = None
    transpose_226 = view_181.transpose(1, 2);  view_181 = None
    size_320 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_v.size(0)
    size_321 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_v.size(1)
    view_182 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_v.view(size_320, size_321, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_v = size_320 = size_321 = None
    transpose_227 = view_182.transpose(1, 2);  view_182 = None
    transpose_228 = transpose_226.transpose(-2, -1);  transpose_226 = None
    matmul_90 = torch.matmul(transpose_225, transpose_228);  transpose_225 = transpose_228 = None
    mul_75 = matmul_90 * 0.125;  matmul_90 = None
    softmax_45 = torch.softmax(mul_75, dim = -1);  mul_75 = None
    matmul_91 = torch.matmul(softmax_45, transpose_227);  softmax_45 = transpose_227 = None
    transpose_229 = matmul_91.transpose(1, 2);  matmul_91 = None
    contiguous_48 = transpose_229.contiguous();  transpose_229 = None
    view_183 = contiguous_48.view(getitem_211, getitem_212, getitem_213);  contiguous_48 = getitem_211 = getitem_212 = getitem_213 = None
    down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn2.to_out, "0")(view_183);  view_183 = None
    down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_out_0 = None
    add_83 = down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_out_1 + add_82;  down_blocks_2_attentions_1_transformer_blocks_8_attn2_to_out_1 = add_82 = None
    down_blocks_2_attentions_1_transformer_blocks_8_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").norm3(add_83)
    down_blocks_2_attentions_1_transformer_blocks_8_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_8_norm3);  down_blocks_2_attentions_1_transformer_blocks_8_norm3 = None
    chunk_22 = down_blocks_2_attentions_1_transformer_blocks_8_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_8_ff_net_0_proj = None
    getitem_214 = chunk_22[0]
    getitem_215 = chunk_22[1];  chunk_22 = None
    gelu_22 = torch._C._nn.gelu(getitem_215);  getitem_215 = None
    mul_76 = getitem_214 * gelu_22;  getitem_214 = gelu_22 = None
    down_blocks_2_attentions_1_transformer_blocks_8_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").ff.net, "1")(mul_76);  mul_76 = None
    down_blocks_2_attentions_1_transformer_blocks_8_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "8").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_8_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_8_ff_net_1 = None
    add_84 = down_blocks_2_attentions_1_transformer_blocks_8_ff_net_2 + add_83;  down_blocks_2_attentions_1_transformer_blocks_8_ff_net_2 = add_83 = None
    down_blocks_2_attentions_1_transformer_blocks_9_norm1 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").norm1(add_84)
    down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn1.to_q(down_blocks_2_attentions_1_transformer_blocks_9_norm1)
    down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn1.to_k(down_blocks_2_attentions_1_transformer_blocks_9_norm1)
    down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn1.to_v(down_blocks_2_attentions_1_transformer_blocks_9_norm1);  down_blocks_2_attentions_1_transformer_blocks_9_norm1 = None
    size_322 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_q.size()
    getitem_216 = size_322[0]
    getitem_217 = size_322[1]
    getitem_218 = size_322[2];  size_322 = None
    size_323 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_q.size(0)
    size_324 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_q.size(1)
    view_184 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_q.view(size_323, size_324, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_q = size_323 = size_324 = None
    transpose_230 = view_184.transpose(1, 2);  view_184 = None
    size_325 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_k.size(0)
    size_326 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_k.size(1)
    view_185 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_k.view(size_325, size_326, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_k = size_325 = size_326 = None
    transpose_231 = view_185.transpose(1, 2);  view_185 = None
    size_327 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_v.size(0)
    size_328 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_v.size(1)
    view_186 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_v.view(size_327, size_328, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_v = size_327 = size_328 = None
    transpose_232 = view_186.transpose(1, 2);  view_186 = None
    transpose_233 = transpose_231.transpose(-2, -1);  transpose_231 = None
    matmul_92 = torch.matmul(transpose_230, transpose_233);  transpose_230 = transpose_233 = None
    mul_77 = matmul_92 * 0.125;  matmul_92 = None
    softmax_46 = torch.softmax(mul_77, dim = -1);  mul_77 = None
    matmul_93 = torch.matmul(softmax_46, transpose_232);  softmax_46 = transpose_232 = None
    transpose_234 = matmul_93.transpose(1, 2);  matmul_93 = None
    contiguous_49 = transpose_234.contiguous();  transpose_234 = None
    view_187 = contiguous_49.view(getitem_216, getitem_217, getitem_218);  contiguous_49 = getitem_216 = getitem_217 = getitem_218 = None
    down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn1.to_out, "0")(view_187);  view_187 = None
    down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn1.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_out_0 = None
    add_85 = down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_out_1 + add_84;  down_blocks_2_attentions_1_transformer_blocks_9_attn1_to_out_1 = add_84 = None
    down_blocks_2_attentions_1_transformer_blocks_9_norm2 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").norm2(add_85)
    down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_q = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn2.to_q(down_blocks_2_attentions_1_transformer_blocks_9_norm2)
    down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_k = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn2.to_k(down_blocks_2_attentions_1_transformer_blocks_9_norm2)
    down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_v = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn2.to_v(down_blocks_2_attentions_1_transformer_blocks_9_norm2);  down_blocks_2_attentions_1_transformer_blocks_9_norm2 = None
    size_329 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_q.size()
    getitem_219 = size_329[0]
    getitem_220 = size_329[1]
    getitem_221 = size_329[2];  size_329 = None
    size_330 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_q.size(0)
    size_331 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_q.size(1)
    view_188 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_q.view(size_330, size_331, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_q = size_330 = size_331 = None
    transpose_235 = view_188.transpose(1, 2);  view_188 = None
    size_332 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_k.size(0)
    size_333 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_k.size(1)
    view_189 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_k.view(size_332, size_333, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_k = size_332 = size_333 = None
    transpose_236 = view_189.transpose(1, 2);  view_189 = None
    size_334 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_v.size(0)
    size_335 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_v.size(1)
    view_190 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_v.view(size_334, size_335, 20, 64);  down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_v = size_334 = size_335 = None
    transpose_237 = view_190.transpose(1, 2);  view_190 = None
    transpose_238 = transpose_236.transpose(-2, -1);  transpose_236 = None
    matmul_94 = torch.matmul(transpose_235, transpose_238);  transpose_235 = transpose_238 = None
    mul_78 = matmul_94 * 0.125;  matmul_94 = None
    softmax_47 = torch.softmax(mul_78, dim = -1);  mul_78 = None
    matmul_95 = torch.matmul(softmax_47, transpose_237);  softmax_47 = transpose_237 = None
    transpose_239 = matmul_95.transpose(1, 2);  matmul_95 = None
    contiguous_50 = transpose_239.contiguous();  transpose_239 = None
    view_191 = contiguous_50.view(getitem_219, getitem_220, getitem_221);  contiguous_50 = getitem_219 = getitem_220 = getitem_221 = None
    down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn2.to_out, "0")(view_191);  view_191 = None
    down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").attn2.to_out, "1")(down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_out_0);  down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_out_0 = None
    add_86 = down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_out_1 + add_85;  down_blocks_2_attentions_1_transformer_blocks_9_attn2_to_out_1 = add_85 = None
    down_blocks_2_attentions_1_transformer_blocks_9_norm3 = getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").norm3(add_86)
    down_blocks_2_attentions_1_transformer_blocks_9_ff_net_0_proj = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").ff.net, "0").proj(down_blocks_2_attentions_1_transformer_blocks_9_norm3);  down_blocks_2_attentions_1_transformer_blocks_9_norm3 = None
    chunk_23 = down_blocks_2_attentions_1_transformer_blocks_9_ff_net_0_proj.chunk(2, dim = -1);  down_blocks_2_attentions_1_transformer_blocks_9_ff_net_0_proj = None
    getitem_222 = chunk_23[0]
    getitem_223 = chunk_23[1];  chunk_23 = None
    gelu_23 = torch._C._nn.gelu(getitem_223);  getitem_223 = None
    mul_79 = getitem_222 * gelu_23;  getitem_222 = gelu_23 = None
    down_blocks_2_attentions_1_transformer_blocks_9_ff_net_1 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").ff.net, "1")(mul_79);  mul_79 = None
    down_blocks_2_attentions_1_transformer_blocks_9_ff_net_2 = getattr(getattr(getattr(getattr(self.down_blocks, "2").attentions, "1").transformer_blocks, "9").ff.net, "2")(down_blocks_2_attentions_1_transformer_blocks_9_ff_net_1);  down_blocks_2_attentions_1_transformer_blocks_9_ff_net_1 = None
    add_87 = down_blocks_2_attentions_1_transformer_blocks_9_ff_net_2 + add_86;  down_blocks_2_attentions_1_transformer_blocks_9_ff_net_2 = add_86 = None
    down_blocks_2_attentions_1_proj_out = getattr(getattr(self.down_blocks, "2").attentions, "1").proj_out(add_87);  add_87 = None
    reshape_8 = down_blocks_2_attentions_1_proj_out.reshape(getitem_139, getitem_141, getitem_142, getitem_143);  down_blocks_2_attentions_1_proj_out = getitem_139 = getitem_141 = getitem_142 = getitem_143 = None
    permute_7 = reshape_8.permute(0, 3, 1, 2);  reshape_8 = None
    contiguous_51 = permute_7.contiguous();  permute_7 = None
    add_88 = contiguous_51 + add_57;  contiguous_51 = add_57 = None
    mid_block_resnets_0_norm1 = getattr(self.mid_block.resnets, "0").norm1(add_88)
    mid_block_resnets_0_nonlinearity = getattr(self.mid_block.resnets, "0").nonlinearity(mid_block_resnets_0_norm1);  mid_block_resnets_0_norm1 = None
    mid_block_resnets_0_conv1 = getattr(self.mid_block.resnets, "0").conv1(mid_block_resnets_0_nonlinearity);  mid_block_resnets_0_nonlinearity = None
    mid_block_resnets_0_nonlinearity_1 = getattr(self.mid_block.resnets, "0").nonlinearity(add)
    mid_block_resnets_0_time_emb_proj = getattr(self.mid_block.resnets, "0").time_emb_proj(mid_block_resnets_0_nonlinearity_1);  mid_block_resnets_0_nonlinearity_1 = None
    getitem_224 = mid_block_resnets_0_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  mid_block_resnets_0_time_emb_proj = None
    add_89 = mid_block_resnets_0_conv1 + getitem_224;  mid_block_resnets_0_conv1 = getitem_224 = None
    mid_block_resnets_0_norm2 = getattr(self.mid_block.resnets, "0").norm2(add_89);  add_89 = None
    mid_block_resnets_0_nonlinearity_2 = getattr(self.mid_block.resnets, "0").nonlinearity(mid_block_resnets_0_norm2);  mid_block_resnets_0_norm2 = None
    mid_block_resnets_0_dropout = getattr(self.mid_block.resnets, "0").dropout(mid_block_resnets_0_nonlinearity_2);  mid_block_resnets_0_nonlinearity_2 = None
    mid_block_resnets_0_conv2 = getattr(self.mid_block.resnets, "0").conv2(mid_block_resnets_0_dropout);  mid_block_resnets_0_dropout = None
    add_90 = add_88 + mid_block_resnets_0_conv2;  mid_block_resnets_0_conv2 = None
    getattr_17 = add_90.shape
    getitem_225 = getattr_17[0]
    getitem_226 = getattr_17[1]
    getitem_227 = getattr_17[2]
    getitem_228 = getattr_17[3];  getattr_17 = None
    mid_block_attentions_0_norm = getattr(self.mid_block.attentions, "0").norm(add_90)
    getattr_18 = mid_block_attentions_0_norm.shape
    getitem_229 = getattr_18[1];  getattr_18 = None
    permute_8 = mid_block_attentions_0_norm.permute(0, 2, 3, 1);  mid_block_attentions_0_norm = None
    mul_80 = getitem_227 * getitem_228
    reshape_9 = permute_8.reshape(getitem_225, mul_80, getitem_229);  permute_8 = mul_80 = None
    mid_block_attentions_0_proj_in = getattr(self.mid_block.attentions, "0").proj_in(reshape_9);  reshape_9 = None
    mid_block_attentions_0_transformer_blocks_0_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").norm1(mid_block_attentions_0_proj_in)
    mid_block_attentions_0_transformer_blocks_0_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn1.to_q(mid_block_attentions_0_transformer_blocks_0_norm1)
    mid_block_attentions_0_transformer_blocks_0_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn1.to_k(mid_block_attentions_0_transformer_blocks_0_norm1)
    mid_block_attentions_0_transformer_blocks_0_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn1.to_v(mid_block_attentions_0_transformer_blocks_0_norm1);  mid_block_attentions_0_transformer_blocks_0_norm1 = None
    size_336 = mid_block_attentions_0_transformer_blocks_0_attn1_to_q.size()
    getitem_230 = size_336[0]
    getitem_231 = size_336[1]
    getitem_232 = size_336[2];  size_336 = None
    size_337 = mid_block_attentions_0_transformer_blocks_0_attn1_to_q.size(0)
    size_338 = mid_block_attentions_0_transformer_blocks_0_attn1_to_q.size(1)
    view_192 = mid_block_attentions_0_transformer_blocks_0_attn1_to_q.view(size_337, size_338, 20, 64);  mid_block_attentions_0_transformer_blocks_0_attn1_to_q = size_337 = size_338 = None
    transpose_240 = view_192.transpose(1, 2);  view_192 = None
    size_339 = mid_block_attentions_0_transformer_blocks_0_attn1_to_k.size(0)
    size_340 = mid_block_attentions_0_transformer_blocks_0_attn1_to_k.size(1)
    view_193 = mid_block_attentions_0_transformer_blocks_0_attn1_to_k.view(size_339, size_340, 20, 64);  mid_block_attentions_0_transformer_blocks_0_attn1_to_k = size_339 = size_340 = None
    transpose_241 = view_193.transpose(1, 2);  view_193 = None
    size_341 = mid_block_attentions_0_transformer_blocks_0_attn1_to_v.size(0)
    size_342 = mid_block_attentions_0_transformer_blocks_0_attn1_to_v.size(1)
    view_194 = mid_block_attentions_0_transformer_blocks_0_attn1_to_v.view(size_341, size_342, 20, 64);  mid_block_attentions_0_transformer_blocks_0_attn1_to_v = size_341 = size_342 = None
    transpose_242 = view_194.transpose(1, 2);  view_194 = None
    transpose_243 = transpose_241.transpose(-2, -1);  transpose_241 = None
    matmul_96 = torch.matmul(transpose_240, transpose_243);  transpose_240 = transpose_243 = None
    mul_81 = matmul_96 * 0.125;  matmul_96 = None
    softmax_48 = torch.softmax(mul_81, dim = -1);  mul_81 = None
    matmul_97 = torch.matmul(softmax_48, transpose_242);  softmax_48 = transpose_242 = None
    transpose_244 = matmul_97.transpose(1, 2);  matmul_97 = None
    contiguous_52 = transpose_244.contiguous();  transpose_244 = None
    view_195 = contiguous_52.view(getitem_230, getitem_231, getitem_232);  contiguous_52 = getitem_230 = getitem_231 = getitem_232 = None
    mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn1.to_out, "0")(view_195);  view_195 = None
    mid_block_attentions_0_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0 = None
    add_91 = mid_block_attentions_0_transformer_blocks_0_attn1_to_out_1 + mid_block_attentions_0_proj_in;  mid_block_attentions_0_transformer_blocks_0_attn1_to_out_1 = mid_block_attentions_0_proj_in = None
    mid_block_attentions_0_transformer_blocks_0_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").norm2(add_91)
    mid_block_attentions_0_transformer_blocks_0_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn2.to_q(mid_block_attentions_0_transformer_blocks_0_norm2)
    mid_block_attentions_0_transformer_blocks_0_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn2.to_k(mid_block_attentions_0_transformer_blocks_0_norm2)
    mid_block_attentions_0_transformer_blocks_0_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn2.to_v(mid_block_attentions_0_transformer_blocks_0_norm2);  mid_block_attentions_0_transformer_blocks_0_norm2 = None
    size_343 = mid_block_attentions_0_transformer_blocks_0_attn2_to_q.size()
    getitem_233 = size_343[0]
    getitem_234 = size_343[1]
    getitem_235 = size_343[2];  size_343 = None
    size_344 = mid_block_attentions_0_transformer_blocks_0_attn2_to_q.size(0)
    size_345 = mid_block_attentions_0_transformer_blocks_0_attn2_to_q.size(1)
    view_196 = mid_block_attentions_0_transformer_blocks_0_attn2_to_q.view(size_344, size_345, 20, 64);  mid_block_attentions_0_transformer_blocks_0_attn2_to_q = size_344 = size_345 = None
    transpose_245 = view_196.transpose(1, 2);  view_196 = None
    size_346 = mid_block_attentions_0_transformer_blocks_0_attn2_to_k.size(0)
    size_347 = mid_block_attentions_0_transformer_blocks_0_attn2_to_k.size(1)
    view_197 = mid_block_attentions_0_transformer_blocks_0_attn2_to_k.view(size_346, size_347, 20, 64);  mid_block_attentions_0_transformer_blocks_0_attn2_to_k = size_346 = size_347 = None
    transpose_246 = view_197.transpose(1, 2);  view_197 = None
    size_348 = mid_block_attentions_0_transformer_blocks_0_attn2_to_v.size(0)
    size_349 = mid_block_attentions_0_transformer_blocks_0_attn2_to_v.size(1)
    view_198 = mid_block_attentions_0_transformer_blocks_0_attn2_to_v.view(size_348, size_349, 20, 64);  mid_block_attentions_0_transformer_blocks_0_attn2_to_v = size_348 = size_349 = None
    transpose_247 = view_198.transpose(1, 2);  view_198 = None
    transpose_248 = transpose_246.transpose(-2, -1);  transpose_246 = None
    matmul_98 = torch.matmul(transpose_245, transpose_248);  transpose_245 = transpose_248 = None
    mul_82 = matmul_98 * 0.125;  matmul_98 = None
    softmax_49 = torch.softmax(mul_82, dim = -1);  mul_82 = None
    matmul_99 = torch.matmul(softmax_49, transpose_247);  softmax_49 = transpose_247 = None
    transpose_249 = matmul_99.transpose(1, 2);  matmul_99 = None
    contiguous_53 = transpose_249.contiguous();  transpose_249 = None
    view_199 = contiguous_53.view(getitem_233, getitem_234, getitem_235);  contiguous_53 = getitem_233 = getitem_234 = getitem_235 = None
    mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn2.to_out, "0")(view_199);  view_199 = None
    mid_block_attentions_0_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0 = None
    add_92 = mid_block_attentions_0_transformer_blocks_0_attn2_to_out_1 + add_91;  mid_block_attentions_0_transformer_blocks_0_attn2_to_out_1 = add_91 = None
    mid_block_attentions_0_transformer_blocks_0_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").norm3(add_92)
    mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_0_norm3);  mid_block_attentions_0_transformer_blocks_0_norm3 = None
    chunk_24 = mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj = None
    getitem_236 = chunk_24[0]
    getitem_237 = chunk_24[1];  chunk_24 = None
    gelu_24 = torch._C._nn.gelu(getitem_237);  getitem_237 = None
    mul_83 = getitem_236 * gelu_24;  getitem_236 = gelu_24 = None
    mid_block_attentions_0_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").ff.net, "1")(mul_83);  mul_83 = None
    mid_block_attentions_0_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "0").ff.net, "2")(mid_block_attentions_0_transformer_blocks_0_ff_net_1);  mid_block_attentions_0_transformer_blocks_0_ff_net_1 = None
    add_93 = mid_block_attentions_0_transformer_blocks_0_ff_net_2 + add_92;  mid_block_attentions_0_transformer_blocks_0_ff_net_2 = add_92 = None
    mid_block_attentions_0_transformer_blocks_1_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").norm1(add_93)
    mid_block_attentions_0_transformer_blocks_1_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn1.to_q(mid_block_attentions_0_transformer_blocks_1_norm1)
    mid_block_attentions_0_transformer_blocks_1_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn1.to_k(mid_block_attentions_0_transformer_blocks_1_norm1)
    mid_block_attentions_0_transformer_blocks_1_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn1.to_v(mid_block_attentions_0_transformer_blocks_1_norm1);  mid_block_attentions_0_transformer_blocks_1_norm1 = None
    size_350 = mid_block_attentions_0_transformer_blocks_1_attn1_to_q.size()
    getitem_238 = size_350[0]
    getitem_239 = size_350[1]
    getitem_240 = size_350[2];  size_350 = None
    size_351 = mid_block_attentions_0_transformer_blocks_1_attn1_to_q.size(0)
    size_352 = mid_block_attentions_0_transformer_blocks_1_attn1_to_q.size(1)
    view_200 = mid_block_attentions_0_transformer_blocks_1_attn1_to_q.view(size_351, size_352, 20, 64);  mid_block_attentions_0_transformer_blocks_1_attn1_to_q = size_351 = size_352 = None
    transpose_250 = view_200.transpose(1, 2);  view_200 = None
    size_353 = mid_block_attentions_0_transformer_blocks_1_attn1_to_k.size(0)
    size_354 = mid_block_attentions_0_transformer_blocks_1_attn1_to_k.size(1)
    view_201 = mid_block_attentions_0_transformer_blocks_1_attn1_to_k.view(size_353, size_354, 20, 64);  mid_block_attentions_0_transformer_blocks_1_attn1_to_k = size_353 = size_354 = None
    transpose_251 = view_201.transpose(1, 2);  view_201 = None
    size_355 = mid_block_attentions_0_transformer_blocks_1_attn1_to_v.size(0)
    size_356 = mid_block_attentions_0_transformer_blocks_1_attn1_to_v.size(1)
    view_202 = mid_block_attentions_0_transformer_blocks_1_attn1_to_v.view(size_355, size_356, 20, 64);  mid_block_attentions_0_transformer_blocks_1_attn1_to_v = size_355 = size_356 = None
    transpose_252 = view_202.transpose(1, 2);  view_202 = None
    transpose_253 = transpose_251.transpose(-2, -1);  transpose_251 = None
    matmul_100 = torch.matmul(transpose_250, transpose_253);  transpose_250 = transpose_253 = None
    mul_84 = matmul_100 * 0.125;  matmul_100 = None
    softmax_50 = torch.softmax(mul_84, dim = -1);  mul_84 = None
    matmul_101 = torch.matmul(softmax_50, transpose_252);  softmax_50 = transpose_252 = None
    transpose_254 = matmul_101.transpose(1, 2);  matmul_101 = None
    contiguous_54 = transpose_254.contiguous();  transpose_254 = None
    view_203 = contiguous_54.view(getitem_238, getitem_239, getitem_240);  contiguous_54 = getitem_238 = getitem_239 = getitem_240 = None
    mid_block_attentions_0_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn1.to_out, "0")(view_203);  view_203 = None
    mid_block_attentions_0_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_1_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_1_attn1_to_out_0 = None
    add_94 = mid_block_attentions_0_transformer_blocks_1_attn1_to_out_1 + add_93;  mid_block_attentions_0_transformer_blocks_1_attn1_to_out_1 = add_93 = None
    mid_block_attentions_0_transformer_blocks_1_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").norm2(add_94)
    mid_block_attentions_0_transformer_blocks_1_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn2.to_q(mid_block_attentions_0_transformer_blocks_1_norm2)
    mid_block_attentions_0_transformer_blocks_1_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn2.to_k(mid_block_attentions_0_transformer_blocks_1_norm2)
    mid_block_attentions_0_transformer_blocks_1_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn2.to_v(mid_block_attentions_0_transformer_blocks_1_norm2);  mid_block_attentions_0_transformer_blocks_1_norm2 = None
    size_357 = mid_block_attentions_0_transformer_blocks_1_attn2_to_q.size()
    getitem_241 = size_357[0]
    getitem_242 = size_357[1]
    getitem_243 = size_357[2];  size_357 = None
    size_358 = mid_block_attentions_0_transformer_blocks_1_attn2_to_q.size(0)
    size_359 = mid_block_attentions_0_transformer_blocks_1_attn2_to_q.size(1)
    view_204 = mid_block_attentions_0_transformer_blocks_1_attn2_to_q.view(size_358, size_359, 20, 64);  mid_block_attentions_0_transformer_blocks_1_attn2_to_q = size_358 = size_359 = None
    transpose_255 = view_204.transpose(1, 2);  view_204 = None
    size_360 = mid_block_attentions_0_transformer_blocks_1_attn2_to_k.size(0)
    size_361 = mid_block_attentions_0_transformer_blocks_1_attn2_to_k.size(1)
    view_205 = mid_block_attentions_0_transformer_blocks_1_attn2_to_k.view(size_360, size_361, 20, 64);  mid_block_attentions_0_transformer_blocks_1_attn2_to_k = size_360 = size_361 = None
    transpose_256 = view_205.transpose(1, 2);  view_205 = None
    size_362 = mid_block_attentions_0_transformer_blocks_1_attn2_to_v.size(0)
    size_363 = mid_block_attentions_0_transformer_blocks_1_attn2_to_v.size(1)
    view_206 = mid_block_attentions_0_transformer_blocks_1_attn2_to_v.view(size_362, size_363, 20, 64);  mid_block_attentions_0_transformer_blocks_1_attn2_to_v = size_362 = size_363 = None
    transpose_257 = view_206.transpose(1, 2);  view_206 = None
    transpose_258 = transpose_256.transpose(-2, -1);  transpose_256 = None
    matmul_102 = torch.matmul(transpose_255, transpose_258);  transpose_255 = transpose_258 = None
    mul_85 = matmul_102 * 0.125;  matmul_102 = None
    softmax_51 = torch.softmax(mul_85, dim = -1);  mul_85 = None
    matmul_103 = torch.matmul(softmax_51, transpose_257);  softmax_51 = transpose_257 = None
    transpose_259 = matmul_103.transpose(1, 2);  matmul_103 = None
    contiguous_55 = transpose_259.contiguous();  transpose_259 = None
    view_207 = contiguous_55.view(getitem_241, getitem_242, getitem_243);  contiguous_55 = getitem_241 = getitem_242 = getitem_243 = None
    mid_block_attentions_0_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn2.to_out, "0")(view_207);  view_207 = None
    mid_block_attentions_0_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_1_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_1_attn2_to_out_0 = None
    add_95 = mid_block_attentions_0_transformer_blocks_1_attn2_to_out_1 + add_94;  mid_block_attentions_0_transformer_blocks_1_attn2_to_out_1 = add_94 = None
    mid_block_attentions_0_transformer_blocks_1_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").norm3(add_95)
    mid_block_attentions_0_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_1_norm3);  mid_block_attentions_0_transformer_blocks_1_norm3 = None
    chunk_25 = mid_block_attentions_0_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_1_ff_net_0_proj = None
    getitem_244 = chunk_25[0]
    getitem_245 = chunk_25[1];  chunk_25 = None
    gelu_25 = torch._C._nn.gelu(getitem_245);  getitem_245 = None
    mul_86 = getitem_244 * gelu_25;  getitem_244 = gelu_25 = None
    mid_block_attentions_0_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").ff.net, "1")(mul_86);  mul_86 = None
    mid_block_attentions_0_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "1").ff.net, "2")(mid_block_attentions_0_transformer_blocks_1_ff_net_1);  mid_block_attentions_0_transformer_blocks_1_ff_net_1 = None
    add_96 = mid_block_attentions_0_transformer_blocks_1_ff_net_2 + add_95;  mid_block_attentions_0_transformer_blocks_1_ff_net_2 = add_95 = None
    mid_block_attentions_0_transformer_blocks_2_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").norm1(add_96)
    mid_block_attentions_0_transformer_blocks_2_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn1.to_q(mid_block_attentions_0_transformer_blocks_2_norm1)
    mid_block_attentions_0_transformer_blocks_2_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn1.to_k(mid_block_attentions_0_transformer_blocks_2_norm1)
    mid_block_attentions_0_transformer_blocks_2_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn1.to_v(mid_block_attentions_0_transformer_blocks_2_norm1);  mid_block_attentions_0_transformer_blocks_2_norm1 = None
    size_364 = mid_block_attentions_0_transformer_blocks_2_attn1_to_q.size()
    getitem_246 = size_364[0]
    getitem_247 = size_364[1]
    getitem_248 = size_364[2];  size_364 = None
    size_365 = mid_block_attentions_0_transformer_blocks_2_attn1_to_q.size(0)
    size_366 = mid_block_attentions_0_transformer_blocks_2_attn1_to_q.size(1)
    view_208 = mid_block_attentions_0_transformer_blocks_2_attn1_to_q.view(size_365, size_366, 20, 64);  mid_block_attentions_0_transformer_blocks_2_attn1_to_q = size_365 = size_366 = None
    transpose_260 = view_208.transpose(1, 2);  view_208 = None
    size_367 = mid_block_attentions_0_transformer_blocks_2_attn1_to_k.size(0)
    size_368 = mid_block_attentions_0_transformer_blocks_2_attn1_to_k.size(1)
    view_209 = mid_block_attentions_0_transformer_blocks_2_attn1_to_k.view(size_367, size_368, 20, 64);  mid_block_attentions_0_transformer_blocks_2_attn1_to_k = size_367 = size_368 = None
    transpose_261 = view_209.transpose(1, 2);  view_209 = None
    size_369 = mid_block_attentions_0_transformer_blocks_2_attn1_to_v.size(0)
    size_370 = mid_block_attentions_0_transformer_blocks_2_attn1_to_v.size(1)
    view_210 = mid_block_attentions_0_transformer_blocks_2_attn1_to_v.view(size_369, size_370, 20, 64);  mid_block_attentions_0_transformer_blocks_2_attn1_to_v = size_369 = size_370 = None
    transpose_262 = view_210.transpose(1, 2);  view_210 = None
    transpose_263 = transpose_261.transpose(-2, -1);  transpose_261 = None
    matmul_104 = torch.matmul(transpose_260, transpose_263);  transpose_260 = transpose_263 = None
    mul_87 = matmul_104 * 0.125;  matmul_104 = None
    softmax_52 = torch.softmax(mul_87, dim = -1);  mul_87 = None
    matmul_105 = torch.matmul(softmax_52, transpose_262);  softmax_52 = transpose_262 = None
    transpose_264 = matmul_105.transpose(1, 2);  matmul_105 = None
    contiguous_56 = transpose_264.contiguous();  transpose_264 = None
    view_211 = contiguous_56.view(getitem_246, getitem_247, getitem_248);  contiguous_56 = getitem_246 = getitem_247 = getitem_248 = None
    mid_block_attentions_0_transformer_blocks_2_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn1.to_out, "0")(view_211);  view_211 = None
    mid_block_attentions_0_transformer_blocks_2_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_2_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_2_attn1_to_out_0 = None
    add_97 = mid_block_attentions_0_transformer_blocks_2_attn1_to_out_1 + add_96;  mid_block_attentions_0_transformer_blocks_2_attn1_to_out_1 = add_96 = None
    mid_block_attentions_0_transformer_blocks_2_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").norm2(add_97)
    mid_block_attentions_0_transformer_blocks_2_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn2.to_q(mid_block_attentions_0_transformer_blocks_2_norm2)
    mid_block_attentions_0_transformer_blocks_2_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn2.to_k(mid_block_attentions_0_transformer_blocks_2_norm2)
    mid_block_attentions_0_transformer_blocks_2_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn2.to_v(mid_block_attentions_0_transformer_blocks_2_norm2);  mid_block_attentions_0_transformer_blocks_2_norm2 = None
    size_371 = mid_block_attentions_0_transformer_blocks_2_attn2_to_q.size()
    getitem_249 = size_371[0]
    getitem_250 = size_371[1]
    getitem_251 = size_371[2];  size_371 = None
    size_372 = mid_block_attentions_0_transformer_blocks_2_attn2_to_q.size(0)
    size_373 = mid_block_attentions_0_transformer_blocks_2_attn2_to_q.size(1)
    view_212 = mid_block_attentions_0_transformer_blocks_2_attn2_to_q.view(size_372, size_373, 20, 64);  mid_block_attentions_0_transformer_blocks_2_attn2_to_q = size_372 = size_373 = None
    transpose_265 = view_212.transpose(1, 2);  view_212 = None
    size_374 = mid_block_attentions_0_transformer_blocks_2_attn2_to_k.size(0)
    size_375 = mid_block_attentions_0_transformer_blocks_2_attn2_to_k.size(1)
    view_213 = mid_block_attentions_0_transformer_blocks_2_attn2_to_k.view(size_374, size_375, 20, 64);  mid_block_attentions_0_transformer_blocks_2_attn2_to_k = size_374 = size_375 = None
    transpose_266 = view_213.transpose(1, 2);  view_213 = None
    size_376 = mid_block_attentions_0_transformer_blocks_2_attn2_to_v.size(0)
    size_377 = mid_block_attentions_0_transformer_blocks_2_attn2_to_v.size(1)
    view_214 = mid_block_attentions_0_transformer_blocks_2_attn2_to_v.view(size_376, size_377, 20, 64);  mid_block_attentions_0_transformer_blocks_2_attn2_to_v = size_376 = size_377 = None
    transpose_267 = view_214.transpose(1, 2);  view_214 = None
    transpose_268 = transpose_266.transpose(-2, -1);  transpose_266 = None
    matmul_106 = torch.matmul(transpose_265, transpose_268);  transpose_265 = transpose_268 = None
    mul_88 = matmul_106 * 0.125;  matmul_106 = None
    softmax_53 = torch.softmax(mul_88, dim = -1);  mul_88 = None
    matmul_107 = torch.matmul(softmax_53, transpose_267);  softmax_53 = transpose_267 = None
    transpose_269 = matmul_107.transpose(1, 2);  matmul_107 = None
    contiguous_57 = transpose_269.contiguous();  transpose_269 = None
    view_215 = contiguous_57.view(getitem_249, getitem_250, getitem_251);  contiguous_57 = getitem_249 = getitem_250 = getitem_251 = None
    mid_block_attentions_0_transformer_blocks_2_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn2.to_out, "0")(view_215);  view_215 = None
    mid_block_attentions_0_transformer_blocks_2_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_2_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_2_attn2_to_out_0 = None
    add_98 = mid_block_attentions_0_transformer_blocks_2_attn2_to_out_1 + add_97;  mid_block_attentions_0_transformer_blocks_2_attn2_to_out_1 = add_97 = None
    mid_block_attentions_0_transformer_blocks_2_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").norm3(add_98)
    mid_block_attentions_0_transformer_blocks_2_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_2_norm3);  mid_block_attentions_0_transformer_blocks_2_norm3 = None
    chunk_26 = mid_block_attentions_0_transformer_blocks_2_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_2_ff_net_0_proj = None
    getitem_252 = chunk_26[0]
    getitem_253 = chunk_26[1];  chunk_26 = None
    gelu_26 = torch._C._nn.gelu(getitem_253);  getitem_253 = None
    mul_89 = getitem_252 * gelu_26;  getitem_252 = gelu_26 = None
    mid_block_attentions_0_transformer_blocks_2_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").ff.net, "1")(mul_89);  mul_89 = None
    mid_block_attentions_0_transformer_blocks_2_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "2").ff.net, "2")(mid_block_attentions_0_transformer_blocks_2_ff_net_1);  mid_block_attentions_0_transformer_blocks_2_ff_net_1 = None
    add_99 = mid_block_attentions_0_transformer_blocks_2_ff_net_2 + add_98;  mid_block_attentions_0_transformer_blocks_2_ff_net_2 = add_98 = None
    mid_block_attentions_0_transformer_blocks_3_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").norm1(add_99)
    mid_block_attentions_0_transformer_blocks_3_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn1.to_q(mid_block_attentions_0_transformer_blocks_3_norm1)
    mid_block_attentions_0_transformer_blocks_3_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn1.to_k(mid_block_attentions_0_transformer_blocks_3_norm1)
    mid_block_attentions_0_transformer_blocks_3_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn1.to_v(mid_block_attentions_0_transformer_blocks_3_norm1);  mid_block_attentions_0_transformer_blocks_3_norm1 = None
    size_378 = mid_block_attentions_0_transformer_blocks_3_attn1_to_q.size()
    getitem_254 = size_378[0]
    getitem_255 = size_378[1]
    getitem_256 = size_378[2];  size_378 = None
    size_379 = mid_block_attentions_0_transformer_blocks_3_attn1_to_q.size(0)
    size_380 = mid_block_attentions_0_transformer_blocks_3_attn1_to_q.size(1)
    view_216 = mid_block_attentions_0_transformer_blocks_3_attn1_to_q.view(size_379, size_380, 20, 64);  mid_block_attentions_0_transformer_blocks_3_attn1_to_q = size_379 = size_380 = None
    transpose_270 = view_216.transpose(1, 2);  view_216 = None
    size_381 = mid_block_attentions_0_transformer_blocks_3_attn1_to_k.size(0)
    size_382 = mid_block_attentions_0_transformer_blocks_3_attn1_to_k.size(1)
    view_217 = mid_block_attentions_0_transformer_blocks_3_attn1_to_k.view(size_381, size_382, 20, 64);  mid_block_attentions_0_transformer_blocks_3_attn1_to_k = size_381 = size_382 = None
    transpose_271 = view_217.transpose(1, 2);  view_217 = None
    size_383 = mid_block_attentions_0_transformer_blocks_3_attn1_to_v.size(0)
    size_384 = mid_block_attentions_0_transformer_blocks_3_attn1_to_v.size(1)
    view_218 = mid_block_attentions_0_transformer_blocks_3_attn1_to_v.view(size_383, size_384, 20, 64);  mid_block_attentions_0_transformer_blocks_3_attn1_to_v = size_383 = size_384 = None
    transpose_272 = view_218.transpose(1, 2);  view_218 = None
    transpose_273 = transpose_271.transpose(-2, -1);  transpose_271 = None
    matmul_108 = torch.matmul(transpose_270, transpose_273);  transpose_270 = transpose_273 = None
    mul_90 = matmul_108 * 0.125;  matmul_108 = None
    softmax_54 = torch.softmax(mul_90, dim = -1);  mul_90 = None
    matmul_109 = torch.matmul(softmax_54, transpose_272);  softmax_54 = transpose_272 = None
    transpose_274 = matmul_109.transpose(1, 2);  matmul_109 = None
    contiguous_58 = transpose_274.contiguous();  transpose_274 = None
    view_219 = contiguous_58.view(getitem_254, getitem_255, getitem_256);  contiguous_58 = getitem_254 = getitem_255 = getitem_256 = None
    mid_block_attentions_0_transformer_blocks_3_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn1.to_out, "0")(view_219);  view_219 = None
    mid_block_attentions_0_transformer_blocks_3_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_3_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_3_attn1_to_out_0 = None
    add_100 = mid_block_attentions_0_transformer_blocks_3_attn1_to_out_1 + add_99;  mid_block_attentions_0_transformer_blocks_3_attn1_to_out_1 = add_99 = None
    mid_block_attentions_0_transformer_blocks_3_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").norm2(add_100)
    mid_block_attentions_0_transformer_blocks_3_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn2.to_q(mid_block_attentions_0_transformer_blocks_3_norm2)
    mid_block_attentions_0_transformer_blocks_3_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn2.to_k(mid_block_attentions_0_transformer_blocks_3_norm2)
    mid_block_attentions_0_transformer_blocks_3_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn2.to_v(mid_block_attentions_0_transformer_blocks_3_norm2);  mid_block_attentions_0_transformer_blocks_3_norm2 = None
    size_385 = mid_block_attentions_0_transformer_blocks_3_attn2_to_q.size()
    getitem_257 = size_385[0]
    getitem_258 = size_385[1]
    getitem_259 = size_385[2];  size_385 = None
    size_386 = mid_block_attentions_0_transformer_blocks_3_attn2_to_q.size(0)
    size_387 = mid_block_attentions_0_transformer_blocks_3_attn2_to_q.size(1)
    view_220 = mid_block_attentions_0_transformer_blocks_3_attn2_to_q.view(size_386, size_387, 20, 64);  mid_block_attentions_0_transformer_blocks_3_attn2_to_q = size_386 = size_387 = None
    transpose_275 = view_220.transpose(1, 2);  view_220 = None
    size_388 = mid_block_attentions_0_transformer_blocks_3_attn2_to_k.size(0)
    size_389 = mid_block_attentions_0_transformer_blocks_3_attn2_to_k.size(1)
    view_221 = mid_block_attentions_0_transformer_blocks_3_attn2_to_k.view(size_388, size_389, 20, 64);  mid_block_attentions_0_transformer_blocks_3_attn2_to_k = size_388 = size_389 = None
    transpose_276 = view_221.transpose(1, 2);  view_221 = None
    size_390 = mid_block_attentions_0_transformer_blocks_3_attn2_to_v.size(0)
    size_391 = mid_block_attentions_0_transformer_blocks_3_attn2_to_v.size(1)
    view_222 = mid_block_attentions_0_transformer_blocks_3_attn2_to_v.view(size_390, size_391, 20, 64);  mid_block_attentions_0_transformer_blocks_3_attn2_to_v = size_390 = size_391 = None
    transpose_277 = view_222.transpose(1, 2);  view_222 = None
    transpose_278 = transpose_276.transpose(-2, -1);  transpose_276 = None
    matmul_110 = torch.matmul(transpose_275, transpose_278);  transpose_275 = transpose_278 = None
    mul_91 = matmul_110 * 0.125;  matmul_110 = None
    softmax_55 = torch.softmax(mul_91, dim = -1);  mul_91 = None
    matmul_111 = torch.matmul(softmax_55, transpose_277);  softmax_55 = transpose_277 = None
    transpose_279 = matmul_111.transpose(1, 2);  matmul_111 = None
    contiguous_59 = transpose_279.contiguous();  transpose_279 = None
    view_223 = contiguous_59.view(getitem_257, getitem_258, getitem_259);  contiguous_59 = getitem_257 = getitem_258 = getitem_259 = None
    mid_block_attentions_0_transformer_blocks_3_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn2.to_out, "0")(view_223);  view_223 = None
    mid_block_attentions_0_transformer_blocks_3_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_3_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_3_attn2_to_out_0 = None
    add_101 = mid_block_attentions_0_transformer_blocks_3_attn2_to_out_1 + add_100;  mid_block_attentions_0_transformer_blocks_3_attn2_to_out_1 = add_100 = None
    mid_block_attentions_0_transformer_blocks_3_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").norm3(add_101)
    mid_block_attentions_0_transformer_blocks_3_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_3_norm3);  mid_block_attentions_0_transformer_blocks_3_norm3 = None
    chunk_27 = mid_block_attentions_0_transformer_blocks_3_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_3_ff_net_0_proj = None
    getitem_260 = chunk_27[0]
    getitem_261 = chunk_27[1];  chunk_27 = None
    gelu_27 = torch._C._nn.gelu(getitem_261);  getitem_261 = None
    mul_92 = getitem_260 * gelu_27;  getitem_260 = gelu_27 = None
    mid_block_attentions_0_transformer_blocks_3_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").ff.net, "1")(mul_92);  mul_92 = None
    mid_block_attentions_0_transformer_blocks_3_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "3").ff.net, "2")(mid_block_attentions_0_transformer_blocks_3_ff_net_1);  mid_block_attentions_0_transformer_blocks_3_ff_net_1 = None
    add_102 = mid_block_attentions_0_transformer_blocks_3_ff_net_2 + add_101;  mid_block_attentions_0_transformer_blocks_3_ff_net_2 = add_101 = None
    mid_block_attentions_0_transformer_blocks_4_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").norm1(add_102)
    mid_block_attentions_0_transformer_blocks_4_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn1.to_q(mid_block_attentions_0_transformer_blocks_4_norm1)
    mid_block_attentions_0_transformer_blocks_4_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn1.to_k(mid_block_attentions_0_transformer_blocks_4_norm1)
    mid_block_attentions_0_transformer_blocks_4_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn1.to_v(mid_block_attentions_0_transformer_blocks_4_norm1);  mid_block_attentions_0_transformer_blocks_4_norm1 = None
    size_392 = mid_block_attentions_0_transformer_blocks_4_attn1_to_q.size()
    getitem_262 = size_392[0]
    getitem_263 = size_392[1]
    getitem_264 = size_392[2];  size_392 = None
    size_393 = mid_block_attentions_0_transformer_blocks_4_attn1_to_q.size(0)
    size_394 = mid_block_attentions_0_transformer_blocks_4_attn1_to_q.size(1)
    view_224 = mid_block_attentions_0_transformer_blocks_4_attn1_to_q.view(size_393, size_394, 20, 64);  mid_block_attentions_0_transformer_blocks_4_attn1_to_q = size_393 = size_394 = None
    transpose_280 = view_224.transpose(1, 2);  view_224 = None
    size_395 = mid_block_attentions_0_transformer_blocks_4_attn1_to_k.size(0)
    size_396 = mid_block_attentions_0_transformer_blocks_4_attn1_to_k.size(1)
    view_225 = mid_block_attentions_0_transformer_blocks_4_attn1_to_k.view(size_395, size_396, 20, 64);  mid_block_attentions_0_transformer_blocks_4_attn1_to_k = size_395 = size_396 = None
    transpose_281 = view_225.transpose(1, 2);  view_225 = None
    size_397 = mid_block_attentions_0_transformer_blocks_4_attn1_to_v.size(0)
    size_398 = mid_block_attentions_0_transformer_blocks_4_attn1_to_v.size(1)
    view_226 = mid_block_attentions_0_transformer_blocks_4_attn1_to_v.view(size_397, size_398, 20, 64);  mid_block_attentions_0_transformer_blocks_4_attn1_to_v = size_397 = size_398 = None
    transpose_282 = view_226.transpose(1, 2);  view_226 = None
    transpose_283 = transpose_281.transpose(-2, -1);  transpose_281 = None
    matmul_112 = torch.matmul(transpose_280, transpose_283);  transpose_280 = transpose_283 = None
    mul_93 = matmul_112 * 0.125;  matmul_112 = None
    softmax_56 = torch.softmax(mul_93, dim = -1);  mul_93 = None
    matmul_113 = torch.matmul(softmax_56, transpose_282);  softmax_56 = transpose_282 = None
    transpose_284 = matmul_113.transpose(1, 2);  matmul_113 = None
    contiguous_60 = transpose_284.contiguous();  transpose_284 = None
    view_227 = contiguous_60.view(getitem_262, getitem_263, getitem_264);  contiguous_60 = getitem_262 = getitem_263 = getitem_264 = None
    mid_block_attentions_0_transformer_blocks_4_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn1.to_out, "0")(view_227);  view_227 = None
    mid_block_attentions_0_transformer_blocks_4_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_4_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_4_attn1_to_out_0 = None
    add_103 = mid_block_attentions_0_transformer_blocks_4_attn1_to_out_1 + add_102;  mid_block_attentions_0_transformer_blocks_4_attn1_to_out_1 = add_102 = None
    mid_block_attentions_0_transformer_blocks_4_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").norm2(add_103)
    mid_block_attentions_0_transformer_blocks_4_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn2.to_q(mid_block_attentions_0_transformer_blocks_4_norm2)
    mid_block_attentions_0_transformer_blocks_4_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn2.to_k(mid_block_attentions_0_transformer_blocks_4_norm2)
    mid_block_attentions_0_transformer_blocks_4_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn2.to_v(mid_block_attentions_0_transformer_blocks_4_norm2);  mid_block_attentions_0_transformer_blocks_4_norm2 = None
    size_399 = mid_block_attentions_0_transformer_blocks_4_attn2_to_q.size()
    getitem_265 = size_399[0]
    getitem_266 = size_399[1]
    getitem_267 = size_399[2];  size_399 = None
    size_400 = mid_block_attentions_0_transformer_blocks_4_attn2_to_q.size(0)
    size_401 = mid_block_attentions_0_transformer_blocks_4_attn2_to_q.size(1)
    view_228 = mid_block_attentions_0_transformer_blocks_4_attn2_to_q.view(size_400, size_401, 20, 64);  mid_block_attentions_0_transformer_blocks_4_attn2_to_q = size_400 = size_401 = None
    transpose_285 = view_228.transpose(1, 2);  view_228 = None
    size_402 = mid_block_attentions_0_transformer_blocks_4_attn2_to_k.size(0)
    size_403 = mid_block_attentions_0_transformer_blocks_4_attn2_to_k.size(1)
    view_229 = mid_block_attentions_0_transformer_blocks_4_attn2_to_k.view(size_402, size_403, 20, 64);  mid_block_attentions_0_transformer_blocks_4_attn2_to_k = size_402 = size_403 = None
    transpose_286 = view_229.transpose(1, 2);  view_229 = None
    size_404 = mid_block_attentions_0_transformer_blocks_4_attn2_to_v.size(0)
    size_405 = mid_block_attentions_0_transformer_blocks_4_attn2_to_v.size(1)
    view_230 = mid_block_attentions_0_transformer_blocks_4_attn2_to_v.view(size_404, size_405, 20, 64);  mid_block_attentions_0_transformer_blocks_4_attn2_to_v = size_404 = size_405 = None
    transpose_287 = view_230.transpose(1, 2);  view_230 = None
    transpose_288 = transpose_286.transpose(-2, -1);  transpose_286 = None
    matmul_114 = torch.matmul(transpose_285, transpose_288);  transpose_285 = transpose_288 = None
    mul_94 = matmul_114 * 0.125;  matmul_114 = None
    softmax_57 = torch.softmax(mul_94, dim = -1);  mul_94 = None
    matmul_115 = torch.matmul(softmax_57, transpose_287);  softmax_57 = transpose_287 = None
    transpose_289 = matmul_115.transpose(1, 2);  matmul_115 = None
    contiguous_61 = transpose_289.contiguous();  transpose_289 = None
    view_231 = contiguous_61.view(getitem_265, getitem_266, getitem_267);  contiguous_61 = getitem_265 = getitem_266 = getitem_267 = None
    mid_block_attentions_0_transformer_blocks_4_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn2.to_out, "0")(view_231);  view_231 = None
    mid_block_attentions_0_transformer_blocks_4_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_4_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_4_attn2_to_out_0 = None
    add_104 = mid_block_attentions_0_transformer_blocks_4_attn2_to_out_1 + add_103;  mid_block_attentions_0_transformer_blocks_4_attn2_to_out_1 = add_103 = None
    mid_block_attentions_0_transformer_blocks_4_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").norm3(add_104)
    mid_block_attentions_0_transformer_blocks_4_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_4_norm3);  mid_block_attentions_0_transformer_blocks_4_norm3 = None
    chunk_28 = mid_block_attentions_0_transformer_blocks_4_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_4_ff_net_0_proj = None
    getitem_268 = chunk_28[0]
    getitem_269 = chunk_28[1];  chunk_28 = None
    gelu_28 = torch._C._nn.gelu(getitem_269);  getitem_269 = None
    mul_95 = getitem_268 * gelu_28;  getitem_268 = gelu_28 = None
    mid_block_attentions_0_transformer_blocks_4_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").ff.net, "1")(mul_95);  mul_95 = None
    mid_block_attentions_0_transformer_blocks_4_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "4").ff.net, "2")(mid_block_attentions_0_transformer_blocks_4_ff_net_1);  mid_block_attentions_0_transformer_blocks_4_ff_net_1 = None
    add_105 = mid_block_attentions_0_transformer_blocks_4_ff_net_2 + add_104;  mid_block_attentions_0_transformer_blocks_4_ff_net_2 = add_104 = None
    mid_block_attentions_0_transformer_blocks_5_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").norm1(add_105)
    mid_block_attentions_0_transformer_blocks_5_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn1.to_q(mid_block_attentions_0_transformer_blocks_5_norm1)
    mid_block_attentions_0_transformer_blocks_5_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn1.to_k(mid_block_attentions_0_transformer_blocks_5_norm1)
    mid_block_attentions_0_transformer_blocks_5_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn1.to_v(mid_block_attentions_0_transformer_blocks_5_norm1);  mid_block_attentions_0_transformer_blocks_5_norm1 = None
    size_406 = mid_block_attentions_0_transformer_blocks_5_attn1_to_q.size()
    getitem_270 = size_406[0]
    getitem_271 = size_406[1]
    getitem_272 = size_406[2];  size_406 = None
    size_407 = mid_block_attentions_0_transformer_blocks_5_attn1_to_q.size(0)
    size_408 = mid_block_attentions_0_transformer_blocks_5_attn1_to_q.size(1)
    view_232 = mid_block_attentions_0_transformer_blocks_5_attn1_to_q.view(size_407, size_408, 20, 64);  mid_block_attentions_0_transformer_blocks_5_attn1_to_q = size_407 = size_408 = None
    transpose_290 = view_232.transpose(1, 2);  view_232 = None
    size_409 = mid_block_attentions_0_transformer_blocks_5_attn1_to_k.size(0)
    size_410 = mid_block_attentions_0_transformer_blocks_5_attn1_to_k.size(1)
    view_233 = mid_block_attentions_0_transformer_blocks_5_attn1_to_k.view(size_409, size_410, 20, 64);  mid_block_attentions_0_transformer_blocks_5_attn1_to_k = size_409 = size_410 = None
    transpose_291 = view_233.transpose(1, 2);  view_233 = None
    size_411 = mid_block_attentions_0_transformer_blocks_5_attn1_to_v.size(0)
    size_412 = mid_block_attentions_0_transformer_blocks_5_attn1_to_v.size(1)
    view_234 = mid_block_attentions_0_transformer_blocks_5_attn1_to_v.view(size_411, size_412, 20, 64);  mid_block_attentions_0_transformer_blocks_5_attn1_to_v = size_411 = size_412 = None
    transpose_292 = view_234.transpose(1, 2);  view_234 = None
    transpose_293 = transpose_291.transpose(-2, -1);  transpose_291 = None
    matmul_116 = torch.matmul(transpose_290, transpose_293);  transpose_290 = transpose_293 = None
    mul_96 = matmul_116 * 0.125;  matmul_116 = None
    softmax_58 = torch.softmax(mul_96, dim = -1);  mul_96 = None
    matmul_117 = torch.matmul(softmax_58, transpose_292);  softmax_58 = transpose_292 = None
    transpose_294 = matmul_117.transpose(1, 2);  matmul_117 = None
    contiguous_62 = transpose_294.contiguous();  transpose_294 = None
    view_235 = contiguous_62.view(getitem_270, getitem_271, getitem_272);  contiguous_62 = getitem_270 = getitem_271 = getitem_272 = None
    mid_block_attentions_0_transformer_blocks_5_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn1.to_out, "0")(view_235);  view_235 = None
    mid_block_attentions_0_transformer_blocks_5_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_5_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_5_attn1_to_out_0 = None
    add_106 = mid_block_attentions_0_transformer_blocks_5_attn1_to_out_1 + add_105;  mid_block_attentions_0_transformer_blocks_5_attn1_to_out_1 = add_105 = None
    mid_block_attentions_0_transformer_blocks_5_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").norm2(add_106)
    mid_block_attentions_0_transformer_blocks_5_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn2.to_q(mid_block_attentions_0_transformer_blocks_5_norm2)
    mid_block_attentions_0_transformer_blocks_5_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn2.to_k(mid_block_attentions_0_transformer_blocks_5_norm2)
    mid_block_attentions_0_transformer_blocks_5_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn2.to_v(mid_block_attentions_0_transformer_blocks_5_norm2);  mid_block_attentions_0_transformer_blocks_5_norm2 = None
    size_413 = mid_block_attentions_0_transformer_blocks_5_attn2_to_q.size()
    getitem_273 = size_413[0]
    getitem_274 = size_413[1]
    getitem_275 = size_413[2];  size_413 = None
    size_414 = mid_block_attentions_0_transformer_blocks_5_attn2_to_q.size(0)
    size_415 = mid_block_attentions_0_transformer_blocks_5_attn2_to_q.size(1)
    view_236 = mid_block_attentions_0_transformer_blocks_5_attn2_to_q.view(size_414, size_415, 20, 64);  mid_block_attentions_0_transformer_blocks_5_attn2_to_q = size_414 = size_415 = None
    transpose_295 = view_236.transpose(1, 2);  view_236 = None
    size_416 = mid_block_attentions_0_transformer_blocks_5_attn2_to_k.size(0)
    size_417 = mid_block_attentions_0_transformer_blocks_5_attn2_to_k.size(1)
    view_237 = mid_block_attentions_0_transformer_blocks_5_attn2_to_k.view(size_416, size_417, 20, 64);  mid_block_attentions_0_transformer_blocks_5_attn2_to_k = size_416 = size_417 = None
    transpose_296 = view_237.transpose(1, 2);  view_237 = None
    size_418 = mid_block_attentions_0_transformer_blocks_5_attn2_to_v.size(0)
    size_419 = mid_block_attentions_0_transformer_blocks_5_attn2_to_v.size(1)
    view_238 = mid_block_attentions_0_transformer_blocks_5_attn2_to_v.view(size_418, size_419, 20, 64);  mid_block_attentions_0_transformer_blocks_5_attn2_to_v = size_418 = size_419 = None
    transpose_297 = view_238.transpose(1, 2);  view_238 = None
    transpose_298 = transpose_296.transpose(-2, -1);  transpose_296 = None
    matmul_118 = torch.matmul(transpose_295, transpose_298);  transpose_295 = transpose_298 = None
    mul_97 = matmul_118 * 0.125;  matmul_118 = None
    softmax_59 = torch.softmax(mul_97, dim = -1);  mul_97 = None
    matmul_119 = torch.matmul(softmax_59, transpose_297);  softmax_59 = transpose_297 = None
    transpose_299 = matmul_119.transpose(1, 2);  matmul_119 = None
    contiguous_63 = transpose_299.contiguous();  transpose_299 = None
    view_239 = contiguous_63.view(getitem_273, getitem_274, getitem_275);  contiguous_63 = getitem_273 = getitem_274 = getitem_275 = None
    mid_block_attentions_0_transformer_blocks_5_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn2.to_out, "0")(view_239);  view_239 = None
    mid_block_attentions_0_transformer_blocks_5_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_5_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_5_attn2_to_out_0 = None
    add_107 = mid_block_attentions_0_transformer_blocks_5_attn2_to_out_1 + add_106;  mid_block_attentions_0_transformer_blocks_5_attn2_to_out_1 = add_106 = None
    mid_block_attentions_0_transformer_blocks_5_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").norm3(add_107)
    mid_block_attentions_0_transformer_blocks_5_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_5_norm3);  mid_block_attentions_0_transformer_blocks_5_norm3 = None
    chunk_29 = mid_block_attentions_0_transformer_blocks_5_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_5_ff_net_0_proj = None
    getitem_276 = chunk_29[0]
    getitem_277 = chunk_29[1];  chunk_29 = None
    gelu_29 = torch._C._nn.gelu(getitem_277);  getitem_277 = None
    mul_98 = getitem_276 * gelu_29;  getitem_276 = gelu_29 = None
    mid_block_attentions_0_transformer_blocks_5_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").ff.net, "1")(mul_98);  mul_98 = None
    mid_block_attentions_0_transformer_blocks_5_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "5").ff.net, "2")(mid_block_attentions_0_transformer_blocks_5_ff_net_1);  mid_block_attentions_0_transformer_blocks_5_ff_net_1 = None
    add_108 = mid_block_attentions_0_transformer_blocks_5_ff_net_2 + add_107;  mid_block_attentions_0_transformer_blocks_5_ff_net_2 = add_107 = None
    mid_block_attentions_0_transformer_blocks_6_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").norm1(add_108)
    mid_block_attentions_0_transformer_blocks_6_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn1.to_q(mid_block_attentions_0_transformer_blocks_6_norm1)
    mid_block_attentions_0_transformer_blocks_6_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn1.to_k(mid_block_attentions_0_transformer_blocks_6_norm1)
    mid_block_attentions_0_transformer_blocks_6_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn1.to_v(mid_block_attentions_0_transformer_blocks_6_norm1);  mid_block_attentions_0_transformer_blocks_6_norm1 = None
    size_420 = mid_block_attentions_0_transformer_blocks_6_attn1_to_q.size()
    getitem_278 = size_420[0]
    getitem_279 = size_420[1]
    getitem_280 = size_420[2];  size_420 = None
    size_421 = mid_block_attentions_0_transformer_blocks_6_attn1_to_q.size(0)
    size_422 = mid_block_attentions_0_transformer_blocks_6_attn1_to_q.size(1)
    view_240 = mid_block_attentions_0_transformer_blocks_6_attn1_to_q.view(size_421, size_422, 20, 64);  mid_block_attentions_0_transformer_blocks_6_attn1_to_q = size_421 = size_422 = None
    transpose_300 = view_240.transpose(1, 2);  view_240 = None
    size_423 = mid_block_attentions_0_transformer_blocks_6_attn1_to_k.size(0)
    size_424 = mid_block_attentions_0_transformer_blocks_6_attn1_to_k.size(1)
    view_241 = mid_block_attentions_0_transformer_blocks_6_attn1_to_k.view(size_423, size_424, 20, 64);  mid_block_attentions_0_transformer_blocks_6_attn1_to_k = size_423 = size_424 = None
    transpose_301 = view_241.transpose(1, 2);  view_241 = None
    size_425 = mid_block_attentions_0_transformer_blocks_6_attn1_to_v.size(0)
    size_426 = mid_block_attentions_0_transformer_blocks_6_attn1_to_v.size(1)
    view_242 = mid_block_attentions_0_transformer_blocks_6_attn1_to_v.view(size_425, size_426, 20, 64);  mid_block_attentions_0_transformer_blocks_6_attn1_to_v = size_425 = size_426 = None
    transpose_302 = view_242.transpose(1, 2);  view_242 = None
    transpose_303 = transpose_301.transpose(-2, -1);  transpose_301 = None
    matmul_120 = torch.matmul(transpose_300, transpose_303);  transpose_300 = transpose_303 = None
    mul_99 = matmul_120 * 0.125;  matmul_120 = None
    softmax_60 = torch.softmax(mul_99, dim = -1);  mul_99 = None
    matmul_121 = torch.matmul(softmax_60, transpose_302);  softmax_60 = transpose_302 = None
    transpose_304 = matmul_121.transpose(1, 2);  matmul_121 = None
    contiguous_64 = transpose_304.contiguous();  transpose_304 = None
    view_243 = contiguous_64.view(getitem_278, getitem_279, getitem_280);  contiguous_64 = getitem_278 = getitem_279 = getitem_280 = None
    mid_block_attentions_0_transformer_blocks_6_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn1.to_out, "0")(view_243);  view_243 = None
    mid_block_attentions_0_transformer_blocks_6_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_6_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_6_attn1_to_out_0 = None
    add_109 = mid_block_attentions_0_transformer_blocks_6_attn1_to_out_1 + add_108;  mid_block_attentions_0_transformer_blocks_6_attn1_to_out_1 = add_108 = None
    mid_block_attentions_0_transformer_blocks_6_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").norm2(add_109)
    mid_block_attentions_0_transformer_blocks_6_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn2.to_q(mid_block_attentions_0_transformer_blocks_6_norm2)
    mid_block_attentions_0_transformer_blocks_6_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn2.to_k(mid_block_attentions_0_transformer_blocks_6_norm2)
    mid_block_attentions_0_transformer_blocks_6_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn2.to_v(mid_block_attentions_0_transformer_blocks_6_norm2);  mid_block_attentions_0_transformer_blocks_6_norm2 = None
    size_427 = mid_block_attentions_0_transformer_blocks_6_attn2_to_q.size()
    getitem_281 = size_427[0]
    getitem_282 = size_427[1]
    getitem_283 = size_427[2];  size_427 = None
    size_428 = mid_block_attentions_0_transformer_blocks_6_attn2_to_q.size(0)
    size_429 = mid_block_attentions_0_transformer_blocks_6_attn2_to_q.size(1)
    view_244 = mid_block_attentions_0_transformer_blocks_6_attn2_to_q.view(size_428, size_429, 20, 64);  mid_block_attentions_0_transformer_blocks_6_attn2_to_q = size_428 = size_429 = None
    transpose_305 = view_244.transpose(1, 2);  view_244 = None
    size_430 = mid_block_attentions_0_transformer_blocks_6_attn2_to_k.size(0)
    size_431 = mid_block_attentions_0_transformer_blocks_6_attn2_to_k.size(1)
    view_245 = mid_block_attentions_0_transformer_blocks_6_attn2_to_k.view(size_430, size_431, 20, 64);  mid_block_attentions_0_transformer_blocks_6_attn2_to_k = size_430 = size_431 = None
    transpose_306 = view_245.transpose(1, 2);  view_245 = None
    size_432 = mid_block_attentions_0_transformer_blocks_6_attn2_to_v.size(0)
    size_433 = mid_block_attentions_0_transformer_blocks_6_attn2_to_v.size(1)
    view_246 = mid_block_attentions_0_transformer_blocks_6_attn2_to_v.view(size_432, size_433, 20, 64);  mid_block_attentions_0_transformer_blocks_6_attn2_to_v = size_432 = size_433 = None
    transpose_307 = view_246.transpose(1, 2);  view_246 = None
    transpose_308 = transpose_306.transpose(-2, -1);  transpose_306 = None
    matmul_122 = torch.matmul(transpose_305, transpose_308);  transpose_305 = transpose_308 = None
    mul_100 = matmul_122 * 0.125;  matmul_122 = None
    softmax_61 = torch.softmax(mul_100, dim = -1);  mul_100 = None
    matmul_123 = torch.matmul(softmax_61, transpose_307);  softmax_61 = transpose_307 = None
    transpose_309 = matmul_123.transpose(1, 2);  matmul_123 = None
    contiguous_65 = transpose_309.contiguous();  transpose_309 = None
    view_247 = contiguous_65.view(getitem_281, getitem_282, getitem_283);  contiguous_65 = getitem_281 = getitem_282 = getitem_283 = None
    mid_block_attentions_0_transformer_blocks_6_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn2.to_out, "0")(view_247);  view_247 = None
    mid_block_attentions_0_transformer_blocks_6_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_6_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_6_attn2_to_out_0 = None
    add_110 = mid_block_attentions_0_transformer_blocks_6_attn2_to_out_1 + add_109;  mid_block_attentions_0_transformer_blocks_6_attn2_to_out_1 = add_109 = None
    mid_block_attentions_0_transformer_blocks_6_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").norm3(add_110)
    mid_block_attentions_0_transformer_blocks_6_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_6_norm3);  mid_block_attentions_0_transformer_blocks_6_norm3 = None
    chunk_30 = mid_block_attentions_0_transformer_blocks_6_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_6_ff_net_0_proj = None
    getitem_284 = chunk_30[0]
    getitem_285 = chunk_30[1];  chunk_30 = None
    gelu_30 = torch._C._nn.gelu(getitem_285);  getitem_285 = None
    mul_101 = getitem_284 * gelu_30;  getitem_284 = gelu_30 = None
    mid_block_attentions_0_transformer_blocks_6_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").ff.net, "1")(mul_101);  mul_101 = None
    mid_block_attentions_0_transformer_blocks_6_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "6").ff.net, "2")(mid_block_attentions_0_transformer_blocks_6_ff_net_1);  mid_block_attentions_0_transformer_blocks_6_ff_net_1 = None
    add_111 = mid_block_attentions_0_transformer_blocks_6_ff_net_2 + add_110;  mid_block_attentions_0_transformer_blocks_6_ff_net_2 = add_110 = None
    mid_block_attentions_0_transformer_blocks_7_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").norm1(add_111)
    mid_block_attentions_0_transformer_blocks_7_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn1.to_q(mid_block_attentions_0_transformer_blocks_7_norm1)
    mid_block_attentions_0_transformer_blocks_7_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn1.to_k(mid_block_attentions_0_transformer_blocks_7_norm1)
    mid_block_attentions_0_transformer_blocks_7_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn1.to_v(mid_block_attentions_0_transformer_blocks_7_norm1);  mid_block_attentions_0_transformer_blocks_7_norm1 = None
    size_434 = mid_block_attentions_0_transformer_blocks_7_attn1_to_q.size()
    getitem_286 = size_434[0]
    getitem_287 = size_434[1]
    getitem_288 = size_434[2];  size_434 = None
    size_435 = mid_block_attentions_0_transformer_blocks_7_attn1_to_q.size(0)
    size_436 = mid_block_attentions_0_transformer_blocks_7_attn1_to_q.size(1)
    view_248 = mid_block_attentions_0_transformer_blocks_7_attn1_to_q.view(size_435, size_436, 20, 64);  mid_block_attentions_0_transformer_blocks_7_attn1_to_q = size_435 = size_436 = None
    transpose_310 = view_248.transpose(1, 2);  view_248 = None
    size_437 = mid_block_attentions_0_transformer_blocks_7_attn1_to_k.size(0)
    size_438 = mid_block_attentions_0_transformer_blocks_7_attn1_to_k.size(1)
    view_249 = mid_block_attentions_0_transformer_blocks_7_attn1_to_k.view(size_437, size_438, 20, 64);  mid_block_attentions_0_transformer_blocks_7_attn1_to_k = size_437 = size_438 = None
    transpose_311 = view_249.transpose(1, 2);  view_249 = None
    size_439 = mid_block_attentions_0_transformer_blocks_7_attn1_to_v.size(0)
    size_440 = mid_block_attentions_0_transformer_blocks_7_attn1_to_v.size(1)
    view_250 = mid_block_attentions_0_transformer_blocks_7_attn1_to_v.view(size_439, size_440, 20, 64);  mid_block_attentions_0_transformer_blocks_7_attn1_to_v = size_439 = size_440 = None
    transpose_312 = view_250.transpose(1, 2);  view_250 = None
    transpose_313 = transpose_311.transpose(-2, -1);  transpose_311 = None
    matmul_124 = torch.matmul(transpose_310, transpose_313);  transpose_310 = transpose_313 = None
    mul_102 = matmul_124 * 0.125;  matmul_124 = None
    softmax_62 = torch.softmax(mul_102, dim = -1);  mul_102 = None
    matmul_125 = torch.matmul(softmax_62, transpose_312);  softmax_62 = transpose_312 = None
    transpose_314 = matmul_125.transpose(1, 2);  matmul_125 = None
    contiguous_66 = transpose_314.contiguous();  transpose_314 = None
    view_251 = contiguous_66.view(getitem_286, getitem_287, getitem_288);  contiguous_66 = getitem_286 = getitem_287 = getitem_288 = None
    mid_block_attentions_0_transformer_blocks_7_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn1.to_out, "0")(view_251);  view_251 = None
    mid_block_attentions_0_transformer_blocks_7_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_7_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_7_attn1_to_out_0 = None
    add_112 = mid_block_attentions_0_transformer_blocks_7_attn1_to_out_1 + add_111;  mid_block_attentions_0_transformer_blocks_7_attn1_to_out_1 = add_111 = None
    mid_block_attentions_0_transformer_blocks_7_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").norm2(add_112)
    mid_block_attentions_0_transformer_blocks_7_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn2.to_q(mid_block_attentions_0_transformer_blocks_7_norm2)
    mid_block_attentions_0_transformer_blocks_7_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn2.to_k(mid_block_attentions_0_transformer_blocks_7_norm2)
    mid_block_attentions_0_transformer_blocks_7_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn2.to_v(mid_block_attentions_0_transformer_blocks_7_norm2);  mid_block_attentions_0_transformer_blocks_7_norm2 = None
    size_441 = mid_block_attentions_0_transformer_blocks_7_attn2_to_q.size()
    getitem_289 = size_441[0]
    getitem_290 = size_441[1]
    getitem_291 = size_441[2];  size_441 = None
    size_442 = mid_block_attentions_0_transformer_blocks_7_attn2_to_q.size(0)
    size_443 = mid_block_attentions_0_transformer_blocks_7_attn2_to_q.size(1)
    view_252 = mid_block_attentions_0_transformer_blocks_7_attn2_to_q.view(size_442, size_443, 20, 64);  mid_block_attentions_0_transformer_blocks_7_attn2_to_q = size_442 = size_443 = None
    transpose_315 = view_252.transpose(1, 2);  view_252 = None
    size_444 = mid_block_attentions_0_transformer_blocks_7_attn2_to_k.size(0)
    size_445 = mid_block_attentions_0_transformer_blocks_7_attn2_to_k.size(1)
    view_253 = mid_block_attentions_0_transformer_blocks_7_attn2_to_k.view(size_444, size_445, 20, 64);  mid_block_attentions_0_transformer_blocks_7_attn2_to_k = size_444 = size_445 = None
    transpose_316 = view_253.transpose(1, 2);  view_253 = None
    size_446 = mid_block_attentions_0_transformer_blocks_7_attn2_to_v.size(0)
    size_447 = mid_block_attentions_0_transformer_blocks_7_attn2_to_v.size(1)
    view_254 = mid_block_attentions_0_transformer_blocks_7_attn2_to_v.view(size_446, size_447, 20, 64);  mid_block_attentions_0_transformer_blocks_7_attn2_to_v = size_446 = size_447 = None
    transpose_317 = view_254.transpose(1, 2);  view_254 = None
    transpose_318 = transpose_316.transpose(-2, -1);  transpose_316 = None
    matmul_126 = torch.matmul(transpose_315, transpose_318);  transpose_315 = transpose_318 = None
    mul_103 = matmul_126 * 0.125;  matmul_126 = None
    softmax_63 = torch.softmax(mul_103, dim = -1);  mul_103 = None
    matmul_127 = torch.matmul(softmax_63, transpose_317);  softmax_63 = transpose_317 = None
    transpose_319 = matmul_127.transpose(1, 2);  matmul_127 = None
    contiguous_67 = transpose_319.contiguous();  transpose_319 = None
    view_255 = contiguous_67.view(getitem_289, getitem_290, getitem_291);  contiguous_67 = getitem_289 = getitem_290 = getitem_291 = None
    mid_block_attentions_0_transformer_blocks_7_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn2.to_out, "0")(view_255);  view_255 = None
    mid_block_attentions_0_transformer_blocks_7_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_7_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_7_attn2_to_out_0 = None
    add_113 = mid_block_attentions_0_transformer_blocks_7_attn2_to_out_1 + add_112;  mid_block_attentions_0_transformer_blocks_7_attn2_to_out_1 = add_112 = None
    mid_block_attentions_0_transformer_blocks_7_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").norm3(add_113)
    mid_block_attentions_0_transformer_blocks_7_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_7_norm3);  mid_block_attentions_0_transformer_blocks_7_norm3 = None
    chunk_31 = mid_block_attentions_0_transformer_blocks_7_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_7_ff_net_0_proj = None
    getitem_292 = chunk_31[0]
    getitem_293 = chunk_31[1];  chunk_31 = None
    gelu_31 = torch._C._nn.gelu(getitem_293);  getitem_293 = None
    mul_104 = getitem_292 * gelu_31;  getitem_292 = gelu_31 = None
    mid_block_attentions_0_transformer_blocks_7_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").ff.net, "1")(mul_104);  mul_104 = None
    mid_block_attentions_0_transformer_blocks_7_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "7").ff.net, "2")(mid_block_attentions_0_transformer_blocks_7_ff_net_1);  mid_block_attentions_0_transformer_blocks_7_ff_net_1 = None
    add_114 = mid_block_attentions_0_transformer_blocks_7_ff_net_2 + add_113;  mid_block_attentions_0_transformer_blocks_7_ff_net_2 = add_113 = None
    mid_block_attentions_0_transformer_blocks_8_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").norm1(add_114)
    mid_block_attentions_0_transformer_blocks_8_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn1.to_q(mid_block_attentions_0_transformer_blocks_8_norm1)
    mid_block_attentions_0_transformer_blocks_8_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn1.to_k(mid_block_attentions_0_transformer_blocks_8_norm1)
    mid_block_attentions_0_transformer_blocks_8_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn1.to_v(mid_block_attentions_0_transformer_blocks_8_norm1);  mid_block_attentions_0_transformer_blocks_8_norm1 = None
    size_448 = mid_block_attentions_0_transformer_blocks_8_attn1_to_q.size()
    getitem_294 = size_448[0]
    getitem_295 = size_448[1]
    getitem_296 = size_448[2];  size_448 = None
    size_449 = mid_block_attentions_0_transformer_blocks_8_attn1_to_q.size(0)
    size_450 = mid_block_attentions_0_transformer_blocks_8_attn1_to_q.size(1)
    view_256 = mid_block_attentions_0_transformer_blocks_8_attn1_to_q.view(size_449, size_450, 20, 64);  mid_block_attentions_0_transformer_blocks_8_attn1_to_q = size_449 = size_450 = None
    transpose_320 = view_256.transpose(1, 2);  view_256 = None
    size_451 = mid_block_attentions_0_transformer_blocks_8_attn1_to_k.size(0)
    size_452 = mid_block_attentions_0_transformer_blocks_8_attn1_to_k.size(1)
    view_257 = mid_block_attentions_0_transformer_blocks_8_attn1_to_k.view(size_451, size_452, 20, 64);  mid_block_attentions_0_transformer_blocks_8_attn1_to_k = size_451 = size_452 = None
    transpose_321 = view_257.transpose(1, 2);  view_257 = None
    size_453 = mid_block_attentions_0_transformer_blocks_8_attn1_to_v.size(0)
    size_454 = mid_block_attentions_0_transformer_blocks_8_attn1_to_v.size(1)
    view_258 = mid_block_attentions_0_transformer_blocks_8_attn1_to_v.view(size_453, size_454, 20, 64);  mid_block_attentions_0_transformer_blocks_8_attn1_to_v = size_453 = size_454 = None
    transpose_322 = view_258.transpose(1, 2);  view_258 = None
    transpose_323 = transpose_321.transpose(-2, -1);  transpose_321 = None
    matmul_128 = torch.matmul(transpose_320, transpose_323);  transpose_320 = transpose_323 = None
    mul_105 = matmul_128 * 0.125;  matmul_128 = None
    softmax_64 = torch.softmax(mul_105, dim = -1);  mul_105 = None
    matmul_129 = torch.matmul(softmax_64, transpose_322);  softmax_64 = transpose_322 = None
    transpose_324 = matmul_129.transpose(1, 2);  matmul_129 = None
    contiguous_68 = transpose_324.contiguous();  transpose_324 = None
    view_259 = contiguous_68.view(getitem_294, getitem_295, getitem_296);  contiguous_68 = getitem_294 = getitem_295 = getitem_296 = None
    mid_block_attentions_0_transformer_blocks_8_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn1.to_out, "0")(view_259);  view_259 = None
    mid_block_attentions_0_transformer_blocks_8_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_8_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_8_attn1_to_out_0 = None
    add_115 = mid_block_attentions_0_transformer_blocks_8_attn1_to_out_1 + add_114;  mid_block_attentions_0_transformer_blocks_8_attn1_to_out_1 = add_114 = None
    mid_block_attentions_0_transformer_blocks_8_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").norm2(add_115)
    mid_block_attentions_0_transformer_blocks_8_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn2.to_q(mid_block_attentions_0_transformer_blocks_8_norm2)
    mid_block_attentions_0_transformer_blocks_8_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn2.to_k(mid_block_attentions_0_transformer_blocks_8_norm2)
    mid_block_attentions_0_transformer_blocks_8_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn2.to_v(mid_block_attentions_0_transformer_blocks_8_norm2);  mid_block_attentions_0_transformer_blocks_8_norm2 = None
    size_455 = mid_block_attentions_0_transformer_blocks_8_attn2_to_q.size()
    getitem_297 = size_455[0]
    getitem_298 = size_455[1]
    getitem_299 = size_455[2];  size_455 = None
    size_456 = mid_block_attentions_0_transformer_blocks_8_attn2_to_q.size(0)
    size_457 = mid_block_attentions_0_transformer_blocks_8_attn2_to_q.size(1)
    view_260 = mid_block_attentions_0_transformer_blocks_8_attn2_to_q.view(size_456, size_457, 20, 64);  mid_block_attentions_0_transformer_blocks_8_attn2_to_q = size_456 = size_457 = None
    transpose_325 = view_260.transpose(1, 2);  view_260 = None
    size_458 = mid_block_attentions_0_transformer_blocks_8_attn2_to_k.size(0)
    size_459 = mid_block_attentions_0_transformer_blocks_8_attn2_to_k.size(1)
    view_261 = mid_block_attentions_0_transformer_blocks_8_attn2_to_k.view(size_458, size_459, 20, 64);  mid_block_attentions_0_transformer_blocks_8_attn2_to_k = size_458 = size_459 = None
    transpose_326 = view_261.transpose(1, 2);  view_261 = None
    size_460 = mid_block_attentions_0_transformer_blocks_8_attn2_to_v.size(0)
    size_461 = mid_block_attentions_0_transformer_blocks_8_attn2_to_v.size(1)
    view_262 = mid_block_attentions_0_transformer_blocks_8_attn2_to_v.view(size_460, size_461, 20, 64);  mid_block_attentions_0_transformer_blocks_8_attn2_to_v = size_460 = size_461 = None
    transpose_327 = view_262.transpose(1, 2);  view_262 = None
    transpose_328 = transpose_326.transpose(-2, -1);  transpose_326 = None
    matmul_130 = torch.matmul(transpose_325, transpose_328);  transpose_325 = transpose_328 = None
    mul_106 = matmul_130 * 0.125;  matmul_130 = None
    softmax_65 = torch.softmax(mul_106, dim = -1);  mul_106 = None
    matmul_131 = torch.matmul(softmax_65, transpose_327);  softmax_65 = transpose_327 = None
    transpose_329 = matmul_131.transpose(1, 2);  matmul_131 = None
    contiguous_69 = transpose_329.contiguous();  transpose_329 = None
    view_263 = contiguous_69.view(getitem_297, getitem_298, getitem_299);  contiguous_69 = getitem_297 = getitem_298 = getitem_299 = None
    mid_block_attentions_0_transformer_blocks_8_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn2.to_out, "0")(view_263);  view_263 = None
    mid_block_attentions_0_transformer_blocks_8_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_8_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_8_attn2_to_out_0 = None
    add_116 = mid_block_attentions_0_transformer_blocks_8_attn2_to_out_1 + add_115;  mid_block_attentions_0_transformer_blocks_8_attn2_to_out_1 = add_115 = None
    mid_block_attentions_0_transformer_blocks_8_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").norm3(add_116)
    mid_block_attentions_0_transformer_blocks_8_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_8_norm3);  mid_block_attentions_0_transformer_blocks_8_norm3 = None
    chunk_32 = mid_block_attentions_0_transformer_blocks_8_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_8_ff_net_0_proj = None
    getitem_300 = chunk_32[0]
    getitem_301 = chunk_32[1];  chunk_32 = None
    gelu_32 = torch._C._nn.gelu(getitem_301);  getitem_301 = None
    mul_107 = getitem_300 * gelu_32;  getitem_300 = gelu_32 = None
    mid_block_attentions_0_transformer_blocks_8_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").ff.net, "1")(mul_107);  mul_107 = None
    mid_block_attentions_0_transformer_blocks_8_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "8").ff.net, "2")(mid_block_attentions_0_transformer_blocks_8_ff_net_1);  mid_block_attentions_0_transformer_blocks_8_ff_net_1 = None
    add_117 = mid_block_attentions_0_transformer_blocks_8_ff_net_2 + add_116;  mid_block_attentions_0_transformer_blocks_8_ff_net_2 = add_116 = None
    mid_block_attentions_0_transformer_blocks_9_norm1 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").norm1(add_117)
    mid_block_attentions_0_transformer_blocks_9_attn1_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn1.to_q(mid_block_attentions_0_transformer_blocks_9_norm1)
    mid_block_attentions_0_transformer_blocks_9_attn1_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn1.to_k(mid_block_attentions_0_transformer_blocks_9_norm1)
    mid_block_attentions_0_transformer_blocks_9_attn1_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn1.to_v(mid_block_attentions_0_transformer_blocks_9_norm1);  mid_block_attentions_0_transformer_blocks_9_norm1 = None
    size_462 = mid_block_attentions_0_transformer_blocks_9_attn1_to_q.size()
    getitem_302 = size_462[0]
    getitem_303 = size_462[1]
    getitem_304 = size_462[2];  size_462 = None
    size_463 = mid_block_attentions_0_transformer_blocks_9_attn1_to_q.size(0)
    size_464 = mid_block_attentions_0_transformer_blocks_9_attn1_to_q.size(1)
    view_264 = mid_block_attentions_0_transformer_blocks_9_attn1_to_q.view(size_463, size_464, 20, 64);  mid_block_attentions_0_transformer_blocks_9_attn1_to_q = size_463 = size_464 = None
    transpose_330 = view_264.transpose(1, 2);  view_264 = None
    size_465 = mid_block_attentions_0_transformer_blocks_9_attn1_to_k.size(0)
    size_466 = mid_block_attentions_0_transformer_blocks_9_attn1_to_k.size(1)
    view_265 = mid_block_attentions_0_transformer_blocks_9_attn1_to_k.view(size_465, size_466, 20, 64);  mid_block_attentions_0_transformer_blocks_9_attn1_to_k = size_465 = size_466 = None
    transpose_331 = view_265.transpose(1, 2);  view_265 = None
    size_467 = mid_block_attentions_0_transformer_blocks_9_attn1_to_v.size(0)
    size_468 = mid_block_attentions_0_transformer_blocks_9_attn1_to_v.size(1)
    view_266 = mid_block_attentions_0_transformer_blocks_9_attn1_to_v.view(size_467, size_468, 20, 64);  mid_block_attentions_0_transformer_blocks_9_attn1_to_v = size_467 = size_468 = None
    transpose_332 = view_266.transpose(1, 2);  view_266 = None
    transpose_333 = transpose_331.transpose(-2, -1);  transpose_331 = None
    matmul_132 = torch.matmul(transpose_330, transpose_333);  transpose_330 = transpose_333 = None
    mul_108 = matmul_132 * 0.125;  matmul_132 = None
    softmax_66 = torch.softmax(mul_108, dim = -1);  mul_108 = None
    matmul_133 = torch.matmul(softmax_66, transpose_332);  softmax_66 = transpose_332 = None
    transpose_334 = matmul_133.transpose(1, 2);  matmul_133 = None
    contiguous_70 = transpose_334.contiguous();  transpose_334 = None
    view_267 = contiguous_70.view(getitem_302, getitem_303, getitem_304);  contiguous_70 = getitem_302 = getitem_303 = getitem_304 = None
    mid_block_attentions_0_transformer_blocks_9_attn1_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn1.to_out, "0")(view_267);  view_267 = None
    mid_block_attentions_0_transformer_blocks_9_attn1_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn1.to_out, "1")(mid_block_attentions_0_transformer_blocks_9_attn1_to_out_0);  mid_block_attentions_0_transformer_blocks_9_attn1_to_out_0 = None
    add_118 = mid_block_attentions_0_transformer_blocks_9_attn1_to_out_1 + add_117;  mid_block_attentions_0_transformer_blocks_9_attn1_to_out_1 = add_117 = None
    mid_block_attentions_0_transformer_blocks_9_norm2 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").norm2(add_118)
    mid_block_attentions_0_transformer_blocks_9_attn2_to_q = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn2.to_q(mid_block_attentions_0_transformer_blocks_9_norm2)
    mid_block_attentions_0_transformer_blocks_9_attn2_to_k = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn2.to_k(mid_block_attentions_0_transformer_blocks_9_norm2)
    mid_block_attentions_0_transformer_blocks_9_attn2_to_v = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn2.to_v(mid_block_attentions_0_transformer_blocks_9_norm2);  mid_block_attentions_0_transformer_blocks_9_norm2 = None
    size_469 = mid_block_attentions_0_transformer_blocks_9_attn2_to_q.size()
    getitem_305 = size_469[0]
    getitem_306 = size_469[1]
    getitem_307 = size_469[2];  size_469 = None
    size_470 = mid_block_attentions_0_transformer_blocks_9_attn2_to_q.size(0)
    size_471 = mid_block_attentions_0_transformer_blocks_9_attn2_to_q.size(1)
    view_268 = mid_block_attentions_0_transformer_blocks_9_attn2_to_q.view(size_470, size_471, 20, 64);  mid_block_attentions_0_transformer_blocks_9_attn2_to_q = size_470 = size_471 = None
    transpose_335 = view_268.transpose(1, 2);  view_268 = None
    size_472 = mid_block_attentions_0_transformer_blocks_9_attn2_to_k.size(0)
    size_473 = mid_block_attentions_0_transformer_blocks_9_attn2_to_k.size(1)
    view_269 = mid_block_attentions_0_transformer_blocks_9_attn2_to_k.view(size_472, size_473, 20, 64);  mid_block_attentions_0_transformer_blocks_9_attn2_to_k = size_472 = size_473 = None
    transpose_336 = view_269.transpose(1, 2);  view_269 = None
    size_474 = mid_block_attentions_0_transformer_blocks_9_attn2_to_v.size(0)
    size_475 = mid_block_attentions_0_transformer_blocks_9_attn2_to_v.size(1)
    view_270 = mid_block_attentions_0_transformer_blocks_9_attn2_to_v.view(size_474, size_475, 20, 64);  mid_block_attentions_0_transformer_blocks_9_attn2_to_v = size_474 = size_475 = None
    transpose_337 = view_270.transpose(1, 2);  view_270 = None
    transpose_338 = transpose_336.transpose(-2, -1);  transpose_336 = None
    matmul_134 = torch.matmul(transpose_335, transpose_338);  transpose_335 = transpose_338 = None
    mul_109 = matmul_134 * 0.125;  matmul_134 = None
    softmax_67 = torch.softmax(mul_109, dim = -1);  mul_109 = None
    matmul_135 = torch.matmul(softmax_67, transpose_337);  softmax_67 = transpose_337 = None
    transpose_339 = matmul_135.transpose(1, 2);  matmul_135 = None
    contiguous_71 = transpose_339.contiguous();  transpose_339 = None
    view_271 = contiguous_71.view(getitem_305, getitem_306, getitem_307);  contiguous_71 = getitem_305 = getitem_306 = getitem_307 = None
    mid_block_attentions_0_transformer_blocks_9_attn2_to_out_0 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn2.to_out, "0")(view_271);  view_271 = None
    mid_block_attentions_0_transformer_blocks_9_attn2_to_out_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").attn2.to_out, "1")(mid_block_attentions_0_transformer_blocks_9_attn2_to_out_0);  mid_block_attentions_0_transformer_blocks_9_attn2_to_out_0 = None
    add_119 = mid_block_attentions_0_transformer_blocks_9_attn2_to_out_1 + add_118;  mid_block_attentions_0_transformer_blocks_9_attn2_to_out_1 = add_118 = None
    mid_block_attentions_0_transformer_blocks_9_norm3 = getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").norm3(add_119)
    mid_block_attentions_0_transformer_blocks_9_ff_net_0_proj = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").ff.net, "0").proj(mid_block_attentions_0_transformer_blocks_9_norm3);  mid_block_attentions_0_transformer_blocks_9_norm3 = None
    chunk_33 = mid_block_attentions_0_transformer_blocks_9_ff_net_0_proj.chunk(2, dim = -1);  mid_block_attentions_0_transformer_blocks_9_ff_net_0_proj = None
    getitem_308 = chunk_33[0]
    getitem_309 = chunk_33[1];  chunk_33 = None
    gelu_33 = torch._C._nn.gelu(getitem_309);  getitem_309 = None
    mul_110 = getitem_308 * gelu_33;  getitem_308 = gelu_33 = None
    mid_block_attentions_0_transformer_blocks_9_ff_net_1 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").ff.net, "1")(mul_110);  mul_110 = None
    mid_block_attentions_0_transformer_blocks_9_ff_net_2 = getattr(getattr(getattr(self.mid_block.attentions, "0").transformer_blocks, "9").ff.net, "2")(mid_block_attentions_0_transformer_blocks_9_ff_net_1);  mid_block_attentions_0_transformer_blocks_9_ff_net_1 = None
    add_120 = mid_block_attentions_0_transformer_blocks_9_ff_net_2 + add_119;  mid_block_attentions_0_transformer_blocks_9_ff_net_2 = add_119 = None
    mid_block_attentions_0_proj_out = getattr(self.mid_block.attentions, "0").proj_out(add_120);  add_120 = None
    reshape_10 = mid_block_attentions_0_proj_out.reshape(getitem_225, getitem_227, getitem_228, getitem_229);  mid_block_attentions_0_proj_out = getitem_225 = getitem_227 = getitem_228 = getitem_229 = None
    permute_9 = reshape_10.permute(0, 3, 1, 2);  reshape_10 = None
    contiguous_72 = permute_9.contiguous();  permute_9 = None
    add_121 = contiguous_72 + add_90;  contiguous_72 = add_90 = None
    mid_block_resnets_1_norm1 = getattr(self.mid_block.resnets, "1").norm1(add_121)
    mid_block_resnets_1_nonlinearity = getattr(self.mid_block.resnets, "1").nonlinearity(mid_block_resnets_1_norm1);  mid_block_resnets_1_norm1 = None
    mid_block_resnets_1_conv1 = getattr(self.mid_block.resnets, "1").conv1(mid_block_resnets_1_nonlinearity);  mid_block_resnets_1_nonlinearity = None
    mid_block_resnets_1_nonlinearity_1 = getattr(self.mid_block.resnets, "1").nonlinearity(add)
    mid_block_resnets_1_time_emb_proj = getattr(self.mid_block.resnets, "1").time_emb_proj(mid_block_resnets_1_nonlinearity_1);  mid_block_resnets_1_nonlinearity_1 = None
    getitem_310 = mid_block_resnets_1_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  mid_block_resnets_1_time_emb_proj = None
    add_122 = mid_block_resnets_1_conv1 + getitem_310;  mid_block_resnets_1_conv1 = getitem_310 = None
    mid_block_resnets_1_norm2 = getattr(self.mid_block.resnets, "1").norm2(add_122);  add_122 = None
    mid_block_resnets_1_nonlinearity_2 = getattr(self.mid_block.resnets, "1").nonlinearity(mid_block_resnets_1_norm2);  mid_block_resnets_1_norm2 = None
    mid_block_resnets_1_dropout = getattr(self.mid_block.resnets, "1").dropout(mid_block_resnets_1_nonlinearity_2);  mid_block_resnets_1_nonlinearity_2 = None
    mid_block_resnets_1_conv2 = getattr(self.mid_block.resnets, "1").conv2(mid_block_resnets_1_dropout);  mid_block_resnets_1_dropout = None
    add_123 = add_121 + mid_block_resnets_1_conv2;  add_121 = mid_block_resnets_1_conv2 = None
    cat_2 = torch.cat([add_123, add_88], dim = 1);  add_123 = add_88 = None
    up_blocks_0_resnets_0_norm1 = getattr(getattr(self.up_blocks, "0").resnets, "0").norm1(cat_2)
    up_blocks_0_resnets_0_nonlinearity = getattr(getattr(self.up_blocks, "0").resnets, "0").nonlinearity(up_blocks_0_resnets_0_norm1);  up_blocks_0_resnets_0_norm1 = None
    up_blocks_0_resnets_0_conv1 = getattr(getattr(self.up_blocks, "0").resnets, "0").conv1(up_blocks_0_resnets_0_nonlinearity);  up_blocks_0_resnets_0_nonlinearity = None
    up_blocks_0_resnets_0_nonlinearity_1 = getattr(getattr(self.up_blocks, "0").resnets, "0").nonlinearity(add)
    up_blocks_0_resnets_0_time_emb_proj = getattr(getattr(self.up_blocks, "0").resnets, "0").time_emb_proj(up_blocks_0_resnets_0_nonlinearity_1);  up_blocks_0_resnets_0_nonlinearity_1 = None
    getitem_311 = up_blocks_0_resnets_0_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_0_resnets_0_time_emb_proj = None
    add_124 = up_blocks_0_resnets_0_conv1 + getitem_311;  up_blocks_0_resnets_0_conv1 = getitem_311 = None
    up_blocks_0_resnets_0_norm2 = getattr(getattr(self.up_blocks, "0").resnets, "0").norm2(add_124);  add_124 = None
    up_blocks_0_resnets_0_nonlinearity_2 = getattr(getattr(self.up_blocks, "0").resnets, "0").nonlinearity(up_blocks_0_resnets_0_norm2);  up_blocks_0_resnets_0_norm2 = None
    up_blocks_0_resnets_0_dropout = getattr(getattr(self.up_blocks, "0").resnets, "0").dropout(up_blocks_0_resnets_0_nonlinearity_2);  up_blocks_0_resnets_0_nonlinearity_2 = None
    up_blocks_0_resnets_0_conv2 = getattr(getattr(self.up_blocks, "0").resnets, "0").conv2(up_blocks_0_resnets_0_dropout);  up_blocks_0_resnets_0_dropout = None
    up_blocks_0_resnets_0_conv_shortcut = getattr(getattr(self.up_blocks, "0").resnets, "0").conv_shortcut(cat_2);  cat_2 = None
    add_125 = up_blocks_0_resnets_0_conv_shortcut + up_blocks_0_resnets_0_conv2;  up_blocks_0_resnets_0_conv_shortcut = up_blocks_0_resnets_0_conv2 = None
    getattr_19 = add_125.shape
    getitem_312 = getattr_19[0]
    getitem_313 = getattr_19[1]
    getitem_314 = getattr_19[2]
    getitem_315 = getattr_19[3];  getattr_19 = None
    up_blocks_0_attentions_0_norm = getattr(getattr(self.up_blocks, "0").attentions, "0").norm(add_125)
    getattr_20 = up_blocks_0_attentions_0_norm.shape
    getitem_316 = getattr_20[1];  getattr_20 = None
    permute_10 = up_blocks_0_attentions_0_norm.permute(0, 2, 3, 1);  up_blocks_0_attentions_0_norm = None
    mul_111 = getitem_314 * getitem_315
    reshape_11 = permute_10.reshape(getitem_312, mul_111, getitem_316);  permute_10 = mul_111 = None
    up_blocks_0_attentions_0_proj_in = getattr(getattr(self.up_blocks, "0").attentions, "0").proj_in(reshape_11);  reshape_11 = None
    up_blocks_0_attentions_0_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").norm1(up_blocks_0_attentions_0_proj_in)
    up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_0_norm1)
    up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_0_norm1)
    up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_0_norm1);  up_blocks_0_attentions_0_transformer_blocks_0_norm1 = None
    size_476 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.size()
    getitem_317 = size_476[0]
    getitem_318 = size_476[1]
    getitem_319 = size_476[2];  size_476 = None
    size_477 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.size(0)
    size_478 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.size(1)
    view_272 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.view(size_477, size_478, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q = size_477 = size_478 = None
    transpose_340 = view_272.transpose(1, 2);  view_272 = None
    size_479 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.size(0)
    size_480 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.size(1)
    view_273 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.view(size_479, size_480, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k = size_479 = size_480 = None
    transpose_341 = view_273.transpose(1, 2);  view_273 = None
    size_481 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.size(0)
    size_482 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.size(1)
    view_274 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.view(size_481, size_482, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v = size_481 = size_482 = None
    transpose_342 = view_274.transpose(1, 2);  view_274 = None
    transpose_343 = transpose_341.transpose(-2, -1);  transpose_341 = None
    matmul_136 = torch.matmul(transpose_340, transpose_343);  transpose_340 = transpose_343 = None
    mul_112 = matmul_136 * 0.125;  matmul_136 = None
    softmax_68 = torch.softmax(mul_112, dim = -1);  mul_112 = None
    matmul_137 = torch.matmul(softmax_68, transpose_342);  softmax_68 = transpose_342 = None
    transpose_344 = matmul_137.transpose(1, 2);  matmul_137 = None
    contiguous_73 = transpose_344.contiguous();  transpose_344 = None
    view_275 = contiguous_73.view(getitem_317, getitem_318, getitem_319);  contiguous_73 = getitem_317 = getitem_318 = getitem_319 = None
    up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn1.to_out, "0")(view_275);  view_275 = None
    up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0 = None
    add_126 = up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_1 + up_blocks_0_attentions_0_proj_in;  up_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_1 = up_blocks_0_attentions_0_proj_in = None
    up_blocks_0_attentions_0_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").norm2(add_126)
    up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_0_norm2)
    up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_0_norm2)
    up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_0_norm2);  up_blocks_0_attentions_0_transformer_blocks_0_norm2 = None
    size_483 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.size()
    getitem_320 = size_483[0]
    getitem_321 = size_483[1]
    getitem_322 = size_483[2];  size_483 = None
    size_484 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.size(0)
    size_485 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.size(1)
    view_276 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.view(size_484, size_485, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q = size_484 = size_485 = None
    transpose_345 = view_276.transpose(1, 2);  view_276 = None
    size_486 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.size(0)
    size_487 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.size(1)
    view_277 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.view(size_486, size_487, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k = size_486 = size_487 = None
    transpose_346 = view_277.transpose(1, 2);  view_277 = None
    size_488 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.size(0)
    size_489 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.size(1)
    view_278 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.view(size_488, size_489, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v = size_488 = size_489 = None
    transpose_347 = view_278.transpose(1, 2);  view_278 = None
    transpose_348 = transpose_346.transpose(-2, -1);  transpose_346 = None
    matmul_138 = torch.matmul(transpose_345, transpose_348);  transpose_345 = transpose_348 = None
    mul_113 = matmul_138 * 0.125;  matmul_138 = None
    softmax_69 = torch.softmax(mul_113, dim = -1);  mul_113 = None
    matmul_139 = torch.matmul(softmax_69, transpose_347);  softmax_69 = transpose_347 = None
    transpose_349 = matmul_139.transpose(1, 2);  matmul_139 = None
    contiguous_74 = transpose_349.contiguous();  transpose_349 = None
    view_279 = contiguous_74.view(getitem_320, getitem_321, getitem_322);  contiguous_74 = getitem_320 = getitem_321 = getitem_322 = None
    up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn2.to_out, "0")(view_279);  view_279 = None
    up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0 = None
    add_127 = up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_1 + add_126;  up_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_1 = add_126 = None
    up_blocks_0_attentions_0_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").norm3(add_127)
    up_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_0_norm3);  up_blocks_0_attentions_0_transformer_blocks_0_norm3 = None
    chunk_34 = up_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj = None
    getitem_323 = chunk_34[0]
    getitem_324 = chunk_34[1];  chunk_34 = None
    gelu_34 = torch._C._nn.gelu(getitem_324);  getitem_324 = None
    mul_114 = getitem_323 * gelu_34;  getitem_323 = gelu_34 = None
    up_blocks_0_attentions_0_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").ff.net, "1")(mul_114);  mul_114 = None
    up_blocks_0_attentions_0_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "0").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_0_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_0_ff_net_1 = None
    add_128 = up_blocks_0_attentions_0_transformer_blocks_0_ff_net_2 + add_127;  up_blocks_0_attentions_0_transformer_blocks_0_ff_net_2 = add_127 = None
    up_blocks_0_attentions_0_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").norm1(add_128)
    up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_1_norm1)
    up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_1_norm1)
    up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_1_norm1);  up_blocks_0_attentions_0_transformer_blocks_1_norm1 = None
    size_490 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_q.size()
    getitem_325 = size_490[0]
    getitem_326 = size_490[1]
    getitem_327 = size_490[2];  size_490 = None
    size_491 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_q.size(0)
    size_492 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_q.size(1)
    view_280 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_q.view(size_491, size_492, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_q = size_491 = size_492 = None
    transpose_350 = view_280.transpose(1, 2);  view_280 = None
    size_493 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_k.size(0)
    size_494 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_k.size(1)
    view_281 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_k.view(size_493, size_494, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_k = size_493 = size_494 = None
    transpose_351 = view_281.transpose(1, 2);  view_281 = None
    size_495 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_v.size(0)
    size_496 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_v.size(1)
    view_282 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_v.view(size_495, size_496, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_v = size_495 = size_496 = None
    transpose_352 = view_282.transpose(1, 2);  view_282 = None
    transpose_353 = transpose_351.transpose(-2, -1);  transpose_351 = None
    matmul_140 = torch.matmul(transpose_350, transpose_353);  transpose_350 = transpose_353 = None
    mul_115 = matmul_140 * 0.125;  matmul_140 = None
    softmax_70 = torch.softmax(mul_115, dim = -1);  mul_115 = None
    matmul_141 = torch.matmul(softmax_70, transpose_352);  softmax_70 = transpose_352 = None
    transpose_354 = matmul_141.transpose(1, 2);  matmul_141 = None
    contiguous_75 = transpose_354.contiguous();  transpose_354 = None
    view_283 = contiguous_75.view(getitem_325, getitem_326, getitem_327);  contiguous_75 = getitem_325 = getitem_326 = getitem_327 = None
    up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn1.to_out, "0")(view_283);  view_283 = None
    up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_out_0 = None
    add_129 = up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_out_1 + add_128;  up_blocks_0_attentions_0_transformer_blocks_1_attn1_to_out_1 = add_128 = None
    up_blocks_0_attentions_0_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").norm2(add_129)
    up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_1_norm2)
    up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_1_norm2)
    up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_1_norm2);  up_blocks_0_attentions_0_transformer_blocks_1_norm2 = None
    size_497 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_q.size()
    getitem_328 = size_497[0]
    getitem_329 = size_497[1]
    getitem_330 = size_497[2];  size_497 = None
    size_498 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_q.size(0)
    size_499 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_q.size(1)
    view_284 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_q.view(size_498, size_499, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_q = size_498 = size_499 = None
    transpose_355 = view_284.transpose(1, 2);  view_284 = None
    size_500 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_k.size(0)
    size_501 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_k.size(1)
    view_285 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_k.view(size_500, size_501, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_k = size_500 = size_501 = None
    transpose_356 = view_285.transpose(1, 2);  view_285 = None
    size_502 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_v.size(0)
    size_503 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_v.size(1)
    view_286 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_v.view(size_502, size_503, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_v = size_502 = size_503 = None
    transpose_357 = view_286.transpose(1, 2);  view_286 = None
    transpose_358 = transpose_356.transpose(-2, -1);  transpose_356 = None
    matmul_142 = torch.matmul(transpose_355, transpose_358);  transpose_355 = transpose_358 = None
    mul_116 = matmul_142 * 0.125;  matmul_142 = None
    softmax_71 = torch.softmax(mul_116, dim = -1);  mul_116 = None
    matmul_143 = torch.matmul(softmax_71, transpose_357);  softmax_71 = transpose_357 = None
    transpose_359 = matmul_143.transpose(1, 2);  matmul_143 = None
    contiguous_76 = transpose_359.contiguous();  transpose_359 = None
    view_287 = contiguous_76.view(getitem_328, getitem_329, getitem_330);  contiguous_76 = getitem_328 = getitem_329 = getitem_330 = None
    up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn2.to_out, "0")(view_287);  view_287 = None
    up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_out_0 = None
    add_130 = up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_out_1 + add_129;  up_blocks_0_attentions_0_transformer_blocks_1_attn2_to_out_1 = add_129 = None
    up_blocks_0_attentions_0_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").norm3(add_130)
    up_blocks_0_attentions_0_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_1_norm3);  up_blocks_0_attentions_0_transformer_blocks_1_norm3 = None
    chunk_35 = up_blocks_0_attentions_0_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_1_ff_net_0_proj = None
    getitem_331 = chunk_35[0]
    getitem_332 = chunk_35[1];  chunk_35 = None
    gelu_35 = torch._C._nn.gelu(getitem_332);  getitem_332 = None
    mul_117 = getitem_331 * gelu_35;  getitem_331 = gelu_35 = None
    up_blocks_0_attentions_0_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").ff.net, "1")(mul_117);  mul_117 = None
    up_blocks_0_attentions_0_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "1").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_1_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_1_ff_net_1 = None
    add_131 = up_blocks_0_attentions_0_transformer_blocks_1_ff_net_2 + add_130;  up_blocks_0_attentions_0_transformer_blocks_1_ff_net_2 = add_130 = None
    up_blocks_0_attentions_0_transformer_blocks_2_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").norm1(add_131)
    up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_2_norm1)
    up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_2_norm1)
    up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_2_norm1);  up_blocks_0_attentions_0_transformer_blocks_2_norm1 = None
    size_504 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_q.size()
    getitem_333 = size_504[0]
    getitem_334 = size_504[1]
    getitem_335 = size_504[2];  size_504 = None
    size_505 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_q.size(0)
    size_506 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_q.size(1)
    view_288 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_q.view(size_505, size_506, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_q = size_505 = size_506 = None
    transpose_360 = view_288.transpose(1, 2);  view_288 = None
    size_507 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_k.size(0)
    size_508 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_k.size(1)
    view_289 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_k.view(size_507, size_508, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_k = size_507 = size_508 = None
    transpose_361 = view_289.transpose(1, 2);  view_289 = None
    size_509 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_v.size(0)
    size_510 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_v.size(1)
    view_290 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_v.view(size_509, size_510, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_v = size_509 = size_510 = None
    transpose_362 = view_290.transpose(1, 2);  view_290 = None
    transpose_363 = transpose_361.transpose(-2, -1);  transpose_361 = None
    matmul_144 = torch.matmul(transpose_360, transpose_363);  transpose_360 = transpose_363 = None
    mul_118 = matmul_144 * 0.125;  matmul_144 = None
    softmax_72 = torch.softmax(mul_118, dim = -1);  mul_118 = None
    matmul_145 = torch.matmul(softmax_72, transpose_362);  softmax_72 = transpose_362 = None
    transpose_364 = matmul_145.transpose(1, 2);  matmul_145 = None
    contiguous_77 = transpose_364.contiguous();  transpose_364 = None
    view_291 = contiguous_77.view(getitem_333, getitem_334, getitem_335);  contiguous_77 = getitem_333 = getitem_334 = getitem_335 = None
    up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn1.to_out, "0")(view_291);  view_291 = None
    up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_out_0 = None
    add_132 = up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_out_1 + add_131;  up_blocks_0_attentions_0_transformer_blocks_2_attn1_to_out_1 = add_131 = None
    up_blocks_0_attentions_0_transformer_blocks_2_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").norm2(add_132)
    up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_2_norm2)
    up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_2_norm2)
    up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_2_norm2);  up_blocks_0_attentions_0_transformer_blocks_2_norm2 = None
    size_511 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_q.size()
    getitem_336 = size_511[0]
    getitem_337 = size_511[1]
    getitem_338 = size_511[2];  size_511 = None
    size_512 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_q.size(0)
    size_513 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_q.size(1)
    view_292 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_q.view(size_512, size_513, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_q = size_512 = size_513 = None
    transpose_365 = view_292.transpose(1, 2);  view_292 = None
    size_514 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_k.size(0)
    size_515 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_k.size(1)
    view_293 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_k.view(size_514, size_515, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_k = size_514 = size_515 = None
    transpose_366 = view_293.transpose(1, 2);  view_293 = None
    size_516 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_v.size(0)
    size_517 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_v.size(1)
    view_294 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_v.view(size_516, size_517, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_v = size_516 = size_517 = None
    transpose_367 = view_294.transpose(1, 2);  view_294 = None
    transpose_368 = transpose_366.transpose(-2, -1);  transpose_366 = None
    matmul_146 = torch.matmul(transpose_365, transpose_368);  transpose_365 = transpose_368 = None
    mul_119 = matmul_146 * 0.125;  matmul_146 = None
    softmax_73 = torch.softmax(mul_119, dim = -1);  mul_119 = None
    matmul_147 = torch.matmul(softmax_73, transpose_367);  softmax_73 = transpose_367 = None
    transpose_369 = matmul_147.transpose(1, 2);  matmul_147 = None
    contiguous_78 = transpose_369.contiguous();  transpose_369 = None
    view_295 = contiguous_78.view(getitem_336, getitem_337, getitem_338);  contiguous_78 = getitem_336 = getitem_337 = getitem_338 = None
    up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn2.to_out, "0")(view_295);  view_295 = None
    up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_out_0 = None
    add_133 = up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_out_1 + add_132;  up_blocks_0_attentions_0_transformer_blocks_2_attn2_to_out_1 = add_132 = None
    up_blocks_0_attentions_0_transformer_blocks_2_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").norm3(add_133)
    up_blocks_0_attentions_0_transformer_blocks_2_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_2_norm3);  up_blocks_0_attentions_0_transformer_blocks_2_norm3 = None
    chunk_36 = up_blocks_0_attentions_0_transformer_blocks_2_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_2_ff_net_0_proj = None
    getitem_339 = chunk_36[0]
    getitem_340 = chunk_36[1];  chunk_36 = None
    gelu_36 = torch._C._nn.gelu(getitem_340);  getitem_340 = None
    mul_120 = getitem_339 * gelu_36;  getitem_339 = gelu_36 = None
    up_blocks_0_attentions_0_transformer_blocks_2_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").ff.net, "1")(mul_120);  mul_120 = None
    up_blocks_0_attentions_0_transformer_blocks_2_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "2").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_2_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_2_ff_net_1 = None
    add_134 = up_blocks_0_attentions_0_transformer_blocks_2_ff_net_2 + add_133;  up_blocks_0_attentions_0_transformer_blocks_2_ff_net_2 = add_133 = None
    up_blocks_0_attentions_0_transformer_blocks_3_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").norm1(add_134)
    up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_3_norm1)
    up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_3_norm1)
    up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_3_norm1);  up_blocks_0_attentions_0_transformer_blocks_3_norm1 = None
    size_518 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_q.size()
    getitem_341 = size_518[0]
    getitem_342 = size_518[1]
    getitem_343 = size_518[2];  size_518 = None
    size_519 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_q.size(0)
    size_520 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_q.size(1)
    view_296 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_q.view(size_519, size_520, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_q = size_519 = size_520 = None
    transpose_370 = view_296.transpose(1, 2);  view_296 = None
    size_521 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_k.size(0)
    size_522 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_k.size(1)
    view_297 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_k.view(size_521, size_522, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_k = size_521 = size_522 = None
    transpose_371 = view_297.transpose(1, 2);  view_297 = None
    size_523 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_v.size(0)
    size_524 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_v.size(1)
    view_298 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_v.view(size_523, size_524, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_v = size_523 = size_524 = None
    transpose_372 = view_298.transpose(1, 2);  view_298 = None
    transpose_373 = transpose_371.transpose(-2, -1);  transpose_371 = None
    matmul_148 = torch.matmul(transpose_370, transpose_373);  transpose_370 = transpose_373 = None
    mul_121 = matmul_148 * 0.125;  matmul_148 = None
    softmax_74 = torch.softmax(mul_121, dim = -1);  mul_121 = None
    matmul_149 = torch.matmul(softmax_74, transpose_372);  softmax_74 = transpose_372 = None
    transpose_374 = matmul_149.transpose(1, 2);  matmul_149 = None
    contiguous_79 = transpose_374.contiguous();  transpose_374 = None
    view_299 = contiguous_79.view(getitem_341, getitem_342, getitem_343);  contiguous_79 = getitem_341 = getitem_342 = getitem_343 = None
    up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn1.to_out, "0")(view_299);  view_299 = None
    up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_out_0 = None
    add_135 = up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_out_1 + add_134;  up_blocks_0_attentions_0_transformer_blocks_3_attn1_to_out_1 = add_134 = None
    up_blocks_0_attentions_0_transformer_blocks_3_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").norm2(add_135)
    up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_3_norm2)
    up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_3_norm2)
    up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_3_norm2);  up_blocks_0_attentions_0_transformer_blocks_3_norm2 = None
    size_525 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_q.size()
    getitem_344 = size_525[0]
    getitem_345 = size_525[1]
    getitem_346 = size_525[2];  size_525 = None
    size_526 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_q.size(0)
    size_527 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_q.size(1)
    view_300 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_q.view(size_526, size_527, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_q = size_526 = size_527 = None
    transpose_375 = view_300.transpose(1, 2);  view_300 = None
    size_528 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_k.size(0)
    size_529 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_k.size(1)
    view_301 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_k.view(size_528, size_529, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_k = size_528 = size_529 = None
    transpose_376 = view_301.transpose(1, 2);  view_301 = None
    size_530 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_v.size(0)
    size_531 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_v.size(1)
    view_302 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_v.view(size_530, size_531, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_v = size_530 = size_531 = None
    transpose_377 = view_302.transpose(1, 2);  view_302 = None
    transpose_378 = transpose_376.transpose(-2, -1);  transpose_376 = None
    matmul_150 = torch.matmul(transpose_375, transpose_378);  transpose_375 = transpose_378 = None
    mul_122 = matmul_150 * 0.125;  matmul_150 = None
    softmax_75 = torch.softmax(mul_122, dim = -1);  mul_122 = None
    matmul_151 = torch.matmul(softmax_75, transpose_377);  softmax_75 = transpose_377 = None
    transpose_379 = matmul_151.transpose(1, 2);  matmul_151 = None
    contiguous_80 = transpose_379.contiguous();  transpose_379 = None
    view_303 = contiguous_80.view(getitem_344, getitem_345, getitem_346);  contiguous_80 = getitem_344 = getitem_345 = getitem_346 = None
    up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn2.to_out, "0")(view_303);  view_303 = None
    up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_out_0 = None
    add_136 = up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_out_1 + add_135;  up_blocks_0_attentions_0_transformer_blocks_3_attn2_to_out_1 = add_135 = None
    up_blocks_0_attentions_0_transformer_blocks_3_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").norm3(add_136)
    up_blocks_0_attentions_0_transformer_blocks_3_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_3_norm3);  up_blocks_0_attentions_0_transformer_blocks_3_norm3 = None
    chunk_37 = up_blocks_0_attentions_0_transformer_blocks_3_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_3_ff_net_0_proj = None
    getitem_347 = chunk_37[0]
    getitem_348 = chunk_37[1];  chunk_37 = None
    gelu_37 = torch._C._nn.gelu(getitem_348);  getitem_348 = None
    mul_123 = getitem_347 * gelu_37;  getitem_347 = gelu_37 = None
    up_blocks_0_attentions_0_transformer_blocks_3_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").ff.net, "1")(mul_123);  mul_123 = None
    up_blocks_0_attentions_0_transformer_blocks_3_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "3").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_3_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_3_ff_net_1 = None
    add_137 = up_blocks_0_attentions_0_transformer_blocks_3_ff_net_2 + add_136;  up_blocks_0_attentions_0_transformer_blocks_3_ff_net_2 = add_136 = None
    up_blocks_0_attentions_0_transformer_blocks_4_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").norm1(add_137)
    up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_4_norm1)
    up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_4_norm1)
    up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_4_norm1);  up_blocks_0_attentions_0_transformer_blocks_4_norm1 = None
    size_532 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_q.size()
    getitem_349 = size_532[0]
    getitem_350 = size_532[1]
    getitem_351 = size_532[2];  size_532 = None
    size_533 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_q.size(0)
    size_534 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_q.size(1)
    view_304 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_q.view(size_533, size_534, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_q = size_533 = size_534 = None
    transpose_380 = view_304.transpose(1, 2);  view_304 = None
    size_535 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_k.size(0)
    size_536 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_k.size(1)
    view_305 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_k.view(size_535, size_536, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_k = size_535 = size_536 = None
    transpose_381 = view_305.transpose(1, 2);  view_305 = None
    size_537 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_v.size(0)
    size_538 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_v.size(1)
    view_306 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_v.view(size_537, size_538, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_v = size_537 = size_538 = None
    transpose_382 = view_306.transpose(1, 2);  view_306 = None
    transpose_383 = transpose_381.transpose(-2, -1);  transpose_381 = None
    matmul_152 = torch.matmul(transpose_380, transpose_383);  transpose_380 = transpose_383 = None
    mul_124 = matmul_152 * 0.125;  matmul_152 = None
    softmax_76 = torch.softmax(mul_124, dim = -1);  mul_124 = None
    matmul_153 = torch.matmul(softmax_76, transpose_382);  softmax_76 = transpose_382 = None
    transpose_384 = matmul_153.transpose(1, 2);  matmul_153 = None
    contiguous_81 = transpose_384.contiguous();  transpose_384 = None
    view_307 = contiguous_81.view(getitem_349, getitem_350, getitem_351);  contiguous_81 = getitem_349 = getitem_350 = getitem_351 = None
    up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn1.to_out, "0")(view_307);  view_307 = None
    up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_out_0 = None
    add_138 = up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_out_1 + add_137;  up_blocks_0_attentions_0_transformer_blocks_4_attn1_to_out_1 = add_137 = None
    up_blocks_0_attentions_0_transformer_blocks_4_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").norm2(add_138)
    up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_4_norm2)
    up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_4_norm2)
    up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_4_norm2);  up_blocks_0_attentions_0_transformer_blocks_4_norm2 = None
    size_539 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_q.size()
    getitem_352 = size_539[0]
    getitem_353 = size_539[1]
    getitem_354 = size_539[2];  size_539 = None
    size_540 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_q.size(0)
    size_541 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_q.size(1)
    view_308 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_q.view(size_540, size_541, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_q = size_540 = size_541 = None
    transpose_385 = view_308.transpose(1, 2);  view_308 = None
    size_542 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_k.size(0)
    size_543 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_k.size(1)
    view_309 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_k.view(size_542, size_543, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_k = size_542 = size_543 = None
    transpose_386 = view_309.transpose(1, 2);  view_309 = None
    size_544 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_v.size(0)
    size_545 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_v.size(1)
    view_310 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_v.view(size_544, size_545, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_v = size_544 = size_545 = None
    transpose_387 = view_310.transpose(1, 2);  view_310 = None
    transpose_388 = transpose_386.transpose(-2, -1);  transpose_386 = None
    matmul_154 = torch.matmul(transpose_385, transpose_388);  transpose_385 = transpose_388 = None
    mul_125 = matmul_154 * 0.125;  matmul_154 = None
    softmax_77 = torch.softmax(mul_125, dim = -1);  mul_125 = None
    matmul_155 = torch.matmul(softmax_77, transpose_387);  softmax_77 = transpose_387 = None
    transpose_389 = matmul_155.transpose(1, 2);  matmul_155 = None
    contiguous_82 = transpose_389.contiguous();  transpose_389 = None
    view_311 = contiguous_82.view(getitem_352, getitem_353, getitem_354);  contiguous_82 = getitem_352 = getitem_353 = getitem_354 = None
    up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn2.to_out, "0")(view_311);  view_311 = None
    up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_out_0 = None
    add_139 = up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_out_1 + add_138;  up_blocks_0_attentions_0_transformer_blocks_4_attn2_to_out_1 = add_138 = None
    up_blocks_0_attentions_0_transformer_blocks_4_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").norm3(add_139)
    up_blocks_0_attentions_0_transformer_blocks_4_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_4_norm3);  up_blocks_0_attentions_0_transformer_blocks_4_norm3 = None
    chunk_38 = up_blocks_0_attentions_0_transformer_blocks_4_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_4_ff_net_0_proj = None
    getitem_355 = chunk_38[0]
    getitem_356 = chunk_38[1];  chunk_38 = None
    gelu_38 = torch._C._nn.gelu(getitem_356);  getitem_356 = None
    mul_126 = getitem_355 * gelu_38;  getitem_355 = gelu_38 = None
    up_blocks_0_attentions_0_transformer_blocks_4_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").ff.net, "1")(mul_126);  mul_126 = None
    up_blocks_0_attentions_0_transformer_blocks_4_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "4").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_4_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_4_ff_net_1 = None
    add_140 = up_blocks_0_attentions_0_transformer_blocks_4_ff_net_2 + add_139;  up_blocks_0_attentions_0_transformer_blocks_4_ff_net_2 = add_139 = None
    up_blocks_0_attentions_0_transformer_blocks_5_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").norm1(add_140)
    up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_5_norm1)
    up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_5_norm1)
    up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_5_norm1);  up_blocks_0_attentions_0_transformer_blocks_5_norm1 = None
    size_546 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_q.size()
    getitem_357 = size_546[0]
    getitem_358 = size_546[1]
    getitem_359 = size_546[2];  size_546 = None
    size_547 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_q.size(0)
    size_548 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_q.size(1)
    view_312 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_q.view(size_547, size_548, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_q = size_547 = size_548 = None
    transpose_390 = view_312.transpose(1, 2);  view_312 = None
    size_549 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_k.size(0)
    size_550 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_k.size(1)
    view_313 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_k.view(size_549, size_550, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_k = size_549 = size_550 = None
    transpose_391 = view_313.transpose(1, 2);  view_313 = None
    size_551 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_v.size(0)
    size_552 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_v.size(1)
    view_314 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_v.view(size_551, size_552, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_v = size_551 = size_552 = None
    transpose_392 = view_314.transpose(1, 2);  view_314 = None
    transpose_393 = transpose_391.transpose(-2, -1);  transpose_391 = None
    matmul_156 = torch.matmul(transpose_390, transpose_393);  transpose_390 = transpose_393 = None
    mul_127 = matmul_156 * 0.125;  matmul_156 = None
    softmax_78 = torch.softmax(mul_127, dim = -1);  mul_127 = None
    matmul_157 = torch.matmul(softmax_78, transpose_392);  softmax_78 = transpose_392 = None
    transpose_394 = matmul_157.transpose(1, 2);  matmul_157 = None
    contiguous_83 = transpose_394.contiguous();  transpose_394 = None
    view_315 = contiguous_83.view(getitem_357, getitem_358, getitem_359);  contiguous_83 = getitem_357 = getitem_358 = getitem_359 = None
    up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn1.to_out, "0")(view_315);  view_315 = None
    up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_out_0 = None
    add_141 = up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_out_1 + add_140;  up_blocks_0_attentions_0_transformer_blocks_5_attn1_to_out_1 = add_140 = None
    up_blocks_0_attentions_0_transformer_blocks_5_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").norm2(add_141)
    up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_5_norm2)
    up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_5_norm2)
    up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_5_norm2);  up_blocks_0_attentions_0_transformer_blocks_5_norm2 = None
    size_553 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_q.size()
    getitem_360 = size_553[0]
    getitem_361 = size_553[1]
    getitem_362 = size_553[2];  size_553 = None
    size_554 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_q.size(0)
    size_555 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_q.size(1)
    view_316 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_q.view(size_554, size_555, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_q = size_554 = size_555 = None
    transpose_395 = view_316.transpose(1, 2);  view_316 = None
    size_556 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_k.size(0)
    size_557 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_k.size(1)
    view_317 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_k.view(size_556, size_557, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_k = size_556 = size_557 = None
    transpose_396 = view_317.transpose(1, 2);  view_317 = None
    size_558 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_v.size(0)
    size_559 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_v.size(1)
    view_318 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_v.view(size_558, size_559, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_v = size_558 = size_559 = None
    transpose_397 = view_318.transpose(1, 2);  view_318 = None
    transpose_398 = transpose_396.transpose(-2, -1);  transpose_396 = None
    matmul_158 = torch.matmul(transpose_395, transpose_398);  transpose_395 = transpose_398 = None
    mul_128 = matmul_158 * 0.125;  matmul_158 = None
    softmax_79 = torch.softmax(mul_128, dim = -1);  mul_128 = None
    matmul_159 = torch.matmul(softmax_79, transpose_397);  softmax_79 = transpose_397 = None
    transpose_399 = matmul_159.transpose(1, 2);  matmul_159 = None
    contiguous_84 = transpose_399.contiguous();  transpose_399 = None
    view_319 = contiguous_84.view(getitem_360, getitem_361, getitem_362);  contiguous_84 = getitem_360 = getitem_361 = getitem_362 = None
    up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn2.to_out, "0")(view_319);  view_319 = None
    up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_out_0 = None
    add_142 = up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_out_1 + add_141;  up_blocks_0_attentions_0_transformer_blocks_5_attn2_to_out_1 = add_141 = None
    up_blocks_0_attentions_0_transformer_blocks_5_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").norm3(add_142)
    up_blocks_0_attentions_0_transformer_blocks_5_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_5_norm3);  up_blocks_0_attentions_0_transformer_blocks_5_norm3 = None
    chunk_39 = up_blocks_0_attentions_0_transformer_blocks_5_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_5_ff_net_0_proj = None
    getitem_363 = chunk_39[0]
    getitem_364 = chunk_39[1];  chunk_39 = None
    gelu_39 = torch._C._nn.gelu(getitem_364);  getitem_364 = None
    mul_129 = getitem_363 * gelu_39;  getitem_363 = gelu_39 = None
    up_blocks_0_attentions_0_transformer_blocks_5_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").ff.net, "1")(mul_129);  mul_129 = None
    up_blocks_0_attentions_0_transformer_blocks_5_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "5").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_5_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_5_ff_net_1 = None
    add_143 = up_blocks_0_attentions_0_transformer_blocks_5_ff_net_2 + add_142;  up_blocks_0_attentions_0_transformer_blocks_5_ff_net_2 = add_142 = None
    up_blocks_0_attentions_0_transformer_blocks_6_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").norm1(add_143)
    up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_6_norm1)
    up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_6_norm1)
    up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_6_norm1);  up_blocks_0_attentions_0_transformer_blocks_6_norm1 = None
    size_560 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_q.size()
    getitem_365 = size_560[0]
    getitem_366 = size_560[1]
    getitem_367 = size_560[2];  size_560 = None
    size_561 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_q.size(0)
    size_562 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_q.size(1)
    view_320 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_q.view(size_561, size_562, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_q = size_561 = size_562 = None
    transpose_400 = view_320.transpose(1, 2);  view_320 = None
    size_563 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_k.size(0)
    size_564 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_k.size(1)
    view_321 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_k.view(size_563, size_564, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_k = size_563 = size_564 = None
    transpose_401 = view_321.transpose(1, 2);  view_321 = None
    size_565 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_v.size(0)
    size_566 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_v.size(1)
    view_322 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_v.view(size_565, size_566, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_v = size_565 = size_566 = None
    transpose_402 = view_322.transpose(1, 2);  view_322 = None
    transpose_403 = transpose_401.transpose(-2, -1);  transpose_401 = None
    matmul_160 = torch.matmul(transpose_400, transpose_403);  transpose_400 = transpose_403 = None
    mul_130 = matmul_160 * 0.125;  matmul_160 = None
    softmax_80 = torch.softmax(mul_130, dim = -1);  mul_130 = None
    matmul_161 = torch.matmul(softmax_80, transpose_402);  softmax_80 = transpose_402 = None
    transpose_404 = matmul_161.transpose(1, 2);  matmul_161 = None
    contiguous_85 = transpose_404.contiguous();  transpose_404 = None
    view_323 = contiguous_85.view(getitem_365, getitem_366, getitem_367);  contiguous_85 = getitem_365 = getitem_366 = getitem_367 = None
    up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn1.to_out, "0")(view_323);  view_323 = None
    up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_out_0 = None
    add_144 = up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_out_1 + add_143;  up_blocks_0_attentions_0_transformer_blocks_6_attn1_to_out_1 = add_143 = None
    up_blocks_0_attentions_0_transformer_blocks_6_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").norm2(add_144)
    up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_6_norm2)
    up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_6_norm2)
    up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_6_norm2);  up_blocks_0_attentions_0_transformer_blocks_6_norm2 = None
    size_567 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_q.size()
    getitem_368 = size_567[0]
    getitem_369 = size_567[1]
    getitem_370 = size_567[2];  size_567 = None
    size_568 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_q.size(0)
    size_569 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_q.size(1)
    view_324 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_q.view(size_568, size_569, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_q = size_568 = size_569 = None
    transpose_405 = view_324.transpose(1, 2);  view_324 = None
    size_570 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_k.size(0)
    size_571 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_k.size(1)
    view_325 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_k.view(size_570, size_571, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_k = size_570 = size_571 = None
    transpose_406 = view_325.transpose(1, 2);  view_325 = None
    size_572 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_v.size(0)
    size_573 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_v.size(1)
    view_326 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_v.view(size_572, size_573, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_v = size_572 = size_573 = None
    transpose_407 = view_326.transpose(1, 2);  view_326 = None
    transpose_408 = transpose_406.transpose(-2, -1);  transpose_406 = None
    matmul_162 = torch.matmul(transpose_405, transpose_408);  transpose_405 = transpose_408 = None
    mul_131 = matmul_162 * 0.125;  matmul_162 = None
    softmax_81 = torch.softmax(mul_131, dim = -1);  mul_131 = None
    matmul_163 = torch.matmul(softmax_81, transpose_407);  softmax_81 = transpose_407 = None
    transpose_409 = matmul_163.transpose(1, 2);  matmul_163 = None
    contiguous_86 = transpose_409.contiguous();  transpose_409 = None
    view_327 = contiguous_86.view(getitem_368, getitem_369, getitem_370);  contiguous_86 = getitem_368 = getitem_369 = getitem_370 = None
    up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn2.to_out, "0")(view_327);  view_327 = None
    up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_out_0 = None
    add_145 = up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_out_1 + add_144;  up_blocks_0_attentions_0_transformer_blocks_6_attn2_to_out_1 = add_144 = None
    up_blocks_0_attentions_0_transformer_blocks_6_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").norm3(add_145)
    up_blocks_0_attentions_0_transformer_blocks_6_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_6_norm3);  up_blocks_0_attentions_0_transformer_blocks_6_norm3 = None
    chunk_40 = up_blocks_0_attentions_0_transformer_blocks_6_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_6_ff_net_0_proj = None
    getitem_371 = chunk_40[0]
    getitem_372 = chunk_40[1];  chunk_40 = None
    gelu_40 = torch._C._nn.gelu(getitem_372);  getitem_372 = None
    mul_132 = getitem_371 * gelu_40;  getitem_371 = gelu_40 = None
    up_blocks_0_attentions_0_transformer_blocks_6_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").ff.net, "1")(mul_132);  mul_132 = None
    up_blocks_0_attentions_0_transformer_blocks_6_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "6").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_6_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_6_ff_net_1 = None
    add_146 = up_blocks_0_attentions_0_transformer_blocks_6_ff_net_2 + add_145;  up_blocks_0_attentions_0_transformer_blocks_6_ff_net_2 = add_145 = None
    up_blocks_0_attentions_0_transformer_blocks_7_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").norm1(add_146)
    up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_7_norm1)
    up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_7_norm1)
    up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_7_norm1);  up_blocks_0_attentions_0_transformer_blocks_7_norm1 = None
    size_574 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_q.size()
    getitem_373 = size_574[0]
    getitem_374 = size_574[1]
    getitem_375 = size_574[2];  size_574 = None
    size_575 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_q.size(0)
    size_576 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_q.size(1)
    view_328 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_q.view(size_575, size_576, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_q = size_575 = size_576 = None
    transpose_410 = view_328.transpose(1, 2);  view_328 = None
    size_577 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_k.size(0)
    size_578 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_k.size(1)
    view_329 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_k.view(size_577, size_578, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_k = size_577 = size_578 = None
    transpose_411 = view_329.transpose(1, 2);  view_329 = None
    size_579 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_v.size(0)
    size_580 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_v.size(1)
    view_330 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_v.view(size_579, size_580, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_v = size_579 = size_580 = None
    transpose_412 = view_330.transpose(1, 2);  view_330 = None
    transpose_413 = transpose_411.transpose(-2, -1);  transpose_411 = None
    matmul_164 = torch.matmul(transpose_410, transpose_413);  transpose_410 = transpose_413 = None
    mul_133 = matmul_164 * 0.125;  matmul_164 = None
    softmax_82 = torch.softmax(mul_133, dim = -1);  mul_133 = None
    matmul_165 = torch.matmul(softmax_82, transpose_412);  softmax_82 = transpose_412 = None
    transpose_414 = matmul_165.transpose(1, 2);  matmul_165 = None
    contiguous_87 = transpose_414.contiguous();  transpose_414 = None
    view_331 = contiguous_87.view(getitem_373, getitem_374, getitem_375);  contiguous_87 = getitem_373 = getitem_374 = getitem_375 = None
    up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn1.to_out, "0")(view_331);  view_331 = None
    up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_out_0 = None
    add_147 = up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_out_1 + add_146;  up_blocks_0_attentions_0_transformer_blocks_7_attn1_to_out_1 = add_146 = None
    up_blocks_0_attentions_0_transformer_blocks_7_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").norm2(add_147)
    up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_7_norm2)
    up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_7_norm2)
    up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_7_norm2);  up_blocks_0_attentions_0_transformer_blocks_7_norm2 = None
    size_581 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_q.size()
    getitem_376 = size_581[0]
    getitem_377 = size_581[1]
    getitem_378 = size_581[2];  size_581 = None
    size_582 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_q.size(0)
    size_583 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_q.size(1)
    view_332 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_q.view(size_582, size_583, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_q = size_582 = size_583 = None
    transpose_415 = view_332.transpose(1, 2);  view_332 = None
    size_584 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_k.size(0)
    size_585 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_k.size(1)
    view_333 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_k.view(size_584, size_585, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_k = size_584 = size_585 = None
    transpose_416 = view_333.transpose(1, 2);  view_333 = None
    size_586 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_v.size(0)
    size_587 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_v.size(1)
    view_334 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_v.view(size_586, size_587, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_v = size_586 = size_587 = None
    transpose_417 = view_334.transpose(1, 2);  view_334 = None
    transpose_418 = transpose_416.transpose(-2, -1);  transpose_416 = None
    matmul_166 = torch.matmul(transpose_415, transpose_418);  transpose_415 = transpose_418 = None
    mul_134 = matmul_166 * 0.125;  matmul_166 = None
    softmax_83 = torch.softmax(mul_134, dim = -1);  mul_134 = None
    matmul_167 = torch.matmul(softmax_83, transpose_417);  softmax_83 = transpose_417 = None
    transpose_419 = matmul_167.transpose(1, 2);  matmul_167 = None
    contiguous_88 = transpose_419.contiguous();  transpose_419 = None
    view_335 = contiguous_88.view(getitem_376, getitem_377, getitem_378);  contiguous_88 = getitem_376 = getitem_377 = getitem_378 = None
    up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn2.to_out, "0")(view_335);  view_335 = None
    up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_out_0 = None
    add_148 = up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_out_1 + add_147;  up_blocks_0_attentions_0_transformer_blocks_7_attn2_to_out_1 = add_147 = None
    up_blocks_0_attentions_0_transformer_blocks_7_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").norm3(add_148)
    up_blocks_0_attentions_0_transformer_blocks_7_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_7_norm3);  up_blocks_0_attentions_0_transformer_blocks_7_norm3 = None
    chunk_41 = up_blocks_0_attentions_0_transformer_blocks_7_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_7_ff_net_0_proj = None
    getitem_379 = chunk_41[0]
    getitem_380 = chunk_41[1];  chunk_41 = None
    gelu_41 = torch._C._nn.gelu(getitem_380);  getitem_380 = None
    mul_135 = getitem_379 * gelu_41;  getitem_379 = gelu_41 = None
    up_blocks_0_attentions_0_transformer_blocks_7_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").ff.net, "1")(mul_135);  mul_135 = None
    up_blocks_0_attentions_0_transformer_blocks_7_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "7").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_7_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_7_ff_net_1 = None
    add_149 = up_blocks_0_attentions_0_transformer_blocks_7_ff_net_2 + add_148;  up_blocks_0_attentions_0_transformer_blocks_7_ff_net_2 = add_148 = None
    up_blocks_0_attentions_0_transformer_blocks_8_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").norm1(add_149)
    up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_8_norm1)
    up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_8_norm1)
    up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_8_norm1);  up_blocks_0_attentions_0_transformer_blocks_8_norm1 = None
    size_588 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_q.size()
    getitem_381 = size_588[0]
    getitem_382 = size_588[1]
    getitem_383 = size_588[2];  size_588 = None
    size_589 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_q.size(0)
    size_590 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_q.size(1)
    view_336 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_q.view(size_589, size_590, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_q = size_589 = size_590 = None
    transpose_420 = view_336.transpose(1, 2);  view_336 = None
    size_591 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_k.size(0)
    size_592 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_k.size(1)
    view_337 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_k.view(size_591, size_592, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_k = size_591 = size_592 = None
    transpose_421 = view_337.transpose(1, 2);  view_337 = None
    size_593 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_v.size(0)
    size_594 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_v.size(1)
    view_338 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_v.view(size_593, size_594, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_v = size_593 = size_594 = None
    transpose_422 = view_338.transpose(1, 2);  view_338 = None
    transpose_423 = transpose_421.transpose(-2, -1);  transpose_421 = None
    matmul_168 = torch.matmul(transpose_420, transpose_423);  transpose_420 = transpose_423 = None
    mul_136 = matmul_168 * 0.125;  matmul_168 = None
    softmax_84 = torch.softmax(mul_136, dim = -1);  mul_136 = None
    matmul_169 = torch.matmul(softmax_84, transpose_422);  softmax_84 = transpose_422 = None
    transpose_424 = matmul_169.transpose(1, 2);  matmul_169 = None
    contiguous_89 = transpose_424.contiguous();  transpose_424 = None
    view_339 = contiguous_89.view(getitem_381, getitem_382, getitem_383);  contiguous_89 = getitem_381 = getitem_382 = getitem_383 = None
    up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn1.to_out, "0")(view_339);  view_339 = None
    up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_out_0 = None
    add_150 = up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_out_1 + add_149;  up_blocks_0_attentions_0_transformer_blocks_8_attn1_to_out_1 = add_149 = None
    up_blocks_0_attentions_0_transformer_blocks_8_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").norm2(add_150)
    up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_8_norm2)
    up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_8_norm2)
    up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_8_norm2);  up_blocks_0_attentions_0_transformer_blocks_8_norm2 = None
    size_595 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_q.size()
    getitem_384 = size_595[0]
    getitem_385 = size_595[1]
    getitem_386 = size_595[2];  size_595 = None
    size_596 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_q.size(0)
    size_597 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_q.size(1)
    view_340 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_q.view(size_596, size_597, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_q = size_596 = size_597 = None
    transpose_425 = view_340.transpose(1, 2);  view_340 = None
    size_598 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_k.size(0)
    size_599 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_k.size(1)
    view_341 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_k.view(size_598, size_599, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_k = size_598 = size_599 = None
    transpose_426 = view_341.transpose(1, 2);  view_341 = None
    size_600 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_v.size(0)
    size_601 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_v.size(1)
    view_342 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_v.view(size_600, size_601, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_v = size_600 = size_601 = None
    transpose_427 = view_342.transpose(1, 2);  view_342 = None
    transpose_428 = transpose_426.transpose(-2, -1);  transpose_426 = None
    matmul_170 = torch.matmul(transpose_425, transpose_428);  transpose_425 = transpose_428 = None
    mul_137 = matmul_170 * 0.125;  matmul_170 = None
    softmax_85 = torch.softmax(mul_137, dim = -1);  mul_137 = None
    matmul_171 = torch.matmul(softmax_85, transpose_427);  softmax_85 = transpose_427 = None
    transpose_429 = matmul_171.transpose(1, 2);  matmul_171 = None
    contiguous_90 = transpose_429.contiguous();  transpose_429 = None
    view_343 = contiguous_90.view(getitem_384, getitem_385, getitem_386);  contiguous_90 = getitem_384 = getitem_385 = getitem_386 = None
    up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn2.to_out, "0")(view_343);  view_343 = None
    up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_out_0 = None
    add_151 = up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_out_1 + add_150;  up_blocks_0_attentions_0_transformer_blocks_8_attn2_to_out_1 = add_150 = None
    up_blocks_0_attentions_0_transformer_blocks_8_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").norm3(add_151)
    up_blocks_0_attentions_0_transformer_blocks_8_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_8_norm3);  up_blocks_0_attentions_0_transformer_blocks_8_norm3 = None
    chunk_42 = up_blocks_0_attentions_0_transformer_blocks_8_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_8_ff_net_0_proj = None
    getitem_387 = chunk_42[0]
    getitem_388 = chunk_42[1];  chunk_42 = None
    gelu_42 = torch._C._nn.gelu(getitem_388);  getitem_388 = None
    mul_138 = getitem_387 * gelu_42;  getitem_387 = gelu_42 = None
    up_blocks_0_attentions_0_transformer_blocks_8_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").ff.net, "1")(mul_138);  mul_138 = None
    up_blocks_0_attentions_0_transformer_blocks_8_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "8").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_8_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_8_ff_net_1 = None
    add_152 = up_blocks_0_attentions_0_transformer_blocks_8_ff_net_2 + add_151;  up_blocks_0_attentions_0_transformer_blocks_8_ff_net_2 = add_151 = None
    up_blocks_0_attentions_0_transformer_blocks_9_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").norm1(add_152)
    up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn1.to_q(up_blocks_0_attentions_0_transformer_blocks_9_norm1)
    up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn1.to_k(up_blocks_0_attentions_0_transformer_blocks_9_norm1)
    up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn1.to_v(up_blocks_0_attentions_0_transformer_blocks_9_norm1);  up_blocks_0_attentions_0_transformer_blocks_9_norm1 = None
    size_602 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_q.size()
    getitem_389 = size_602[0]
    getitem_390 = size_602[1]
    getitem_391 = size_602[2];  size_602 = None
    size_603 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_q.size(0)
    size_604 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_q.size(1)
    view_344 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_q.view(size_603, size_604, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_q = size_603 = size_604 = None
    transpose_430 = view_344.transpose(1, 2);  view_344 = None
    size_605 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_k.size(0)
    size_606 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_k.size(1)
    view_345 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_k.view(size_605, size_606, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_k = size_605 = size_606 = None
    transpose_431 = view_345.transpose(1, 2);  view_345 = None
    size_607 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_v.size(0)
    size_608 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_v.size(1)
    view_346 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_v.view(size_607, size_608, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_v = size_607 = size_608 = None
    transpose_432 = view_346.transpose(1, 2);  view_346 = None
    transpose_433 = transpose_431.transpose(-2, -1);  transpose_431 = None
    matmul_172 = torch.matmul(transpose_430, transpose_433);  transpose_430 = transpose_433 = None
    mul_139 = matmul_172 * 0.125;  matmul_172 = None
    softmax_86 = torch.softmax(mul_139, dim = -1);  mul_139 = None
    matmul_173 = torch.matmul(softmax_86, transpose_432);  softmax_86 = transpose_432 = None
    transpose_434 = matmul_173.transpose(1, 2);  matmul_173 = None
    contiguous_91 = transpose_434.contiguous();  transpose_434 = None
    view_347 = contiguous_91.view(getitem_389, getitem_390, getitem_391);  contiguous_91 = getitem_389 = getitem_390 = getitem_391 = None
    up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn1.to_out, "0")(view_347);  view_347 = None
    up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn1.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_out_0 = None
    add_153 = up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_out_1 + add_152;  up_blocks_0_attentions_0_transformer_blocks_9_attn1_to_out_1 = add_152 = None
    up_blocks_0_attentions_0_transformer_blocks_9_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").norm2(add_153)
    up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn2.to_q(up_blocks_0_attentions_0_transformer_blocks_9_norm2)
    up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn2.to_k(up_blocks_0_attentions_0_transformer_blocks_9_norm2)
    up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn2.to_v(up_blocks_0_attentions_0_transformer_blocks_9_norm2);  up_blocks_0_attentions_0_transformer_blocks_9_norm2 = None
    size_609 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_q.size()
    getitem_392 = size_609[0]
    getitem_393 = size_609[1]
    getitem_394 = size_609[2];  size_609 = None
    size_610 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_q.size(0)
    size_611 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_q.size(1)
    view_348 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_q.view(size_610, size_611, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_q = size_610 = size_611 = None
    transpose_435 = view_348.transpose(1, 2);  view_348 = None
    size_612 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_k.size(0)
    size_613 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_k.size(1)
    view_349 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_k.view(size_612, size_613, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_k = size_612 = size_613 = None
    transpose_436 = view_349.transpose(1, 2);  view_349 = None
    size_614 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_v.size(0)
    size_615 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_v.size(1)
    view_350 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_v.view(size_614, size_615, 20, 64);  up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_v = size_614 = size_615 = None
    transpose_437 = view_350.transpose(1, 2);  view_350 = None
    transpose_438 = transpose_436.transpose(-2, -1);  transpose_436 = None
    matmul_174 = torch.matmul(transpose_435, transpose_438);  transpose_435 = transpose_438 = None
    mul_140 = matmul_174 * 0.125;  matmul_174 = None
    softmax_87 = torch.softmax(mul_140, dim = -1);  mul_140 = None
    matmul_175 = torch.matmul(softmax_87, transpose_437);  softmax_87 = transpose_437 = None
    transpose_439 = matmul_175.transpose(1, 2);  matmul_175 = None
    contiguous_92 = transpose_439.contiguous();  transpose_439 = None
    view_351 = contiguous_92.view(getitem_392, getitem_393, getitem_394);  contiguous_92 = getitem_392 = getitem_393 = getitem_394 = None
    up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn2.to_out, "0")(view_351);  view_351 = None
    up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").attn2.to_out, "1")(up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_out_0);  up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_out_0 = None
    add_154 = up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_out_1 + add_153;  up_blocks_0_attentions_0_transformer_blocks_9_attn2_to_out_1 = add_153 = None
    up_blocks_0_attentions_0_transformer_blocks_9_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").norm3(add_154)
    up_blocks_0_attentions_0_transformer_blocks_9_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").ff.net, "0").proj(up_blocks_0_attentions_0_transformer_blocks_9_norm3);  up_blocks_0_attentions_0_transformer_blocks_9_norm3 = None
    chunk_43 = up_blocks_0_attentions_0_transformer_blocks_9_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_0_transformer_blocks_9_ff_net_0_proj = None
    getitem_395 = chunk_43[0]
    getitem_396 = chunk_43[1];  chunk_43 = None
    gelu_43 = torch._C._nn.gelu(getitem_396);  getitem_396 = None
    mul_141 = getitem_395 * gelu_43;  getitem_395 = gelu_43 = None
    up_blocks_0_attentions_0_transformer_blocks_9_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").ff.net, "1")(mul_141);  mul_141 = None
    up_blocks_0_attentions_0_transformer_blocks_9_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "0").transformer_blocks, "9").ff.net, "2")(up_blocks_0_attentions_0_transformer_blocks_9_ff_net_1);  up_blocks_0_attentions_0_transformer_blocks_9_ff_net_1 = None
    add_155 = up_blocks_0_attentions_0_transformer_blocks_9_ff_net_2 + add_154;  up_blocks_0_attentions_0_transformer_blocks_9_ff_net_2 = add_154 = None
    up_blocks_0_attentions_0_proj_out = getattr(getattr(self.up_blocks, "0").attentions, "0").proj_out(add_155);  add_155 = None
    reshape_12 = up_blocks_0_attentions_0_proj_out.reshape(getitem_312, getitem_314, getitem_315, getitem_316);  up_blocks_0_attentions_0_proj_out = getitem_312 = getitem_314 = getitem_315 = getitem_316 = None
    permute_11 = reshape_12.permute(0, 3, 1, 2);  reshape_12 = None
    contiguous_93 = permute_11.contiguous();  permute_11 = None
    add_156 = contiguous_93 + add_125;  contiguous_93 = add_125 = None
    cat_3 = torch.cat([add_156, add_55], dim = 1);  add_156 = add_55 = None
    up_blocks_0_resnets_1_norm1 = getattr(getattr(self.up_blocks, "0").resnets, "1").norm1(cat_3)
    up_blocks_0_resnets_1_nonlinearity = getattr(getattr(self.up_blocks, "0").resnets, "1").nonlinearity(up_blocks_0_resnets_1_norm1);  up_blocks_0_resnets_1_norm1 = None
    up_blocks_0_resnets_1_conv1 = getattr(getattr(self.up_blocks, "0").resnets, "1").conv1(up_blocks_0_resnets_1_nonlinearity);  up_blocks_0_resnets_1_nonlinearity = None
    up_blocks_0_resnets_1_nonlinearity_1 = getattr(getattr(self.up_blocks, "0").resnets, "1").nonlinearity(add)
    up_blocks_0_resnets_1_time_emb_proj = getattr(getattr(self.up_blocks, "0").resnets, "1").time_emb_proj(up_blocks_0_resnets_1_nonlinearity_1);  up_blocks_0_resnets_1_nonlinearity_1 = None
    getitem_397 = up_blocks_0_resnets_1_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_0_resnets_1_time_emb_proj = None
    add_157 = up_blocks_0_resnets_1_conv1 + getitem_397;  up_blocks_0_resnets_1_conv1 = getitem_397 = None
    up_blocks_0_resnets_1_norm2 = getattr(getattr(self.up_blocks, "0").resnets, "1").norm2(add_157);  add_157 = None
    up_blocks_0_resnets_1_nonlinearity_2 = getattr(getattr(self.up_blocks, "0").resnets, "1").nonlinearity(up_blocks_0_resnets_1_norm2);  up_blocks_0_resnets_1_norm2 = None
    up_blocks_0_resnets_1_dropout = getattr(getattr(self.up_blocks, "0").resnets, "1").dropout(up_blocks_0_resnets_1_nonlinearity_2);  up_blocks_0_resnets_1_nonlinearity_2 = None
    up_blocks_0_resnets_1_conv2 = getattr(getattr(self.up_blocks, "0").resnets, "1").conv2(up_blocks_0_resnets_1_dropout);  up_blocks_0_resnets_1_dropout = None
    up_blocks_0_resnets_1_conv_shortcut = getattr(getattr(self.up_blocks, "0").resnets, "1").conv_shortcut(cat_3);  cat_3 = None
    add_158 = up_blocks_0_resnets_1_conv_shortcut + up_blocks_0_resnets_1_conv2;  up_blocks_0_resnets_1_conv_shortcut = up_blocks_0_resnets_1_conv2 = None
    getattr_21 = add_158.shape
    getitem_398 = getattr_21[0]
    getitem_399 = getattr_21[1]
    getitem_400 = getattr_21[2]
    getitem_401 = getattr_21[3];  getattr_21 = None
    up_blocks_0_attentions_1_norm = getattr(getattr(self.up_blocks, "0").attentions, "1").norm(add_158)
    getattr_22 = up_blocks_0_attentions_1_norm.shape
    getitem_402 = getattr_22[1];  getattr_22 = None
    permute_12 = up_blocks_0_attentions_1_norm.permute(0, 2, 3, 1);  up_blocks_0_attentions_1_norm = None
    mul_142 = getitem_400 * getitem_401
    reshape_13 = permute_12.reshape(getitem_398, mul_142, getitem_402);  permute_12 = mul_142 = None
    up_blocks_0_attentions_1_proj_in = getattr(getattr(self.up_blocks, "0").attentions, "1").proj_in(reshape_13);  reshape_13 = None
    up_blocks_0_attentions_1_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").norm1(up_blocks_0_attentions_1_proj_in)
    up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_0_norm1)
    up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_0_norm1)
    up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_0_norm1);  up_blocks_0_attentions_1_transformer_blocks_0_norm1 = None
    size_616 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.size()
    getitem_403 = size_616[0]
    getitem_404 = size_616[1]
    getitem_405 = size_616[2];  size_616 = None
    size_617 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.size(0)
    size_618 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.size(1)
    view_352 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.view(size_617, size_618, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q = size_617 = size_618 = None
    transpose_440 = view_352.transpose(1, 2);  view_352 = None
    size_619 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.size(0)
    size_620 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.size(1)
    view_353 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.view(size_619, size_620, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k = size_619 = size_620 = None
    transpose_441 = view_353.transpose(1, 2);  view_353 = None
    size_621 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.size(0)
    size_622 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.size(1)
    view_354 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.view(size_621, size_622, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v = size_621 = size_622 = None
    transpose_442 = view_354.transpose(1, 2);  view_354 = None
    transpose_443 = transpose_441.transpose(-2, -1);  transpose_441 = None
    matmul_176 = torch.matmul(transpose_440, transpose_443);  transpose_440 = transpose_443 = None
    mul_143 = matmul_176 * 0.125;  matmul_176 = None
    softmax_88 = torch.softmax(mul_143, dim = -1);  mul_143 = None
    matmul_177 = torch.matmul(softmax_88, transpose_442);  softmax_88 = transpose_442 = None
    transpose_444 = matmul_177.transpose(1, 2);  matmul_177 = None
    contiguous_94 = transpose_444.contiguous();  transpose_444 = None
    view_355 = contiguous_94.view(getitem_403, getitem_404, getitem_405);  contiguous_94 = getitem_403 = getitem_404 = getitem_405 = None
    up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn1.to_out, "0")(view_355);  view_355 = None
    up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0 = None
    add_159 = up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_1 + up_blocks_0_attentions_1_proj_in;  up_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_1 = up_blocks_0_attentions_1_proj_in = None
    up_blocks_0_attentions_1_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").norm2(add_159)
    up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_0_norm2)
    up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_0_norm2)
    up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_0_norm2);  up_blocks_0_attentions_1_transformer_blocks_0_norm2 = None
    size_623 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.size()
    getitem_406 = size_623[0]
    getitem_407 = size_623[1]
    getitem_408 = size_623[2];  size_623 = None
    size_624 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.size(0)
    size_625 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.size(1)
    view_356 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.view(size_624, size_625, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q = size_624 = size_625 = None
    transpose_445 = view_356.transpose(1, 2);  view_356 = None
    size_626 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.size(0)
    size_627 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.size(1)
    view_357 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.view(size_626, size_627, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k = size_626 = size_627 = None
    transpose_446 = view_357.transpose(1, 2);  view_357 = None
    size_628 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.size(0)
    size_629 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.size(1)
    view_358 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.view(size_628, size_629, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v = size_628 = size_629 = None
    transpose_447 = view_358.transpose(1, 2);  view_358 = None
    transpose_448 = transpose_446.transpose(-2, -1);  transpose_446 = None
    matmul_178 = torch.matmul(transpose_445, transpose_448);  transpose_445 = transpose_448 = None
    mul_144 = matmul_178 * 0.125;  matmul_178 = None
    softmax_89 = torch.softmax(mul_144, dim = -1);  mul_144 = None
    matmul_179 = torch.matmul(softmax_89, transpose_447);  softmax_89 = transpose_447 = None
    transpose_449 = matmul_179.transpose(1, 2);  matmul_179 = None
    contiguous_95 = transpose_449.contiguous();  transpose_449 = None
    view_359 = contiguous_95.view(getitem_406, getitem_407, getitem_408);  contiguous_95 = getitem_406 = getitem_407 = getitem_408 = None
    up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn2.to_out, "0")(view_359);  view_359 = None
    up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0 = None
    add_160 = up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_1 + add_159;  up_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_1 = add_159 = None
    up_blocks_0_attentions_1_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").norm3(add_160)
    up_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_0_norm3);  up_blocks_0_attentions_1_transformer_blocks_0_norm3 = None
    chunk_44 = up_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj = None
    getitem_409 = chunk_44[0]
    getitem_410 = chunk_44[1];  chunk_44 = None
    gelu_44 = torch._C._nn.gelu(getitem_410);  getitem_410 = None
    mul_145 = getitem_409 * gelu_44;  getitem_409 = gelu_44 = None
    up_blocks_0_attentions_1_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").ff.net, "1")(mul_145);  mul_145 = None
    up_blocks_0_attentions_1_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "0").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_0_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_0_ff_net_1 = None
    add_161 = up_blocks_0_attentions_1_transformer_blocks_0_ff_net_2 + add_160;  up_blocks_0_attentions_1_transformer_blocks_0_ff_net_2 = add_160 = None
    up_blocks_0_attentions_1_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").norm1(add_161)
    up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_1_norm1)
    up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_1_norm1)
    up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_1_norm1);  up_blocks_0_attentions_1_transformer_blocks_1_norm1 = None
    size_630 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_q.size()
    getitem_411 = size_630[0]
    getitem_412 = size_630[1]
    getitem_413 = size_630[2];  size_630 = None
    size_631 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_q.size(0)
    size_632 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_q.size(1)
    view_360 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_q.view(size_631, size_632, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_q = size_631 = size_632 = None
    transpose_450 = view_360.transpose(1, 2);  view_360 = None
    size_633 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_k.size(0)
    size_634 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_k.size(1)
    view_361 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_k.view(size_633, size_634, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_k = size_633 = size_634 = None
    transpose_451 = view_361.transpose(1, 2);  view_361 = None
    size_635 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_v.size(0)
    size_636 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_v.size(1)
    view_362 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_v.view(size_635, size_636, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_v = size_635 = size_636 = None
    transpose_452 = view_362.transpose(1, 2);  view_362 = None
    transpose_453 = transpose_451.transpose(-2, -1);  transpose_451 = None
    matmul_180 = torch.matmul(transpose_450, transpose_453);  transpose_450 = transpose_453 = None
    mul_146 = matmul_180 * 0.125;  matmul_180 = None
    softmax_90 = torch.softmax(mul_146, dim = -1);  mul_146 = None
    matmul_181 = torch.matmul(softmax_90, transpose_452);  softmax_90 = transpose_452 = None
    transpose_454 = matmul_181.transpose(1, 2);  matmul_181 = None
    contiguous_96 = transpose_454.contiguous();  transpose_454 = None
    view_363 = contiguous_96.view(getitem_411, getitem_412, getitem_413);  contiguous_96 = getitem_411 = getitem_412 = getitem_413 = None
    up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn1.to_out, "0")(view_363);  view_363 = None
    up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_out_0 = None
    add_162 = up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_out_1 + add_161;  up_blocks_0_attentions_1_transformer_blocks_1_attn1_to_out_1 = add_161 = None
    up_blocks_0_attentions_1_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").norm2(add_162)
    up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_1_norm2)
    up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_1_norm2)
    up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_1_norm2);  up_blocks_0_attentions_1_transformer_blocks_1_norm2 = None
    size_637 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_q.size()
    getitem_414 = size_637[0]
    getitem_415 = size_637[1]
    getitem_416 = size_637[2];  size_637 = None
    size_638 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_q.size(0)
    size_639 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_q.size(1)
    view_364 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_q.view(size_638, size_639, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_q = size_638 = size_639 = None
    transpose_455 = view_364.transpose(1, 2);  view_364 = None
    size_640 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_k.size(0)
    size_641 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_k.size(1)
    view_365 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_k.view(size_640, size_641, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_k = size_640 = size_641 = None
    transpose_456 = view_365.transpose(1, 2);  view_365 = None
    size_642 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_v.size(0)
    size_643 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_v.size(1)
    view_366 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_v.view(size_642, size_643, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_v = size_642 = size_643 = None
    transpose_457 = view_366.transpose(1, 2);  view_366 = None
    transpose_458 = transpose_456.transpose(-2, -1);  transpose_456 = None
    matmul_182 = torch.matmul(transpose_455, transpose_458);  transpose_455 = transpose_458 = None
    mul_147 = matmul_182 * 0.125;  matmul_182 = None
    softmax_91 = torch.softmax(mul_147, dim = -1);  mul_147 = None
    matmul_183 = torch.matmul(softmax_91, transpose_457);  softmax_91 = transpose_457 = None
    transpose_459 = matmul_183.transpose(1, 2);  matmul_183 = None
    contiguous_97 = transpose_459.contiguous();  transpose_459 = None
    view_367 = contiguous_97.view(getitem_414, getitem_415, getitem_416);  contiguous_97 = getitem_414 = getitem_415 = getitem_416 = None
    up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn2.to_out, "0")(view_367);  view_367 = None
    up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_out_0 = None
    add_163 = up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_out_1 + add_162;  up_blocks_0_attentions_1_transformer_blocks_1_attn2_to_out_1 = add_162 = None
    up_blocks_0_attentions_1_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").norm3(add_163)
    up_blocks_0_attentions_1_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_1_norm3);  up_blocks_0_attentions_1_transformer_blocks_1_norm3 = None
    chunk_45 = up_blocks_0_attentions_1_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_1_ff_net_0_proj = None
    getitem_417 = chunk_45[0]
    getitem_418 = chunk_45[1];  chunk_45 = None
    gelu_45 = torch._C._nn.gelu(getitem_418);  getitem_418 = None
    mul_148 = getitem_417 * gelu_45;  getitem_417 = gelu_45 = None
    up_blocks_0_attentions_1_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").ff.net, "1")(mul_148);  mul_148 = None
    up_blocks_0_attentions_1_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "1").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_1_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_1_ff_net_1 = None
    add_164 = up_blocks_0_attentions_1_transformer_blocks_1_ff_net_2 + add_163;  up_blocks_0_attentions_1_transformer_blocks_1_ff_net_2 = add_163 = None
    up_blocks_0_attentions_1_transformer_blocks_2_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").norm1(add_164)
    up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_2_norm1)
    up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_2_norm1)
    up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_2_norm1);  up_blocks_0_attentions_1_transformer_blocks_2_norm1 = None
    size_644 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_q.size()
    getitem_419 = size_644[0]
    getitem_420 = size_644[1]
    getitem_421 = size_644[2];  size_644 = None
    size_645 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_q.size(0)
    size_646 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_q.size(1)
    view_368 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_q.view(size_645, size_646, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_q = size_645 = size_646 = None
    transpose_460 = view_368.transpose(1, 2);  view_368 = None
    size_647 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_k.size(0)
    size_648 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_k.size(1)
    view_369 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_k.view(size_647, size_648, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_k = size_647 = size_648 = None
    transpose_461 = view_369.transpose(1, 2);  view_369 = None
    size_649 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_v.size(0)
    size_650 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_v.size(1)
    view_370 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_v.view(size_649, size_650, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_v = size_649 = size_650 = None
    transpose_462 = view_370.transpose(1, 2);  view_370 = None
    transpose_463 = transpose_461.transpose(-2, -1);  transpose_461 = None
    matmul_184 = torch.matmul(transpose_460, transpose_463);  transpose_460 = transpose_463 = None
    mul_149 = matmul_184 * 0.125;  matmul_184 = None
    softmax_92 = torch.softmax(mul_149, dim = -1);  mul_149 = None
    matmul_185 = torch.matmul(softmax_92, transpose_462);  softmax_92 = transpose_462 = None
    transpose_464 = matmul_185.transpose(1, 2);  matmul_185 = None
    contiguous_98 = transpose_464.contiguous();  transpose_464 = None
    view_371 = contiguous_98.view(getitem_419, getitem_420, getitem_421);  contiguous_98 = getitem_419 = getitem_420 = getitem_421 = None
    up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn1.to_out, "0")(view_371);  view_371 = None
    up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_out_0 = None
    add_165 = up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_out_1 + add_164;  up_blocks_0_attentions_1_transformer_blocks_2_attn1_to_out_1 = add_164 = None
    up_blocks_0_attentions_1_transformer_blocks_2_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").norm2(add_165)
    up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_2_norm2)
    up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_2_norm2)
    up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_2_norm2);  up_blocks_0_attentions_1_transformer_blocks_2_norm2 = None
    size_651 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_q.size()
    getitem_422 = size_651[0]
    getitem_423 = size_651[1]
    getitem_424 = size_651[2];  size_651 = None
    size_652 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_q.size(0)
    size_653 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_q.size(1)
    view_372 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_q.view(size_652, size_653, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_q = size_652 = size_653 = None
    transpose_465 = view_372.transpose(1, 2);  view_372 = None
    size_654 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_k.size(0)
    size_655 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_k.size(1)
    view_373 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_k.view(size_654, size_655, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_k = size_654 = size_655 = None
    transpose_466 = view_373.transpose(1, 2);  view_373 = None
    size_656 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_v.size(0)
    size_657 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_v.size(1)
    view_374 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_v.view(size_656, size_657, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_v = size_656 = size_657 = None
    transpose_467 = view_374.transpose(1, 2);  view_374 = None
    transpose_468 = transpose_466.transpose(-2, -1);  transpose_466 = None
    matmul_186 = torch.matmul(transpose_465, transpose_468);  transpose_465 = transpose_468 = None
    mul_150 = matmul_186 * 0.125;  matmul_186 = None
    softmax_93 = torch.softmax(mul_150, dim = -1);  mul_150 = None
    matmul_187 = torch.matmul(softmax_93, transpose_467);  softmax_93 = transpose_467 = None
    transpose_469 = matmul_187.transpose(1, 2);  matmul_187 = None
    contiguous_99 = transpose_469.contiguous();  transpose_469 = None
    view_375 = contiguous_99.view(getitem_422, getitem_423, getitem_424);  contiguous_99 = getitem_422 = getitem_423 = getitem_424 = None
    up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn2.to_out, "0")(view_375);  view_375 = None
    up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_out_0 = None
    add_166 = up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_out_1 + add_165;  up_blocks_0_attentions_1_transformer_blocks_2_attn2_to_out_1 = add_165 = None
    up_blocks_0_attentions_1_transformer_blocks_2_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").norm3(add_166)
    up_blocks_0_attentions_1_transformer_blocks_2_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_2_norm3);  up_blocks_0_attentions_1_transformer_blocks_2_norm3 = None
    chunk_46 = up_blocks_0_attentions_1_transformer_blocks_2_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_2_ff_net_0_proj = None
    getitem_425 = chunk_46[0]
    getitem_426 = chunk_46[1];  chunk_46 = None
    gelu_46 = torch._C._nn.gelu(getitem_426);  getitem_426 = None
    mul_151 = getitem_425 * gelu_46;  getitem_425 = gelu_46 = None
    up_blocks_0_attentions_1_transformer_blocks_2_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").ff.net, "1")(mul_151);  mul_151 = None
    up_blocks_0_attentions_1_transformer_blocks_2_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "2").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_2_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_2_ff_net_1 = None
    add_167 = up_blocks_0_attentions_1_transformer_blocks_2_ff_net_2 + add_166;  up_blocks_0_attentions_1_transformer_blocks_2_ff_net_2 = add_166 = None
    up_blocks_0_attentions_1_transformer_blocks_3_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").norm1(add_167)
    up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_3_norm1)
    up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_3_norm1)
    up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_3_norm1);  up_blocks_0_attentions_1_transformer_blocks_3_norm1 = None
    size_658 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_q.size()
    getitem_427 = size_658[0]
    getitem_428 = size_658[1]
    getitem_429 = size_658[2];  size_658 = None
    size_659 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_q.size(0)
    size_660 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_q.size(1)
    view_376 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_q.view(size_659, size_660, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_q = size_659 = size_660 = None
    transpose_470 = view_376.transpose(1, 2);  view_376 = None
    size_661 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_k.size(0)
    size_662 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_k.size(1)
    view_377 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_k.view(size_661, size_662, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_k = size_661 = size_662 = None
    transpose_471 = view_377.transpose(1, 2);  view_377 = None
    size_663 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_v.size(0)
    size_664 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_v.size(1)
    view_378 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_v.view(size_663, size_664, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_v = size_663 = size_664 = None
    transpose_472 = view_378.transpose(1, 2);  view_378 = None
    transpose_473 = transpose_471.transpose(-2, -1);  transpose_471 = None
    matmul_188 = torch.matmul(transpose_470, transpose_473);  transpose_470 = transpose_473 = None
    mul_152 = matmul_188 * 0.125;  matmul_188 = None
    softmax_94 = torch.softmax(mul_152, dim = -1);  mul_152 = None
    matmul_189 = torch.matmul(softmax_94, transpose_472);  softmax_94 = transpose_472 = None
    transpose_474 = matmul_189.transpose(1, 2);  matmul_189 = None
    contiguous_100 = transpose_474.contiguous();  transpose_474 = None
    view_379 = contiguous_100.view(getitem_427, getitem_428, getitem_429);  contiguous_100 = getitem_427 = getitem_428 = getitem_429 = None
    up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn1.to_out, "0")(view_379);  view_379 = None
    up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_out_0 = None
    add_168 = up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_out_1 + add_167;  up_blocks_0_attentions_1_transformer_blocks_3_attn1_to_out_1 = add_167 = None
    up_blocks_0_attentions_1_transformer_blocks_3_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").norm2(add_168)
    up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_3_norm2)
    up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_3_norm2)
    up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_3_norm2);  up_blocks_0_attentions_1_transformer_blocks_3_norm2 = None
    size_665 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_q.size()
    getitem_430 = size_665[0]
    getitem_431 = size_665[1]
    getitem_432 = size_665[2];  size_665 = None
    size_666 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_q.size(0)
    size_667 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_q.size(1)
    view_380 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_q.view(size_666, size_667, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_q = size_666 = size_667 = None
    transpose_475 = view_380.transpose(1, 2);  view_380 = None
    size_668 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_k.size(0)
    size_669 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_k.size(1)
    view_381 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_k.view(size_668, size_669, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_k = size_668 = size_669 = None
    transpose_476 = view_381.transpose(1, 2);  view_381 = None
    size_670 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_v.size(0)
    size_671 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_v.size(1)
    view_382 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_v.view(size_670, size_671, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_v = size_670 = size_671 = None
    transpose_477 = view_382.transpose(1, 2);  view_382 = None
    transpose_478 = transpose_476.transpose(-2, -1);  transpose_476 = None
    matmul_190 = torch.matmul(transpose_475, transpose_478);  transpose_475 = transpose_478 = None
    mul_153 = matmul_190 * 0.125;  matmul_190 = None
    softmax_95 = torch.softmax(mul_153, dim = -1);  mul_153 = None
    matmul_191 = torch.matmul(softmax_95, transpose_477);  softmax_95 = transpose_477 = None
    transpose_479 = matmul_191.transpose(1, 2);  matmul_191 = None
    contiguous_101 = transpose_479.contiguous();  transpose_479 = None
    view_383 = contiguous_101.view(getitem_430, getitem_431, getitem_432);  contiguous_101 = getitem_430 = getitem_431 = getitem_432 = None
    up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn2.to_out, "0")(view_383);  view_383 = None
    up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_out_0 = None
    add_169 = up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_out_1 + add_168;  up_blocks_0_attentions_1_transformer_blocks_3_attn2_to_out_1 = add_168 = None
    up_blocks_0_attentions_1_transformer_blocks_3_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").norm3(add_169)
    up_blocks_0_attentions_1_transformer_blocks_3_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_3_norm3);  up_blocks_0_attentions_1_transformer_blocks_3_norm3 = None
    chunk_47 = up_blocks_0_attentions_1_transformer_blocks_3_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_3_ff_net_0_proj = None
    getitem_433 = chunk_47[0]
    getitem_434 = chunk_47[1];  chunk_47 = None
    gelu_47 = torch._C._nn.gelu(getitem_434);  getitem_434 = None
    mul_154 = getitem_433 * gelu_47;  getitem_433 = gelu_47 = None
    up_blocks_0_attentions_1_transformer_blocks_3_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").ff.net, "1")(mul_154);  mul_154 = None
    up_blocks_0_attentions_1_transformer_blocks_3_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "3").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_3_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_3_ff_net_1 = None
    add_170 = up_blocks_0_attentions_1_transformer_blocks_3_ff_net_2 + add_169;  up_blocks_0_attentions_1_transformer_blocks_3_ff_net_2 = add_169 = None
    up_blocks_0_attentions_1_transformer_blocks_4_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").norm1(add_170)
    up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_4_norm1)
    up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_4_norm1)
    up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_4_norm1);  up_blocks_0_attentions_1_transformer_blocks_4_norm1 = None
    size_672 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_q.size()
    getitem_435 = size_672[0]
    getitem_436 = size_672[1]
    getitem_437 = size_672[2];  size_672 = None
    size_673 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_q.size(0)
    size_674 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_q.size(1)
    view_384 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_q.view(size_673, size_674, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_q = size_673 = size_674 = None
    transpose_480 = view_384.transpose(1, 2);  view_384 = None
    size_675 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_k.size(0)
    size_676 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_k.size(1)
    view_385 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_k.view(size_675, size_676, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_k = size_675 = size_676 = None
    transpose_481 = view_385.transpose(1, 2);  view_385 = None
    size_677 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_v.size(0)
    size_678 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_v.size(1)
    view_386 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_v.view(size_677, size_678, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_v = size_677 = size_678 = None
    transpose_482 = view_386.transpose(1, 2);  view_386 = None
    transpose_483 = transpose_481.transpose(-2, -1);  transpose_481 = None
    matmul_192 = torch.matmul(transpose_480, transpose_483);  transpose_480 = transpose_483 = None
    mul_155 = matmul_192 * 0.125;  matmul_192 = None
    softmax_96 = torch.softmax(mul_155, dim = -1);  mul_155 = None
    matmul_193 = torch.matmul(softmax_96, transpose_482);  softmax_96 = transpose_482 = None
    transpose_484 = matmul_193.transpose(1, 2);  matmul_193 = None
    contiguous_102 = transpose_484.contiguous();  transpose_484 = None
    view_387 = contiguous_102.view(getitem_435, getitem_436, getitem_437);  contiguous_102 = getitem_435 = getitem_436 = getitem_437 = None
    up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn1.to_out, "0")(view_387);  view_387 = None
    up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_out_0 = None
    add_171 = up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_out_1 + add_170;  up_blocks_0_attentions_1_transformer_blocks_4_attn1_to_out_1 = add_170 = None
    up_blocks_0_attentions_1_transformer_blocks_4_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").norm2(add_171)
    up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_4_norm2)
    up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_4_norm2)
    up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_4_norm2);  up_blocks_0_attentions_1_transformer_blocks_4_norm2 = None
    size_679 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_q.size()
    getitem_438 = size_679[0]
    getitem_439 = size_679[1]
    getitem_440 = size_679[2];  size_679 = None
    size_680 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_q.size(0)
    size_681 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_q.size(1)
    view_388 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_q.view(size_680, size_681, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_q = size_680 = size_681 = None
    transpose_485 = view_388.transpose(1, 2);  view_388 = None
    size_682 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_k.size(0)
    size_683 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_k.size(1)
    view_389 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_k.view(size_682, size_683, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_k = size_682 = size_683 = None
    transpose_486 = view_389.transpose(1, 2);  view_389 = None
    size_684 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_v.size(0)
    size_685 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_v.size(1)
    view_390 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_v.view(size_684, size_685, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_v = size_684 = size_685 = None
    transpose_487 = view_390.transpose(1, 2);  view_390 = None
    transpose_488 = transpose_486.transpose(-2, -1);  transpose_486 = None
    matmul_194 = torch.matmul(transpose_485, transpose_488);  transpose_485 = transpose_488 = None
    mul_156 = matmul_194 * 0.125;  matmul_194 = None
    softmax_97 = torch.softmax(mul_156, dim = -1);  mul_156 = None
    matmul_195 = torch.matmul(softmax_97, transpose_487);  softmax_97 = transpose_487 = None
    transpose_489 = matmul_195.transpose(1, 2);  matmul_195 = None
    contiguous_103 = transpose_489.contiguous();  transpose_489 = None
    view_391 = contiguous_103.view(getitem_438, getitem_439, getitem_440);  contiguous_103 = getitem_438 = getitem_439 = getitem_440 = None
    up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn2.to_out, "0")(view_391);  view_391 = None
    up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_out_0 = None
    add_172 = up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_out_1 + add_171;  up_blocks_0_attentions_1_transformer_blocks_4_attn2_to_out_1 = add_171 = None
    up_blocks_0_attentions_1_transformer_blocks_4_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").norm3(add_172)
    up_blocks_0_attentions_1_transformer_blocks_4_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_4_norm3);  up_blocks_0_attentions_1_transformer_blocks_4_norm3 = None
    chunk_48 = up_blocks_0_attentions_1_transformer_blocks_4_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_4_ff_net_0_proj = None
    getitem_441 = chunk_48[0]
    getitem_442 = chunk_48[1];  chunk_48 = None
    gelu_48 = torch._C._nn.gelu(getitem_442);  getitem_442 = None
    mul_157 = getitem_441 * gelu_48;  getitem_441 = gelu_48 = None
    up_blocks_0_attentions_1_transformer_blocks_4_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").ff.net, "1")(mul_157);  mul_157 = None
    up_blocks_0_attentions_1_transformer_blocks_4_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "4").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_4_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_4_ff_net_1 = None
    add_173 = up_blocks_0_attentions_1_transformer_blocks_4_ff_net_2 + add_172;  up_blocks_0_attentions_1_transformer_blocks_4_ff_net_2 = add_172 = None
    up_blocks_0_attentions_1_transformer_blocks_5_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").norm1(add_173)
    up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_5_norm1)
    up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_5_norm1)
    up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_5_norm1);  up_blocks_0_attentions_1_transformer_blocks_5_norm1 = None
    size_686 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_q.size()
    getitem_443 = size_686[0]
    getitem_444 = size_686[1]
    getitem_445 = size_686[2];  size_686 = None
    size_687 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_q.size(0)
    size_688 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_q.size(1)
    view_392 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_q.view(size_687, size_688, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_q = size_687 = size_688 = None
    transpose_490 = view_392.transpose(1, 2);  view_392 = None
    size_689 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_k.size(0)
    size_690 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_k.size(1)
    view_393 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_k.view(size_689, size_690, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_k = size_689 = size_690 = None
    transpose_491 = view_393.transpose(1, 2);  view_393 = None
    size_691 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_v.size(0)
    size_692 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_v.size(1)
    view_394 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_v.view(size_691, size_692, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_v = size_691 = size_692 = None
    transpose_492 = view_394.transpose(1, 2);  view_394 = None
    transpose_493 = transpose_491.transpose(-2, -1);  transpose_491 = None
    matmul_196 = torch.matmul(transpose_490, transpose_493);  transpose_490 = transpose_493 = None
    mul_158 = matmul_196 * 0.125;  matmul_196 = None
    softmax_98 = torch.softmax(mul_158, dim = -1);  mul_158 = None
    matmul_197 = torch.matmul(softmax_98, transpose_492);  softmax_98 = transpose_492 = None
    transpose_494 = matmul_197.transpose(1, 2);  matmul_197 = None
    contiguous_104 = transpose_494.contiguous();  transpose_494 = None
    view_395 = contiguous_104.view(getitem_443, getitem_444, getitem_445);  contiguous_104 = getitem_443 = getitem_444 = getitem_445 = None
    up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn1.to_out, "0")(view_395);  view_395 = None
    up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_out_0 = None
    add_174 = up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_out_1 + add_173;  up_blocks_0_attentions_1_transformer_blocks_5_attn1_to_out_1 = add_173 = None
    up_blocks_0_attentions_1_transformer_blocks_5_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").norm2(add_174)
    up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_5_norm2)
    up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_5_norm2)
    up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_5_norm2);  up_blocks_0_attentions_1_transformer_blocks_5_norm2 = None
    size_693 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_q.size()
    getitem_446 = size_693[0]
    getitem_447 = size_693[1]
    getitem_448 = size_693[2];  size_693 = None
    size_694 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_q.size(0)
    size_695 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_q.size(1)
    view_396 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_q.view(size_694, size_695, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_q = size_694 = size_695 = None
    transpose_495 = view_396.transpose(1, 2);  view_396 = None
    size_696 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_k.size(0)
    size_697 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_k.size(1)
    view_397 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_k.view(size_696, size_697, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_k = size_696 = size_697 = None
    transpose_496 = view_397.transpose(1, 2);  view_397 = None
    size_698 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_v.size(0)
    size_699 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_v.size(1)
    view_398 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_v.view(size_698, size_699, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_v = size_698 = size_699 = None
    transpose_497 = view_398.transpose(1, 2);  view_398 = None
    transpose_498 = transpose_496.transpose(-2, -1);  transpose_496 = None
    matmul_198 = torch.matmul(transpose_495, transpose_498);  transpose_495 = transpose_498 = None
    mul_159 = matmul_198 * 0.125;  matmul_198 = None
    softmax_99 = torch.softmax(mul_159, dim = -1);  mul_159 = None
    matmul_199 = torch.matmul(softmax_99, transpose_497);  softmax_99 = transpose_497 = None
    transpose_499 = matmul_199.transpose(1, 2);  matmul_199 = None
    contiguous_105 = transpose_499.contiguous();  transpose_499 = None
    view_399 = contiguous_105.view(getitem_446, getitem_447, getitem_448);  contiguous_105 = getitem_446 = getitem_447 = getitem_448 = None
    up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn2.to_out, "0")(view_399);  view_399 = None
    up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_out_0 = None
    add_175 = up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_out_1 + add_174;  up_blocks_0_attentions_1_transformer_blocks_5_attn2_to_out_1 = add_174 = None
    up_blocks_0_attentions_1_transformer_blocks_5_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").norm3(add_175)
    up_blocks_0_attentions_1_transformer_blocks_5_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_5_norm3);  up_blocks_0_attentions_1_transformer_blocks_5_norm3 = None
    chunk_49 = up_blocks_0_attentions_1_transformer_blocks_5_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_5_ff_net_0_proj = None
    getitem_449 = chunk_49[0]
    getitem_450 = chunk_49[1];  chunk_49 = None
    gelu_49 = torch._C._nn.gelu(getitem_450);  getitem_450 = None
    mul_160 = getitem_449 * gelu_49;  getitem_449 = gelu_49 = None
    up_blocks_0_attentions_1_transformer_blocks_5_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").ff.net, "1")(mul_160);  mul_160 = None
    up_blocks_0_attentions_1_transformer_blocks_5_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "5").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_5_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_5_ff_net_1 = None
    add_176 = up_blocks_0_attentions_1_transformer_blocks_5_ff_net_2 + add_175;  up_blocks_0_attentions_1_transformer_blocks_5_ff_net_2 = add_175 = None
    up_blocks_0_attentions_1_transformer_blocks_6_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").norm1(add_176)
    up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_6_norm1)
    up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_6_norm1)
    up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_6_norm1);  up_blocks_0_attentions_1_transformer_blocks_6_norm1 = None
    size_700 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_q.size()
    getitem_451 = size_700[0]
    getitem_452 = size_700[1]
    getitem_453 = size_700[2];  size_700 = None
    size_701 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_q.size(0)
    size_702 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_q.size(1)
    view_400 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_q.view(size_701, size_702, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_q = size_701 = size_702 = None
    transpose_500 = view_400.transpose(1, 2);  view_400 = None
    size_703 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_k.size(0)
    size_704 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_k.size(1)
    view_401 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_k.view(size_703, size_704, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_k = size_703 = size_704 = None
    transpose_501 = view_401.transpose(1, 2);  view_401 = None
    size_705 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_v.size(0)
    size_706 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_v.size(1)
    view_402 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_v.view(size_705, size_706, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_v = size_705 = size_706 = None
    transpose_502 = view_402.transpose(1, 2);  view_402 = None
    transpose_503 = transpose_501.transpose(-2, -1);  transpose_501 = None
    matmul_200 = torch.matmul(transpose_500, transpose_503);  transpose_500 = transpose_503 = None
    mul_161 = matmul_200 * 0.125;  matmul_200 = None
    softmax_100 = torch.softmax(mul_161, dim = -1);  mul_161 = None
    matmul_201 = torch.matmul(softmax_100, transpose_502);  softmax_100 = transpose_502 = None
    transpose_504 = matmul_201.transpose(1, 2);  matmul_201 = None
    contiguous_106 = transpose_504.contiguous();  transpose_504 = None
    view_403 = contiguous_106.view(getitem_451, getitem_452, getitem_453);  contiguous_106 = getitem_451 = getitem_452 = getitem_453 = None
    up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn1.to_out, "0")(view_403);  view_403 = None
    up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_out_0 = None
    add_177 = up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_out_1 + add_176;  up_blocks_0_attentions_1_transformer_blocks_6_attn1_to_out_1 = add_176 = None
    up_blocks_0_attentions_1_transformer_blocks_6_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").norm2(add_177)
    up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_6_norm2)
    up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_6_norm2)
    up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_6_norm2);  up_blocks_0_attentions_1_transformer_blocks_6_norm2 = None
    size_707 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_q.size()
    getitem_454 = size_707[0]
    getitem_455 = size_707[1]
    getitem_456 = size_707[2];  size_707 = None
    size_708 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_q.size(0)
    size_709 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_q.size(1)
    view_404 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_q.view(size_708, size_709, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_q = size_708 = size_709 = None
    transpose_505 = view_404.transpose(1, 2);  view_404 = None
    size_710 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_k.size(0)
    size_711 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_k.size(1)
    view_405 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_k.view(size_710, size_711, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_k = size_710 = size_711 = None
    transpose_506 = view_405.transpose(1, 2);  view_405 = None
    size_712 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_v.size(0)
    size_713 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_v.size(1)
    view_406 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_v.view(size_712, size_713, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_v = size_712 = size_713 = None
    transpose_507 = view_406.transpose(1, 2);  view_406 = None
    transpose_508 = transpose_506.transpose(-2, -1);  transpose_506 = None
    matmul_202 = torch.matmul(transpose_505, transpose_508);  transpose_505 = transpose_508 = None
    mul_162 = matmul_202 * 0.125;  matmul_202 = None
    softmax_101 = torch.softmax(mul_162, dim = -1);  mul_162 = None
    matmul_203 = torch.matmul(softmax_101, transpose_507);  softmax_101 = transpose_507 = None
    transpose_509 = matmul_203.transpose(1, 2);  matmul_203 = None
    contiguous_107 = transpose_509.contiguous();  transpose_509 = None
    view_407 = contiguous_107.view(getitem_454, getitem_455, getitem_456);  contiguous_107 = getitem_454 = getitem_455 = getitem_456 = None
    up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn2.to_out, "0")(view_407);  view_407 = None
    up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_out_0 = None
    add_178 = up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_out_1 + add_177;  up_blocks_0_attentions_1_transformer_blocks_6_attn2_to_out_1 = add_177 = None
    up_blocks_0_attentions_1_transformer_blocks_6_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").norm3(add_178)
    up_blocks_0_attentions_1_transformer_blocks_6_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_6_norm3);  up_blocks_0_attentions_1_transformer_blocks_6_norm3 = None
    chunk_50 = up_blocks_0_attentions_1_transformer_blocks_6_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_6_ff_net_0_proj = None
    getitem_457 = chunk_50[0]
    getitem_458 = chunk_50[1];  chunk_50 = None
    gelu_50 = torch._C._nn.gelu(getitem_458);  getitem_458 = None
    mul_163 = getitem_457 * gelu_50;  getitem_457 = gelu_50 = None
    up_blocks_0_attentions_1_transformer_blocks_6_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").ff.net, "1")(mul_163);  mul_163 = None
    up_blocks_0_attentions_1_transformer_blocks_6_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "6").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_6_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_6_ff_net_1 = None
    add_179 = up_blocks_0_attentions_1_transformer_blocks_6_ff_net_2 + add_178;  up_blocks_0_attentions_1_transformer_blocks_6_ff_net_2 = add_178 = None
    up_blocks_0_attentions_1_transformer_blocks_7_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").norm1(add_179)
    up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_7_norm1)
    up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_7_norm1)
    up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_7_norm1);  up_blocks_0_attentions_1_transformer_blocks_7_norm1 = None
    size_714 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_q.size()
    getitem_459 = size_714[0]
    getitem_460 = size_714[1]
    getitem_461 = size_714[2];  size_714 = None
    size_715 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_q.size(0)
    size_716 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_q.size(1)
    view_408 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_q.view(size_715, size_716, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_q = size_715 = size_716 = None
    transpose_510 = view_408.transpose(1, 2);  view_408 = None
    size_717 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_k.size(0)
    size_718 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_k.size(1)
    view_409 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_k.view(size_717, size_718, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_k = size_717 = size_718 = None
    transpose_511 = view_409.transpose(1, 2);  view_409 = None
    size_719 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_v.size(0)
    size_720 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_v.size(1)
    view_410 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_v.view(size_719, size_720, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_v = size_719 = size_720 = None
    transpose_512 = view_410.transpose(1, 2);  view_410 = None
    transpose_513 = transpose_511.transpose(-2, -1);  transpose_511 = None
    matmul_204 = torch.matmul(transpose_510, transpose_513);  transpose_510 = transpose_513 = None
    mul_164 = matmul_204 * 0.125;  matmul_204 = None
    softmax_102 = torch.softmax(mul_164, dim = -1);  mul_164 = None
    matmul_205 = torch.matmul(softmax_102, transpose_512);  softmax_102 = transpose_512 = None
    transpose_514 = matmul_205.transpose(1, 2);  matmul_205 = None
    contiguous_108 = transpose_514.contiguous();  transpose_514 = None
    view_411 = contiguous_108.view(getitem_459, getitem_460, getitem_461);  contiguous_108 = getitem_459 = getitem_460 = getitem_461 = None
    up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn1.to_out, "0")(view_411);  view_411 = None
    up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_out_0 = None
    add_180 = up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_out_1 + add_179;  up_blocks_0_attentions_1_transformer_blocks_7_attn1_to_out_1 = add_179 = None
    up_blocks_0_attentions_1_transformer_blocks_7_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").norm2(add_180)
    up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_7_norm2)
    up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_7_norm2)
    up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_7_norm2);  up_blocks_0_attentions_1_transformer_blocks_7_norm2 = None
    size_721 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_q.size()
    getitem_462 = size_721[0]
    getitem_463 = size_721[1]
    getitem_464 = size_721[2];  size_721 = None
    size_722 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_q.size(0)
    size_723 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_q.size(1)
    view_412 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_q.view(size_722, size_723, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_q = size_722 = size_723 = None
    transpose_515 = view_412.transpose(1, 2);  view_412 = None
    size_724 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_k.size(0)
    size_725 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_k.size(1)
    view_413 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_k.view(size_724, size_725, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_k = size_724 = size_725 = None
    transpose_516 = view_413.transpose(1, 2);  view_413 = None
    size_726 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_v.size(0)
    size_727 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_v.size(1)
    view_414 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_v.view(size_726, size_727, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_v = size_726 = size_727 = None
    transpose_517 = view_414.transpose(1, 2);  view_414 = None
    transpose_518 = transpose_516.transpose(-2, -1);  transpose_516 = None
    matmul_206 = torch.matmul(transpose_515, transpose_518);  transpose_515 = transpose_518 = None
    mul_165 = matmul_206 * 0.125;  matmul_206 = None
    softmax_103 = torch.softmax(mul_165, dim = -1);  mul_165 = None
    matmul_207 = torch.matmul(softmax_103, transpose_517);  softmax_103 = transpose_517 = None
    transpose_519 = matmul_207.transpose(1, 2);  matmul_207 = None
    contiguous_109 = transpose_519.contiguous();  transpose_519 = None
    view_415 = contiguous_109.view(getitem_462, getitem_463, getitem_464);  contiguous_109 = getitem_462 = getitem_463 = getitem_464 = None
    up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn2.to_out, "0")(view_415);  view_415 = None
    up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_out_0 = None
    add_181 = up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_out_1 + add_180;  up_blocks_0_attentions_1_transformer_blocks_7_attn2_to_out_1 = add_180 = None
    up_blocks_0_attentions_1_transformer_blocks_7_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").norm3(add_181)
    up_blocks_0_attentions_1_transformer_blocks_7_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_7_norm3);  up_blocks_0_attentions_1_transformer_blocks_7_norm3 = None
    chunk_51 = up_blocks_0_attentions_1_transformer_blocks_7_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_7_ff_net_0_proj = None
    getitem_465 = chunk_51[0]
    getitem_466 = chunk_51[1];  chunk_51 = None
    gelu_51 = torch._C._nn.gelu(getitem_466);  getitem_466 = None
    mul_166 = getitem_465 * gelu_51;  getitem_465 = gelu_51 = None
    up_blocks_0_attentions_1_transformer_blocks_7_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").ff.net, "1")(mul_166);  mul_166 = None
    up_blocks_0_attentions_1_transformer_blocks_7_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "7").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_7_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_7_ff_net_1 = None
    add_182 = up_blocks_0_attentions_1_transformer_blocks_7_ff_net_2 + add_181;  up_blocks_0_attentions_1_transformer_blocks_7_ff_net_2 = add_181 = None
    up_blocks_0_attentions_1_transformer_blocks_8_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").norm1(add_182)
    up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_8_norm1)
    up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_8_norm1)
    up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_8_norm1);  up_blocks_0_attentions_1_transformer_blocks_8_norm1 = None
    size_728 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_q.size()
    getitem_467 = size_728[0]
    getitem_468 = size_728[1]
    getitem_469 = size_728[2];  size_728 = None
    size_729 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_q.size(0)
    size_730 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_q.size(1)
    view_416 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_q.view(size_729, size_730, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_q = size_729 = size_730 = None
    transpose_520 = view_416.transpose(1, 2);  view_416 = None
    size_731 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_k.size(0)
    size_732 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_k.size(1)
    view_417 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_k.view(size_731, size_732, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_k = size_731 = size_732 = None
    transpose_521 = view_417.transpose(1, 2);  view_417 = None
    size_733 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_v.size(0)
    size_734 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_v.size(1)
    view_418 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_v.view(size_733, size_734, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_v = size_733 = size_734 = None
    transpose_522 = view_418.transpose(1, 2);  view_418 = None
    transpose_523 = transpose_521.transpose(-2, -1);  transpose_521 = None
    matmul_208 = torch.matmul(transpose_520, transpose_523);  transpose_520 = transpose_523 = None
    mul_167 = matmul_208 * 0.125;  matmul_208 = None
    softmax_104 = torch.softmax(mul_167, dim = -1);  mul_167 = None
    matmul_209 = torch.matmul(softmax_104, transpose_522);  softmax_104 = transpose_522 = None
    transpose_524 = matmul_209.transpose(1, 2);  matmul_209 = None
    contiguous_110 = transpose_524.contiguous();  transpose_524 = None
    view_419 = contiguous_110.view(getitem_467, getitem_468, getitem_469);  contiguous_110 = getitem_467 = getitem_468 = getitem_469 = None
    up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn1.to_out, "0")(view_419);  view_419 = None
    up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_out_0 = None
    add_183 = up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_out_1 + add_182;  up_blocks_0_attentions_1_transformer_blocks_8_attn1_to_out_1 = add_182 = None
    up_blocks_0_attentions_1_transformer_blocks_8_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").norm2(add_183)
    up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_8_norm2)
    up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_8_norm2)
    up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_8_norm2);  up_blocks_0_attentions_1_transformer_blocks_8_norm2 = None
    size_735 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_q.size()
    getitem_470 = size_735[0]
    getitem_471 = size_735[1]
    getitem_472 = size_735[2];  size_735 = None
    size_736 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_q.size(0)
    size_737 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_q.size(1)
    view_420 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_q.view(size_736, size_737, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_q = size_736 = size_737 = None
    transpose_525 = view_420.transpose(1, 2);  view_420 = None
    size_738 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_k.size(0)
    size_739 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_k.size(1)
    view_421 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_k.view(size_738, size_739, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_k = size_738 = size_739 = None
    transpose_526 = view_421.transpose(1, 2);  view_421 = None
    size_740 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_v.size(0)
    size_741 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_v.size(1)
    view_422 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_v.view(size_740, size_741, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_v = size_740 = size_741 = None
    transpose_527 = view_422.transpose(1, 2);  view_422 = None
    transpose_528 = transpose_526.transpose(-2, -1);  transpose_526 = None
    matmul_210 = torch.matmul(transpose_525, transpose_528);  transpose_525 = transpose_528 = None
    mul_168 = matmul_210 * 0.125;  matmul_210 = None
    softmax_105 = torch.softmax(mul_168, dim = -1);  mul_168 = None
    matmul_211 = torch.matmul(softmax_105, transpose_527);  softmax_105 = transpose_527 = None
    transpose_529 = matmul_211.transpose(1, 2);  matmul_211 = None
    contiguous_111 = transpose_529.contiguous();  transpose_529 = None
    view_423 = contiguous_111.view(getitem_470, getitem_471, getitem_472);  contiguous_111 = getitem_470 = getitem_471 = getitem_472 = None
    up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn2.to_out, "0")(view_423);  view_423 = None
    up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_out_0 = None
    add_184 = up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_out_1 + add_183;  up_blocks_0_attentions_1_transformer_blocks_8_attn2_to_out_1 = add_183 = None
    up_blocks_0_attentions_1_transformer_blocks_8_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").norm3(add_184)
    up_blocks_0_attentions_1_transformer_blocks_8_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_8_norm3);  up_blocks_0_attentions_1_transformer_blocks_8_norm3 = None
    chunk_52 = up_blocks_0_attentions_1_transformer_blocks_8_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_8_ff_net_0_proj = None
    getitem_473 = chunk_52[0]
    getitem_474 = chunk_52[1];  chunk_52 = None
    gelu_52 = torch._C._nn.gelu(getitem_474);  getitem_474 = None
    mul_169 = getitem_473 * gelu_52;  getitem_473 = gelu_52 = None
    up_blocks_0_attentions_1_transformer_blocks_8_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").ff.net, "1")(mul_169);  mul_169 = None
    up_blocks_0_attentions_1_transformer_blocks_8_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "8").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_8_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_8_ff_net_1 = None
    add_185 = up_blocks_0_attentions_1_transformer_blocks_8_ff_net_2 + add_184;  up_blocks_0_attentions_1_transformer_blocks_8_ff_net_2 = add_184 = None
    up_blocks_0_attentions_1_transformer_blocks_9_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").norm1(add_185)
    up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn1.to_q(up_blocks_0_attentions_1_transformer_blocks_9_norm1)
    up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn1.to_k(up_blocks_0_attentions_1_transformer_blocks_9_norm1)
    up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn1.to_v(up_blocks_0_attentions_1_transformer_blocks_9_norm1);  up_blocks_0_attentions_1_transformer_blocks_9_norm1 = None
    size_742 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_q.size()
    getitem_475 = size_742[0]
    getitem_476 = size_742[1]
    getitem_477 = size_742[2];  size_742 = None
    size_743 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_q.size(0)
    size_744 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_q.size(1)
    view_424 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_q.view(size_743, size_744, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_q = size_743 = size_744 = None
    transpose_530 = view_424.transpose(1, 2);  view_424 = None
    size_745 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_k.size(0)
    size_746 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_k.size(1)
    view_425 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_k.view(size_745, size_746, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_k = size_745 = size_746 = None
    transpose_531 = view_425.transpose(1, 2);  view_425 = None
    size_747 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_v.size(0)
    size_748 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_v.size(1)
    view_426 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_v.view(size_747, size_748, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_v = size_747 = size_748 = None
    transpose_532 = view_426.transpose(1, 2);  view_426 = None
    transpose_533 = transpose_531.transpose(-2, -1);  transpose_531 = None
    matmul_212 = torch.matmul(transpose_530, transpose_533);  transpose_530 = transpose_533 = None
    mul_170 = matmul_212 * 0.125;  matmul_212 = None
    softmax_106 = torch.softmax(mul_170, dim = -1);  mul_170 = None
    matmul_213 = torch.matmul(softmax_106, transpose_532);  softmax_106 = transpose_532 = None
    transpose_534 = matmul_213.transpose(1, 2);  matmul_213 = None
    contiguous_112 = transpose_534.contiguous();  transpose_534 = None
    view_427 = contiguous_112.view(getitem_475, getitem_476, getitem_477);  contiguous_112 = getitem_475 = getitem_476 = getitem_477 = None
    up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn1.to_out, "0")(view_427);  view_427 = None
    up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn1.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_out_0 = None
    add_186 = up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_out_1 + add_185;  up_blocks_0_attentions_1_transformer_blocks_9_attn1_to_out_1 = add_185 = None
    up_blocks_0_attentions_1_transformer_blocks_9_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").norm2(add_186)
    up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn2.to_q(up_blocks_0_attentions_1_transformer_blocks_9_norm2)
    up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn2.to_k(up_blocks_0_attentions_1_transformer_blocks_9_norm2)
    up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn2.to_v(up_blocks_0_attentions_1_transformer_blocks_9_norm2);  up_blocks_0_attentions_1_transformer_blocks_9_norm2 = None
    size_749 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_q.size()
    getitem_478 = size_749[0]
    getitem_479 = size_749[1]
    getitem_480 = size_749[2];  size_749 = None
    size_750 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_q.size(0)
    size_751 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_q.size(1)
    view_428 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_q.view(size_750, size_751, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_q = size_750 = size_751 = None
    transpose_535 = view_428.transpose(1, 2);  view_428 = None
    size_752 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_k.size(0)
    size_753 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_k.size(1)
    view_429 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_k.view(size_752, size_753, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_k = size_752 = size_753 = None
    transpose_536 = view_429.transpose(1, 2);  view_429 = None
    size_754 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_v.size(0)
    size_755 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_v.size(1)
    view_430 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_v.view(size_754, size_755, 20, 64);  up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_v = size_754 = size_755 = None
    transpose_537 = view_430.transpose(1, 2);  view_430 = None
    transpose_538 = transpose_536.transpose(-2, -1);  transpose_536 = None
    matmul_214 = torch.matmul(transpose_535, transpose_538);  transpose_535 = transpose_538 = None
    mul_171 = matmul_214 * 0.125;  matmul_214 = None
    softmax_107 = torch.softmax(mul_171, dim = -1);  mul_171 = None
    matmul_215 = torch.matmul(softmax_107, transpose_537);  softmax_107 = transpose_537 = None
    transpose_539 = matmul_215.transpose(1, 2);  matmul_215 = None
    contiguous_113 = transpose_539.contiguous();  transpose_539 = None
    view_431 = contiguous_113.view(getitem_478, getitem_479, getitem_480);  contiguous_113 = getitem_478 = getitem_479 = getitem_480 = None
    up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn2.to_out, "0")(view_431);  view_431 = None
    up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").attn2.to_out, "1")(up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_out_0);  up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_out_0 = None
    add_187 = up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_out_1 + add_186;  up_blocks_0_attentions_1_transformer_blocks_9_attn2_to_out_1 = add_186 = None
    up_blocks_0_attentions_1_transformer_blocks_9_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").norm3(add_187)
    up_blocks_0_attentions_1_transformer_blocks_9_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").ff.net, "0").proj(up_blocks_0_attentions_1_transformer_blocks_9_norm3);  up_blocks_0_attentions_1_transformer_blocks_9_norm3 = None
    chunk_53 = up_blocks_0_attentions_1_transformer_blocks_9_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_1_transformer_blocks_9_ff_net_0_proj = None
    getitem_481 = chunk_53[0]
    getitem_482 = chunk_53[1];  chunk_53 = None
    gelu_53 = torch._C._nn.gelu(getitem_482);  getitem_482 = None
    mul_172 = getitem_481 * gelu_53;  getitem_481 = gelu_53 = None
    up_blocks_0_attentions_1_transformer_blocks_9_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").ff.net, "1")(mul_172);  mul_172 = None
    up_blocks_0_attentions_1_transformer_blocks_9_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "1").transformer_blocks, "9").ff.net, "2")(up_blocks_0_attentions_1_transformer_blocks_9_ff_net_1);  up_blocks_0_attentions_1_transformer_blocks_9_ff_net_1 = None
    add_188 = up_blocks_0_attentions_1_transformer_blocks_9_ff_net_2 + add_187;  up_blocks_0_attentions_1_transformer_blocks_9_ff_net_2 = add_187 = None
    up_blocks_0_attentions_1_proj_out = getattr(getattr(self.up_blocks, "0").attentions, "1").proj_out(add_188);  add_188 = None
    reshape_14 = up_blocks_0_attentions_1_proj_out.reshape(getitem_398, getitem_400, getitem_401, getitem_402);  up_blocks_0_attentions_1_proj_out = getitem_398 = getitem_400 = getitem_401 = getitem_402 = None
    permute_13 = reshape_14.permute(0, 3, 1, 2);  reshape_14 = None
    contiguous_114 = permute_13.contiguous();  permute_13 = None
    add_189 = contiguous_114 + add_158;  contiguous_114 = add_158 = None
    cat_4 = torch.cat([add_189, down_blocks_1_downsamplers_0_conv], dim = 1);  add_189 = down_blocks_1_downsamplers_0_conv = None
    up_blocks_0_resnets_2_norm1 = getattr(getattr(self.up_blocks, "0").resnets, "2").norm1(cat_4)
    up_blocks_0_resnets_2_nonlinearity = getattr(getattr(self.up_blocks, "0").resnets, "2").nonlinearity(up_blocks_0_resnets_2_norm1);  up_blocks_0_resnets_2_norm1 = None
    up_blocks_0_resnets_2_conv1 = getattr(getattr(self.up_blocks, "0").resnets, "2").conv1(up_blocks_0_resnets_2_nonlinearity);  up_blocks_0_resnets_2_nonlinearity = None
    up_blocks_0_resnets_2_nonlinearity_1 = getattr(getattr(self.up_blocks, "0").resnets, "2").nonlinearity(add)
    up_blocks_0_resnets_2_time_emb_proj = getattr(getattr(self.up_blocks, "0").resnets, "2").time_emb_proj(up_blocks_0_resnets_2_nonlinearity_1);  up_blocks_0_resnets_2_nonlinearity_1 = None
    getitem_483 = up_blocks_0_resnets_2_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_0_resnets_2_time_emb_proj = None
    add_190 = up_blocks_0_resnets_2_conv1 + getitem_483;  up_blocks_0_resnets_2_conv1 = getitem_483 = None
    up_blocks_0_resnets_2_norm2 = getattr(getattr(self.up_blocks, "0").resnets, "2").norm2(add_190);  add_190 = None
    up_blocks_0_resnets_2_nonlinearity_2 = getattr(getattr(self.up_blocks, "0").resnets, "2").nonlinearity(up_blocks_0_resnets_2_norm2);  up_blocks_0_resnets_2_norm2 = None
    up_blocks_0_resnets_2_dropout = getattr(getattr(self.up_blocks, "0").resnets, "2").dropout(up_blocks_0_resnets_2_nonlinearity_2);  up_blocks_0_resnets_2_nonlinearity_2 = None
    up_blocks_0_resnets_2_conv2 = getattr(getattr(self.up_blocks, "0").resnets, "2").conv2(up_blocks_0_resnets_2_dropout);  up_blocks_0_resnets_2_dropout = None
    up_blocks_0_resnets_2_conv_shortcut = getattr(getattr(self.up_blocks, "0").resnets, "2").conv_shortcut(cat_4);  cat_4 = None
    add_191 = up_blocks_0_resnets_2_conv_shortcut + up_blocks_0_resnets_2_conv2;  up_blocks_0_resnets_2_conv_shortcut = up_blocks_0_resnets_2_conv2 = None
    getattr_23 = add_191.shape
    getitem_484 = getattr_23[0]
    getitem_485 = getattr_23[1]
    getitem_486 = getattr_23[2]
    getitem_487 = getattr_23[3];  getattr_23 = None
    up_blocks_0_attentions_2_norm = getattr(getattr(self.up_blocks, "0").attentions, "2").norm(add_191)
    getattr_24 = up_blocks_0_attentions_2_norm.shape
    getitem_488 = getattr_24[1];  getattr_24 = None
    permute_14 = up_blocks_0_attentions_2_norm.permute(0, 2, 3, 1);  up_blocks_0_attentions_2_norm = None
    mul_173 = getitem_486 * getitem_487
    reshape_15 = permute_14.reshape(getitem_484, mul_173, getitem_488);  permute_14 = mul_173 = None
    up_blocks_0_attentions_2_proj_in = getattr(getattr(self.up_blocks, "0").attentions, "2").proj_in(reshape_15);  reshape_15 = None
    up_blocks_0_attentions_2_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").norm1(up_blocks_0_attentions_2_proj_in)
    up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_0_norm1)
    up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_0_norm1)
    up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_0_norm1);  up_blocks_0_attentions_2_transformer_blocks_0_norm1 = None
    size_756 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_q.size()
    getitem_489 = size_756[0]
    getitem_490 = size_756[1]
    getitem_491 = size_756[2];  size_756 = None
    size_757 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_q.size(0)
    size_758 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_q.size(1)
    view_432 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_q.view(size_757, size_758, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_q = size_757 = size_758 = None
    transpose_540 = view_432.transpose(1, 2);  view_432 = None
    size_759 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_k.size(0)
    size_760 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_k.size(1)
    view_433 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_k.view(size_759, size_760, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_k = size_759 = size_760 = None
    transpose_541 = view_433.transpose(1, 2);  view_433 = None
    size_761 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_v.size(0)
    size_762 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_v.size(1)
    view_434 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_v.view(size_761, size_762, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_v = size_761 = size_762 = None
    transpose_542 = view_434.transpose(1, 2);  view_434 = None
    transpose_543 = transpose_541.transpose(-2, -1);  transpose_541 = None
    matmul_216 = torch.matmul(transpose_540, transpose_543);  transpose_540 = transpose_543 = None
    mul_174 = matmul_216 * 0.125;  matmul_216 = None
    softmax_108 = torch.softmax(mul_174, dim = -1);  mul_174 = None
    matmul_217 = torch.matmul(softmax_108, transpose_542);  softmax_108 = transpose_542 = None
    transpose_544 = matmul_217.transpose(1, 2);  matmul_217 = None
    contiguous_115 = transpose_544.contiguous();  transpose_544 = None
    view_435 = contiguous_115.view(getitem_489, getitem_490, getitem_491);  contiguous_115 = getitem_489 = getitem_490 = getitem_491 = None
    up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn1.to_out, "0")(view_435);  view_435 = None
    up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_out_0 = None
    add_192 = up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_out_1 + up_blocks_0_attentions_2_proj_in;  up_blocks_0_attentions_2_transformer_blocks_0_attn1_to_out_1 = up_blocks_0_attentions_2_proj_in = None
    up_blocks_0_attentions_2_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").norm2(add_192)
    up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_0_norm2)
    up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_0_norm2)
    up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_0_norm2);  up_blocks_0_attentions_2_transformer_blocks_0_norm2 = None
    size_763 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_q.size()
    getitem_492 = size_763[0]
    getitem_493 = size_763[1]
    getitem_494 = size_763[2];  size_763 = None
    size_764 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_q.size(0)
    size_765 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_q.size(1)
    view_436 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_q.view(size_764, size_765, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_q = size_764 = size_765 = None
    transpose_545 = view_436.transpose(1, 2);  view_436 = None
    size_766 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_k.size(0)
    size_767 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_k.size(1)
    view_437 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_k.view(size_766, size_767, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_k = size_766 = size_767 = None
    transpose_546 = view_437.transpose(1, 2);  view_437 = None
    size_768 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_v.size(0)
    size_769 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_v.size(1)
    view_438 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_v.view(size_768, size_769, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_v = size_768 = size_769 = None
    transpose_547 = view_438.transpose(1, 2);  view_438 = None
    transpose_548 = transpose_546.transpose(-2, -1);  transpose_546 = None
    matmul_218 = torch.matmul(transpose_545, transpose_548);  transpose_545 = transpose_548 = None
    mul_175 = matmul_218 * 0.125;  matmul_218 = None
    softmax_109 = torch.softmax(mul_175, dim = -1);  mul_175 = None
    matmul_219 = torch.matmul(softmax_109, transpose_547);  softmax_109 = transpose_547 = None
    transpose_549 = matmul_219.transpose(1, 2);  matmul_219 = None
    contiguous_116 = transpose_549.contiguous();  transpose_549 = None
    view_439 = contiguous_116.view(getitem_492, getitem_493, getitem_494);  contiguous_116 = getitem_492 = getitem_493 = getitem_494 = None
    up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn2.to_out, "0")(view_439);  view_439 = None
    up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_out_0 = None
    add_193 = up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_out_1 + add_192;  up_blocks_0_attentions_2_transformer_blocks_0_attn2_to_out_1 = add_192 = None
    up_blocks_0_attentions_2_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").norm3(add_193)
    up_blocks_0_attentions_2_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_0_norm3);  up_blocks_0_attentions_2_transformer_blocks_0_norm3 = None
    chunk_54 = up_blocks_0_attentions_2_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_0_ff_net_0_proj = None
    getitem_495 = chunk_54[0]
    getitem_496 = chunk_54[1];  chunk_54 = None
    gelu_54 = torch._C._nn.gelu(getitem_496);  getitem_496 = None
    mul_176 = getitem_495 * gelu_54;  getitem_495 = gelu_54 = None
    up_blocks_0_attentions_2_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").ff.net, "1")(mul_176);  mul_176 = None
    up_blocks_0_attentions_2_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "0").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_0_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_0_ff_net_1 = None
    add_194 = up_blocks_0_attentions_2_transformer_blocks_0_ff_net_2 + add_193;  up_blocks_0_attentions_2_transformer_blocks_0_ff_net_2 = add_193 = None
    up_blocks_0_attentions_2_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").norm1(add_194)
    up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_1_norm1)
    up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_1_norm1)
    up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_1_norm1);  up_blocks_0_attentions_2_transformer_blocks_1_norm1 = None
    size_770 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_q.size()
    getitem_497 = size_770[0]
    getitem_498 = size_770[1]
    getitem_499 = size_770[2];  size_770 = None
    size_771 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_q.size(0)
    size_772 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_q.size(1)
    view_440 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_q.view(size_771, size_772, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_q = size_771 = size_772 = None
    transpose_550 = view_440.transpose(1, 2);  view_440 = None
    size_773 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_k.size(0)
    size_774 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_k.size(1)
    view_441 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_k.view(size_773, size_774, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_k = size_773 = size_774 = None
    transpose_551 = view_441.transpose(1, 2);  view_441 = None
    size_775 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_v.size(0)
    size_776 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_v.size(1)
    view_442 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_v.view(size_775, size_776, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_v = size_775 = size_776 = None
    transpose_552 = view_442.transpose(1, 2);  view_442 = None
    transpose_553 = transpose_551.transpose(-2, -1);  transpose_551 = None
    matmul_220 = torch.matmul(transpose_550, transpose_553);  transpose_550 = transpose_553 = None
    mul_177 = matmul_220 * 0.125;  matmul_220 = None
    softmax_110 = torch.softmax(mul_177, dim = -1);  mul_177 = None
    matmul_221 = torch.matmul(softmax_110, transpose_552);  softmax_110 = transpose_552 = None
    transpose_554 = matmul_221.transpose(1, 2);  matmul_221 = None
    contiguous_117 = transpose_554.contiguous();  transpose_554 = None
    view_443 = contiguous_117.view(getitem_497, getitem_498, getitem_499);  contiguous_117 = getitem_497 = getitem_498 = getitem_499 = None
    up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn1.to_out, "0")(view_443);  view_443 = None
    up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_out_0 = None
    add_195 = up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_out_1 + add_194;  up_blocks_0_attentions_2_transformer_blocks_1_attn1_to_out_1 = add_194 = None
    up_blocks_0_attentions_2_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").norm2(add_195)
    up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_1_norm2)
    up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_1_norm2)
    up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_1_norm2);  up_blocks_0_attentions_2_transformer_blocks_1_norm2 = None
    size_777 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_q.size()
    getitem_500 = size_777[0]
    getitem_501 = size_777[1]
    getitem_502 = size_777[2];  size_777 = None
    size_778 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_q.size(0)
    size_779 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_q.size(1)
    view_444 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_q.view(size_778, size_779, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_q = size_778 = size_779 = None
    transpose_555 = view_444.transpose(1, 2);  view_444 = None
    size_780 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_k.size(0)
    size_781 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_k.size(1)
    view_445 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_k.view(size_780, size_781, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_k = size_780 = size_781 = None
    transpose_556 = view_445.transpose(1, 2);  view_445 = None
    size_782 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_v.size(0)
    size_783 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_v.size(1)
    view_446 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_v.view(size_782, size_783, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_v = size_782 = size_783 = None
    transpose_557 = view_446.transpose(1, 2);  view_446 = None
    transpose_558 = transpose_556.transpose(-2, -1);  transpose_556 = None
    matmul_222 = torch.matmul(transpose_555, transpose_558);  transpose_555 = transpose_558 = None
    mul_178 = matmul_222 * 0.125;  matmul_222 = None
    softmax_111 = torch.softmax(mul_178, dim = -1);  mul_178 = None
    matmul_223 = torch.matmul(softmax_111, transpose_557);  softmax_111 = transpose_557 = None
    transpose_559 = matmul_223.transpose(1, 2);  matmul_223 = None
    contiguous_118 = transpose_559.contiguous();  transpose_559 = None
    view_447 = contiguous_118.view(getitem_500, getitem_501, getitem_502);  contiguous_118 = getitem_500 = getitem_501 = getitem_502 = None
    up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn2.to_out, "0")(view_447);  view_447 = None
    up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_out_0 = None
    add_196 = up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_out_1 + add_195;  up_blocks_0_attentions_2_transformer_blocks_1_attn2_to_out_1 = add_195 = None
    up_blocks_0_attentions_2_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").norm3(add_196)
    up_blocks_0_attentions_2_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_1_norm3);  up_blocks_0_attentions_2_transformer_blocks_1_norm3 = None
    chunk_55 = up_blocks_0_attentions_2_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_1_ff_net_0_proj = None
    getitem_503 = chunk_55[0]
    getitem_504 = chunk_55[1];  chunk_55 = None
    gelu_55 = torch._C._nn.gelu(getitem_504);  getitem_504 = None
    mul_179 = getitem_503 * gelu_55;  getitem_503 = gelu_55 = None
    up_blocks_0_attentions_2_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").ff.net, "1")(mul_179);  mul_179 = None
    up_blocks_0_attentions_2_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "1").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_1_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_1_ff_net_1 = None
    add_197 = up_blocks_0_attentions_2_transformer_blocks_1_ff_net_2 + add_196;  up_blocks_0_attentions_2_transformer_blocks_1_ff_net_2 = add_196 = None
    up_blocks_0_attentions_2_transformer_blocks_2_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").norm1(add_197)
    up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_2_norm1)
    up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_2_norm1)
    up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_2_norm1);  up_blocks_0_attentions_2_transformer_blocks_2_norm1 = None
    size_784 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_q.size()
    getitem_505 = size_784[0]
    getitem_506 = size_784[1]
    getitem_507 = size_784[2];  size_784 = None
    size_785 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_q.size(0)
    size_786 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_q.size(1)
    view_448 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_q.view(size_785, size_786, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_q = size_785 = size_786 = None
    transpose_560 = view_448.transpose(1, 2);  view_448 = None
    size_787 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_k.size(0)
    size_788 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_k.size(1)
    view_449 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_k.view(size_787, size_788, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_k = size_787 = size_788 = None
    transpose_561 = view_449.transpose(1, 2);  view_449 = None
    size_789 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_v.size(0)
    size_790 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_v.size(1)
    view_450 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_v.view(size_789, size_790, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_v = size_789 = size_790 = None
    transpose_562 = view_450.transpose(1, 2);  view_450 = None
    transpose_563 = transpose_561.transpose(-2, -1);  transpose_561 = None
    matmul_224 = torch.matmul(transpose_560, transpose_563);  transpose_560 = transpose_563 = None
    mul_180 = matmul_224 * 0.125;  matmul_224 = None
    softmax_112 = torch.softmax(mul_180, dim = -1);  mul_180 = None
    matmul_225 = torch.matmul(softmax_112, transpose_562);  softmax_112 = transpose_562 = None
    transpose_564 = matmul_225.transpose(1, 2);  matmul_225 = None
    contiguous_119 = transpose_564.contiguous();  transpose_564 = None
    view_451 = contiguous_119.view(getitem_505, getitem_506, getitem_507);  contiguous_119 = getitem_505 = getitem_506 = getitem_507 = None
    up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn1.to_out, "0")(view_451);  view_451 = None
    up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_out_0 = None
    add_198 = up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_out_1 + add_197;  up_blocks_0_attentions_2_transformer_blocks_2_attn1_to_out_1 = add_197 = None
    up_blocks_0_attentions_2_transformer_blocks_2_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").norm2(add_198)
    up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_2_norm2)
    up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_2_norm2)
    up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_2_norm2);  up_blocks_0_attentions_2_transformer_blocks_2_norm2 = None
    size_791 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_q.size()
    getitem_508 = size_791[0]
    getitem_509 = size_791[1]
    getitem_510 = size_791[2];  size_791 = None
    size_792 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_q.size(0)
    size_793 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_q.size(1)
    view_452 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_q.view(size_792, size_793, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_q = size_792 = size_793 = None
    transpose_565 = view_452.transpose(1, 2);  view_452 = None
    size_794 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_k.size(0)
    size_795 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_k.size(1)
    view_453 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_k.view(size_794, size_795, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_k = size_794 = size_795 = None
    transpose_566 = view_453.transpose(1, 2);  view_453 = None
    size_796 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_v.size(0)
    size_797 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_v.size(1)
    view_454 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_v.view(size_796, size_797, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_v = size_796 = size_797 = None
    transpose_567 = view_454.transpose(1, 2);  view_454 = None
    transpose_568 = transpose_566.transpose(-2, -1);  transpose_566 = None
    matmul_226 = torch.matmul(transpose_565, transpose_568);  transpose_565 = transpose_568 = None
    mul_181 = matmul_226 * 0.125;  matmul_226 = None
    softmax_113 = torch.softmax(mul_181, dim = -1);  mul_181 = None
    matmul_227 = torch.matmul(softmax_113, transpose_567);  softmax_113 = transpose_567 = None
    transpose_569 = matmul_227.transpose(1, 2);  matmul_227 = None
    contiguous_120 = transpose_569.contiguous();  transpose_569 = None
    view_455 = contiguous_120.view(getitem_508, getitem_509, getitem_510);  contiguous_120 = getitem_508 = getitem_509 = getitem_510 = None
    up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn2.to_out, "0")(view_455);  view_455 = None
    up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_out_0 = None
    add_199 = up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_out_1 + add_198;  up_blocks_0_attentions_2_transformer_blocks_2_attn2_to_out_1 = add_198 = None
    up_blocks_0_attentions_2_transformer_blocks_2_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").norm3(add_199)
    up_blocks_0_attentions_2_transformer_blocks_2_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_2_norm3);  up_blocks_0_attentions_2_transformer_blocks_2_norm3 = None
    chunk_56 = up_blocks_0_attentions_2_transformer_blocks_2_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_2_ff_net_0_proj = None
    getitem_511 = chunk_56[0]
    getitem_512 = chunk_56[1];  chunk_56 = None
    gelu_56 = torch._C._nn.gelu(getitem_512);  getitem_512 = None
    mul_182 = getitem_511 * gelu_56;  getitem_511 = gelu_56 = None
    up_blocks_0_attentions_2_transformer_blocks_2_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").ff.net, "1")(mul_182);  mul_182 = None
    up_blocks_0_attentions_2_transformer_blocks_2_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "2").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_2_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_2_ff_net_1 = None
    add_200 = up_blocks_0_attentions_2_transformer_blocks_2_ff_net_2 + add_199;  up_blocks_0_attentions_2_transformer_blocks_2_ff_net_2 = add_199 = None
    up_blocks_0_attentions_2_transformer_blocks_3_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").norm1(add_200)
    up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_3_norm1)
    up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_3_norm1)
    up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_3_norm1);  up_blocks_0_attentions_2_transformer_blocks_3_norm1 = None
    size_798 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_q.size()
    getitem_513 = size_798[0]
    getitem_514 = size_798[1]
    getitem_515 = size_798[2];  size_798 = None
    size_799 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_q.size(0)
    size_800 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_q.size(1)
    view_456 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_q.view(size_799, size_800, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_q = size_799 = size_800 = None
    transpose_570 = view_456.transpose(1, 2);  view_456 = None
    size_801 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_k.size(0)
    size_802 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_k.size(1)
    view_457 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_k.view(size_801, size_802, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_k = size_801 = size_802 = None
    transpose_571 = view_457.transpose(1, 2);  view_457 = None
    size_803 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_v.size(0)
    size_804 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_v.size(1)
    view_458 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_v.view(size_803, size_804, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_v = size_803 = size_804 = None
    transpose_572 = view_458.transpose(1, 2);  view_458 = None
    transpose_573 = transpose_571.transpose(-2, -1);  transpose_571 = None
    matmul_228 = torch.matmul(transpose_570, transpose_573);  transpose_570 = transpose_573 = None
    mul_183 = matmul_228 * 0.125;  matmul_228 = None
    softmax_114 = torch.softmax(mul_183, dim = -1);  mul_183 = None
    matmul_229 = torch.matmul(softmax_114, transpose_572);  softmax_114 = transpose_572 = None
    transpose_574 = matmul_229.transpose(1, 2);  matmul_229 = None
    contiguous_121 = transpose_574.contiguous();  transpose_574 = None
    view_459 = contiguous_121.view(getitem_513, getitem_514, getitem_515);  contiguous_121 = getitem_513 = getitem_514 = getitem_515 = None
    up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn1.to_out, "0")(view_459);  view_459 = None
    up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_out_0 = None
    add_201 = up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_out_1 + add_200;  up_blocks_0_attentions_2_transformer_blocks_3_attn1_to_out_1 = add_200 = None
    up_blocks_0_attentions_2_transformer_blocks_3_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").norm2(add_201)
    up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_3_norm2)
    up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_3_norm2)
    up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_3_norm2);  up_blocks_0_attentions_2_transformer_blocks_3_norm2 = None
    size_805 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_q.size()
    getitem_516 = size_805[0]
    getitem_517 = size_805[1]
    getitem_518 = size_805[2];  size_805 = None
    size_806 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_q.size(0)
    size_807 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_q.size(1)
    view_460 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_q.view(size_806, size_807, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_q = size_806 = size_807 = None
    transpose_575 = view_460.transpose(1, 2);  view_460 = None
    size_808 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_k.size(0)
    size_809 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_k.size(1)
    view_461 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_k.view(size_808, size_809, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_k = size_808 = size_809 = None
    transpose_576 = view_461.transpose(1, 2);  view_461 = None
    size_810 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_v.size(0)
    size_811 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_v.size(1)
    view_462 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_v.view(size_810, size_811, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_v = size_810 = size_811 = None
    transpose_577 = view_462.transpose(1, 2);  view_462 = None
    transpose_578 = transpose_576.transpose(-2, -1);  transpose_576 = None
    matmul_230 = torch.matmul(transpose_575, transpose_578);  transpose_575 = transpose_578 = None
    mul_184 = matmul_230 * 0.125;  matmul_230 = None
    softmax_115 = torch.softmax(mul_184, dim = -1);  mul_184 = None
    matmul_231 = torch.matmul(softmax_115, transpose_577);  softmax_115 = transpose_577 = None
    transpose_579 = matmul_231.transpose(1, 2);  matmul_231 = None
    contiguous_122 = transpose_579.contiguous();  transpose_579 = None
    view_463 = contiguous_122.view(getitem_516, getitem_517, getitem_518);  contiguous_122 = getitem_516 = getitem_517 = getitem_518 = None
    up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn2.to_out, "0")(view_463);  view_463 = None
    up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_out_0 = None
    add_202 = up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_out_1 + add_201;  up_blocks_0_attentions_2_transformer_blocks_3_attn2_to_out_1 = add_201 = None
    up_blocks_0_attentions_2_transformer_blocks_3_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").norm3(add_202)
    up_blocks_0_attentions_2_transformer_blocks_3_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_3_norm3);  up_blocks_0_attentions_2_transformer_blocks_3_norm3 = None
    chunk_57 = up_blocks_0_attentions_2_transformer_blocks_3_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_3_ff_net_0_proj = None
    getitem_519 = chunk_57[0]
    getitem_520 = chunk_57[1];  chunk_57 = None
    gelu_57 = torch._C._nn.gelu(getitem_520);  getitem_520 = None
    mul_185 = getitem_519 * gelu_57;  getitem_519 = gelu_57 = None
    up_blocks_0_attentions_2_transformer_blocks_3_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").ff.net, "1")(mul_185);  mul_185 = None
    up_blocks_0_attentions_2_transformer_blocks_3_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "3").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_3_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_3_ff_net_1 = None
    add_203 = up_blocks_0_attentions_2_transformer_blocks_3_ff_net_2 + add_202;  up_blocks_0_attentions_2_transformer_blocks_3_ff_net_2 = add_202 = None
    up_blocks_0_attentions_2_transformer_blocks_4_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").norm1(add_203)
    up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_4_norm1)
    up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_4_norm1)
    up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_4_norm1);  up_blocks_0_attentions_2_transformer_blocks_4_norm1 = None
    size_812 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_q.size()
    getitem_521 = size_812[0]
    getitem_522 = size_812[1]
    getitem_523 = size_812[2];  size_812 = None
    size_813 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_q.size(0)
    size_814 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_q.size(1)
    view_464 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_q.view(size_813, size_814, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_q = size_813 = size_814 = None
    transpose_580 = view_464.transpose(1, 2);  view_464 = None
    size_815 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_k.size(0)
    size_816 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_k.size(1)
    view_465 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_k.view(size_815, size_816, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_k = size_815 = size_816 = None
    transpose_581 = view_465.transpose(1, 2);  view_465 = None
    size_817 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_v.size(0)
    size_818 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_v.size(1)
    view_466 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_v.view(size_817, size_818, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_v = size_817 = size_818 = None
    transpose_582 = view_466.transpose(1, 2);  view_466 = None
    transpose_583 = transpose_581.transpose(-2, -1);  transpose_581 = None
    matmul_232 = torch.matmul(transpose_580, transpose_583);  transpose_580 = transpose_583 = None
    mul_186 = matmul_232 * 0.125;  matmul_232 = None
    softmax_116 = torch.softmax(mul_186, dim = -1);  mul_186 = None
    matmul_233 = torch.matmul(softmax_116, transpose_582);  softmax_116 = transpose_582 = None
    transpose_584 = matmul_233.transpose(1, 2);  matmul_233 = None
    contiguous_123 = transpose_584.contiguous();  transpose_584 = None
    view_467 = contiguous_123.view(getitem_521, getitem_522, getitem_523);  contiguous_123 = getitem_521 = getitem_522 = getitem_523 = None
    up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn1.to_out, "0")(view_467);  view_467 = None
    up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_out_0 = None
    add_204 = up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_out_1 + add_203;  up_blocks_0_attentions_2_transformer_blocks_4_attn1_to_out_1 = add_203 = None
    up_blocks_0_attentions_2_transformer_blocks_4_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").norm2(add_204)
    up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_4_norm2)
    up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_4_norm2)
    up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_4_norm2);  up_blocks_0_attentions_2_transformer_blocks_4_norm2 = None
    size_819 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_q.size()
    getitem_524 = size_819[0]
    getitem_525 = size_819[1]
    getitem_526 = size_819[2];  size_819 = None
    size_820 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_q.size(0)
    size_821 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_q.size(1)
    view_468 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_q.view(size_820, size_821, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_q = size_820 = size_821 = None
    transpose_585 = view_468.transpose(1, 2);  view_468 = None
    size_822 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_k.size(0)
    size_823 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_k.size(1)
    view_469 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_k.view(size_822, size_823, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_k = size_822 = size_823 = None
    transpose_586 = view_469.transpose(1, 2);  view_469 = None
    size_824 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_v.size(0)
    size_825 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_v.size(1)
    view_470 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_v.view(size_824, size_825, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_v = size_824 = size_825 = None
    transpose_587 = view_470.transpose(1, 2);  view_470 = None
    transpose_588 = transpose_586.transpose(-2, -1);  transpose_586 = None
    matmul_234 = torch.matmul(transpose_585, transpose_588);  transpose_585 = transpose_588 = None
    mul_187 = matmul_234 * 0.125;  matmul_234 = None
    softmax_117 = torch.softmax(mul_187, dim = -1);  mul_187 = None
    matmul_235 = torch.matmul(softmax_117, transpose_587);  softmax_117 = transpose_587 = None
    transpose_589 = matmul_235.transpose(1, 2);  matmul_235 = None
    contiguous_124 = transpose_589.contiguous();  transpose_589 = None
    view_471 = contiguous_124.view(getitem_524, getitem_525, getitem_526);  contiguous_124 = getitem_524 = getitem_525 = getitem_526 = None
    up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn2.to_out, "0")(view_471);  view_471 = None
    up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_out_0 = None
    add_205 = up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_out_1 + add_204;  up_blocks_0_attentions_2_transformer_blocks_4_attn2_to_out_1 = add_204 = None
    up_blocks_0_attentions_2_transformer_blocks_4_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").norm3(add_205)
    up_blocks_0_attentions_2_transformer_blocks_4_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_4_norm3);  up_blocks_0_attentions_2_transformer_blocks_4_norm3 = None
    chunk_58 = up_blocks_0_attentions_2_transformer_blocks_4_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_4_ff_net_0_proj = None
    getitem_527 = chunk_58[0]
    getitem_528 = chunk_58[1];  chunk_58 = None
    gelu_58 = torch._C._nn.gelu(getitem_528);  getitem_528 = None
    mul_188 = getitem_527 * gelu_58;  getitem_527 = gelu_58 = None
    up_blocks_0_attentions_2_transformer_blocks_4_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").ff.net, "1")(mul_188);  mul_188 = None
    up_blocks_0_attentions_2_transformer_blocks_4_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "4").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_4_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_4_ff_net_1 = None
    add_206 = up_blocks_0_attentions_2_transformer_blocks_4_ff_net_2 + add_205;  up_blocks_0_attentions_2_transformer_blocks_4_ff_net_2 = add_205 = None
    up_blocks_0_attentions_2_transformer_blocks_5_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").norm1(add_206)
    up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_5_norm1)
    up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_5_norm1)
    up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_5_norm1);  up_blocks_0_attentions_2_transformer_blocks_5_norm1 = None
    size_826 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_q.size()
    getitem_529 = size_826[0]
    getitem_530 = size_826[1]
    getitem_531 = size_826[2];  size_826 = None
    size_827 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_q.size(0)
    size_828 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_q.size(1)
    view_472 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_q.view(size_827, size_828, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_q = size_827 = size_828 = None
    transpose_590 = view_472.transpose(1, 2);  view_472 = None
    size_829 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_k.size(0)
    size_830 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_k.size(1)
    view_473 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_k.view(size_829, size_830, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_k = size_829 = size_830 = None
    transpose_591 = view_473.transpose(1, 2);  view_473 = None
    size_831 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_v.size(0)
    size_832 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_v.size(1)
    view_474 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_v.view(size_831, size_832, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_v = size_831 = size_832 = None
    transpose_592 = view_474.transpose(1, 2);  view_474 = None
    transpose_593 = transpose_591.transpose(-2, -1);  transpose_591 = None
    matmul_236 = torch.matmul(transpose_590, transpose_593);  transpose_590 = transpose_593 = None
    mul_189 = matmul_236 * 0.125;  matmul_236 = None
    softmax_118 = torch.softmax(mul_189, dim = -1);  mul_189 = None
    matmul_237 = torch.matmul(softmax_118, transpose_592);  softmax_118 = transpose_592 = None
    transpose_594 = matmul_237.transpose(1, 2);  matmul_237 = None
    contiguous_125 = transpose_594.contiguous();  transpose_594 = None
    view_475 = contiguous_125.view(getitem_529, getitem_530, getitem_531);  contiguous_125 = getitem_529 = getitem_530 = getitem_531 = None
    up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn1.to_out, "0")(view_475);  view_475 = None
    up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_out_0 = None
    add_207 = up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_out_1 + add_206;  up_blocks_0_attentions_2_transformer_blocks_5_attn1_to_out_1 = add_206 = None
    up_blocks_0_attentions_2_transformer_blocks_5_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").norm2(add_207)
    up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_5_norm2)
    up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_5_norm2)
    up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_5_norm2);  up_blocks_0_attentions_2_transformer_blocks_5_norm2 = None
    size_833 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_q.size()
    getitem_532 = size_833[0]
    getitem_533 = size_833[1]
    getitem_534 = size_833[2];  size_833 = None
    size_834 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_q.size(0)
    size_835 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_q.size(1)
    view_476 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_q.view(size_834, size_835, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_q = size_834 = size_835 = None
    transpose_595 = view_476.transpose(1, 2);  view_476 = None
    size_836 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_k.size(0)
    size_837 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_k.size(1)
    view_477 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_k.view(size_836, size_837, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_k = size_836 = size_837 = None
    transpose_596 = view_477.transpose(1, 2);  view_477 = None
    size_838 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_v.size(0)
    size_839 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_v.size(1)
    view_478 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_v.view(size_838, size_839, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_v = size_838 = size_839 = None
    transpose_597 = view_478.transpose(1, 2);  view_478 = None
    transpose_598 = transpose_596.transpose(-2, -1);  transpose_596 = None
    matmul_238 = torch.matmul(transpose_595, transpose_598);  transpose_595 = transpose_598 = None
    mul_190 = matmul_238 * 0.125;  matmul_238 = None
    softmax_119 = torch.softmax(mul_190, dim = -1);  mul_190 = None
    matmul_239 = torch.matmul(softmax_119, transpose_597);  softmax_119 = transpose_597 = None
    transpose_599 = matmul_239.transpose(1, 2);  matmul_239 = None
    contiguous_126 = transpose_599.contiguous();  transpose_599 = None
    view_479 = contiguous_126.view(getitem_532, getitem_533, getitem_534);  contiguous_126 = getitem_532 = getitem_533 = getitem_534 = None
    up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn2.to_out, "0")(view_479);  view_479 = None
    up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_out_0 = None
    add_208 = up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_out_1 + add_207;  up_blocks_0_attentions_2_transformer_blocks_5_attn2_to_out_1 = add_207 = None
    up_blocks_0_attentions_2_transformer_blocks_5_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").norm3(add_208)
    up_blocks_0_attentions_2_transformer_blocks_5_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_5_norm3);  up_blocks_0_attentions_2_transformer_blocks_5_norm3 = None
    chunk_59 = up_blocks_0_attentions_2_transformer_blocks_5_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_5_ff_net_0_proj = None
    getitem_535 = chunk_59[0]
    getitem_536 = chunk_59[1];  chunk_59 = None
    gelu_59 = torch._C._nn.gelu(getitem_536);  getitem_536 = None
    mul_191 = getitem_535 * gelu_59;  getitem_535 = gelu_59 = None
    up_blocks_0_attentions_2_transformer_blocks_5_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").ff.net, "1")(mul_191);  mul_191 = None
    up_blocks_0_attentions_2_transformer_blocks_5_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "5").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_5_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_5_ff_net_1 = None
    add_209 = up_blocks_0_attentions_2_transformer_blocks_5_ff_net_2 + add_208;  up_blocks_0_attentions_2_transformer_blocks_5_ff_net_2 = add_208 = None
    up_blocks_0_attentions_2_transformer_blocks_6_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").norm1(add_209)
    up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_6_norm1)
    up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_6_norm1)
    up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_6_norm1);  up_blocks_0_attentions_2_transformer_blocks_6_norm1 = None
    size_840 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_q.size()
    getitem_537 = size_840[0]
    getitem_538 = size_840[1]
    getitem_539 = size_840[2];  size_840 = None
    size_841 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_q.size(0)
    size_842 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_q.size(1)
    view_480 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_q.view(size_841, size_842, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_q = size_841 = size_842 = None
    transpose_600 = view_480.transpose(1, 2);  view_480 = None
    size_843 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_k.size(0)
    size_844 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_k.size(1)
    view_481 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_k.view(size_843, size_844, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_k = size_843 = size_844 = None
    transpose_601 = view_481.transpose(1, 2);  view_481 = None
    size_845 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_v.size(0)
    size_846 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_v.size(1)
    view_482 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_v.view(size_845, size_846, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_v = size_845 = size_846 = None
    transpose_602 = view_482.transpose(1, 2);  view_482 = None
    transpose_603 = transpose_601.transpose(-2, -1);  transpose_601 = None
    matmul_240 = torch.matmul(transpose_600, transpose_603);  transpose_600 = transpose_603 = None
    mul_192 = matmul_240 * 0.125;  matmul_240 = None
    softmax_120 = torch.softmax(mul_192, dim = -1);  mul_192 = None
    matmul_241 = torch.matmul(softmax_120, transpose_602);  softmax_120 = transpose_602 = None
    transpose_604 = matmul_241.transpose(1, 2);  matmul_241 = None
    contiguous_127 = transpose_604.contiguous();  transpose_604 = None
    view_483 = contiguous_127.view(getitem_537, getitem_538, getitem_539);  contiguous_127 = getitem_537 = getitem_538 = getitem_539 = None
    up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn1.to_out, "0")(view_483);  view_483 = None
    up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_out_0 = None
    add_210 = up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_out_1 + add_209;  up_blocks_0_attentions_2_transformer_blocks_6_attn1_to_out_1 = add_209 = None
    up_blocks_0_attentions_2_transformer_blocks_6_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").norm2(add_210)
    up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_6_norm2)
    up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_6_norm2)
    up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_6_norm2);  up_blocks_0_attentions_2_transformer_blocks_6_norm2 = None
    size_847 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_q.size()
    getitem_540 = size_847[0]
    getitem_541 = size_847[1]
    getitem_542 = size_847[2];  size_847 = None
    size_848 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_q.size(0)
    size_849 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_q.size(1)
    view_484 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_q.view(size_848, size_849, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_q = size_848 = size_849 = None
    transpose_605 = view_484.transpose(1, 2);  view_484 = None
    size_850 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_k.size(0)
    size_851 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_k.size(1)
    view_485 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_k.view(size_850, size_851, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_k = size_850 = size_851 = None
    transpose_606 = view_485.transpose(1, 2);  view_485 = None
    size_852 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_v.size(0)
    size_853 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_v.size(1)
    view_486 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_v.view(size_852, size_853, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_v = size_852 = size_853 = None
    transpose_607 = view_486.transpose(1, 2);  view_486 = None
    transpose_608 = transpose_606.transpose(-2, -1);  transpose_606 = None
    matmul_242 = torch.matmul(transpose_605, transpose_608);  transpose_605 = transpose_608 = None
    mul_193 = matmul_242 * 0.125;  matmul_242 = None
    softmax_121 = torch.softmax(mul_193, dim = -1);  mul_193 = None
    matmul_243 = torch.matmul(softmax_121, transpose_607);  softmax_121 = transpose_607 = None
    transpose_609 = matmul_243.transpose(1, 2);  matmul_243 = None
    contiguous_128 = transpose_609.contiguous();  transpose_609 = None
    view_487 = contiguous_128.view(getitem_540, getitem_541, getitem_542);  contiguous_128 = getitem_540 = getitem_541 = getitem_542 = None
    up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn2.to_out, "0")(view_487);  view_487 = None
    up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_out_0 = None
    add_211 = up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_out_1 + add_210;  up_blocks_0_attentions_2_transformer_blocks_6_attn2_to_out_1 = add_210 = None
    up_blocks_0_attentions_2_transformer_blocks_6_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").norm3(add_211)
    up_blocks_0_attentions_2_transformer_blocks_6_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_6_norm3);  up_blocks_0_attentions_2_transformer_blocks_6_norm3 = None
    chunk_60 = up_blocks_0_attentions_2_transformer_blocks_6_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_6_ff_net_0_proj = None
    getitem_543 = chunk_60[0]
    getitem_544 = chunk_60[1];  chunk_60 = None
    gelu_60 = torch._C._nn.gelu(getitem_544);  getitem_544 = None
    mul_194 = getitem_543 * gelu_60;  getitem_543 = gelu_60 = None
    up_blocks_0_attentions_2_transformer_blocks_6_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").ff.net, "1")(mul_194);  mul_194 = None
    up_blocks_0_attentions_2_transformer_blocks_6_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "6").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_6_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_6_ff_net_1 = None
    add_212 = up_blocks_0_attentions_2_transformer_blocks_6_ff_net_2 + add_211;  up_blocks_0_attentions_2_transformer_blocks_6_ff_net_2 = add_211 = None
    up_blocks_0_attentions_2_transformer_blocks_7_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").norm1(add_212)
    up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_7_norm1)
    up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_7_norm1)
    up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_7_norm1);  up_blocks_0_attentions_2_transformer_blocks_7_norm1 = None
    size_854 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_q.size()
    getitem_545 = size_854[0]
    getitem_546 = size_854[1]
    getitem_547 = size_854[2];  size_854 = None
    size_855 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_q.size(0)
    size_856 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_q.size(1)
    view_488 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_q.view(size_855, size_856, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_q = size_855 = size_856 = None
    transpose_610 = view_488.transpose(1, 2);  view_488 = None
    size_857 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_k.size(0)
    size_858 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_k.size(1)
    view_489 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_k.view(size_857, size_858, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_k = size_857 = size_858 = None
    transpose_611 = view_489.transpose(1, 2);  view_489 = None
    size_859 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_v.size(0)
    size_860 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_v.size(1)
    view_490 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_v.view(size_859, size_860, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_v = size_859 = size_860 = None
    transpose_612 = view_490.transpose(1, 2);  view_490 = None
    transpose_613 = transpose_611.transpose(-2, -1);  transpose_611 = None
    matmul_244 = torch.matmul(transpose_610, transpose_613);  transpose_610 = transpose_613 = None
    mul_195 = matmul_244 * 0.125;  matmul_244 = None
    softmax_122 = torch.softmax(mul_195, dim = -1);  mul_195 = None
    matmul_245 = torch.matmul(softmax_122, transpose_612);  softmax_122 = transpose_612 = None
    transpose_614 = matmul_245.transpose(1, 2);  matmul_245 = None
    contiguous_129 = transpose_614.contiguous();  transpose_614 = None
    view_491 = contiguous_129.view(getitem_545, getitem_546, getitem_547);  contiguous_129 = getitem_545 = getitem_546 = getitem_547 = None
    up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn1.to_out, "0")(view_491);  view_491 = None
    up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_out_0 = None
    add_213 = up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_out_1 + add_212;  up_blocks_0_attentions_2_transformer_blocks_7_attn1_to_out_1 = add_212 = None
    up_blocks_0_attentions_2_transformer_blocks_7_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").norm2(add_213)
    up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_7_norm2)
    up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_7_norm2)
    up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_7_norm2);  up_blocks_0_attentions_2_transformer_blocks_7_norm2 = None
    size_861 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_q.size()
    getitem_548 = size_861[0]
    getitem_549 = size_861[1]
    getitem_550 = size_861[2];  size_861 = None
    size_862 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_q.size(0)
    size_863 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_q.size(1)
    view_492 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_q.view(size_862, size_863, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_q = size_862 = size_863 = None
    transpose_615 = view_492.transpose(1, 2);  view_492 = None
    size_864 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_k.size(0)
    size_865 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_k.size(1)
    view_493 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_k.view(size_864, size_865, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_k = size_864 = size_865 = None
    transpose_616 = view_493.transpose(1, 2);  view_493 = None
    size_866 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_v.size(0)
    size_867 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_v.size(1)
    view_494 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_v.view(size_866, size_867, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_v = size_866 = size_867 = None
    transpose_617 = view_494.transpose(1, 2);  view_494 = None
    transpose_618 = transpose_616.transpose(-2, -1);  transpose_616 = None
    matmul_246 = torch.matmul(transpose_615, transpose_618);  transpose_615 = transpose_618 = None
    mul_196 = matmul_246 * 0.125;  matmul_246 = None
    softmax_123 = torch.softmax(mul_196, dim = -1);  mul_196 = None
    matmul_247 = torch.matmul(softmax_123, transpose_617);  softmax_123 = transpose_617 = None
    transpose_619 = matmul_247.transpose(1, 2);  matmul_247 = None
    contiguous_130 = transpose_619.contiguous();  transpose_619 = None
    view_495 = contiguous_130.view(getitem_548, getitem_549, getitem_550);  contiguous_130 = getitem_548 = getitem_549 = getitem_550 = None
    up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn2.to_out, "0")(view_495);  view_495 = None
    up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_out_0 = None
    add_214 = up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_out_1 + add_213;  up_blocks_0_attentions_2_transformer_blocks_7_attn2_to_out_1 = add_213 = None
    up_blocks_0_attentions_2_transformer_blocks_7_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").norm3(add_214)
    up_blocks_0_attentions_2_transformer_blocks_7_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_7_norm3);  up_blocks_0_attentions_2_transformer_blocks_7_norm3 = None
    chunk_61 = up_blocks_0_attentions_2_transformer_blocks_7_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_7_ff_net_0_proj = None
    getitem_551 = chunk_61[0]
    getitem_552 = chunk_61[1];  chunk_61 = None
    gelu_61 = torch._C._nn.gelu(getitem_552);  getitem_552 = None
    mul_197 = getitem_551 * gelu_61;  getitem_551 = gelu_61 = None
    up_blocks_0_attentions_2_transformer_blocks_7_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").ff.net, "1")(mul_197);  mul_197 = None
    up_blocks_0_attentions_2_transformer_blocks_7_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "7").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_7_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_7_ff_net_1 = None
    add_215 = up_blocks_0_attentions_2_transformer_blocks_7_ff_net_2 + add_214;  up_blocks_0_attentions_2_transformer_blocks_7_ff_net_2 = add_214 = None
    up_blocks_0_attentions_2_transformer_blocks_8_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").norm1(add_215)
    up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_8_norm1)
    up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_8_norm1)
    up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_8_norm1);  up_blocks_0_attentions_2_transformer_blocks_8_norm1 = None
    size_868 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_q.size()
    getitem_553 = size_868[0]
    getitem_554 = size_868[1]
    getitem_555 = size_868[2];  size_868 = None
    size_869 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_q.size(0)
    size_870 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_q.size(1)
    view_496 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_q.view(size_869, size_870, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_q = size_869 = size_870 = None
    transpose_620 = view_496.transpose(1, 2);  view_496 = None
    size_871 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_k.size(0)
    size_872 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_k.size(1)
    view_497 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_k.view(size_871, size_872, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_k = size_871 = size_872 = None
    transpose_621 = view_497.transpose(1, 2);  view_497 = None
    size_873 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_v.size(0)
    size_874 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_v.size(1)
    view_498 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_v.view(size_873, size_874, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_v = size_873 = size_874 = None
    transpose_622 = view_498.transpose(1, 2);  view_498 = None
    transpose_623 = transpose_621.transpose(-2, -1);  transpose_621 = None
    matmul_248 = torch.matmul(transpose_620, transpose_623);  transpose_620 = transpose_623 = None
    mul_198 = matmul_248 * 0.125;  matmul_248 = None
    softmax_124 = torch.softmax(mul_198, dim = -1);  mul_198 = None
    matmul_249 = torch.matmul(softmax_124, transpose_622);  softmax_124 = transpose_622 = None
    transpose_624 = matmul_249.transpose(1, 2);  matmul_249 = None
    contiguous_131 = transpose_624.contiguous();  transpose_624 = None
    view_499 = contiguous_131.view(getitem_553, getitem_554, getitem_555);  contiguous_131 = getitem_553 = getitem_554 = getitem_555 = None
    up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn1.to_out, "0")(view_499);  view_499 = None
    up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_out_0 = None
    add_216 = up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_out_1 + add_215;  up_blocks_0_attentions_2_transformer_blocks_8_attn1_to_out_1 = add_215 = None
    up_blocks_0_attentions_2_transformer_blocks_8_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").norm2(add_216)
    up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_8_norm2)
    up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_8_norm2)
    up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_8_norm2);  up_blocks_0_attentions_2_transformer_blocks_8_norm2 = None
    size_875 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_q.size()
    getitem_556 = size_875[0]
    getitem_557 = size_875[1]
    getitem_558 = size_875[2];  size_875 = None
    size_876 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_q.size(0)
    size_877 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_q.size(1)
    view_500 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_q.view(size_876, size_877, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_q = size_876 = size_877 = None
    transpose_625 = view_500.transpose(1, 2);  view_500 = None
    size_878 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_k.size(0)
    size_879 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_k.size(1)
    view_501 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_k.view(size_878, size_879, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_k = size_878 = size_879 = None
    transpose_626 = view_501.transpose(1, 2);  view_501 = None
    size_880 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_v.size(0)
    size_881 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_v.size(1)
    view_502 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_v.view(size_880, size_881, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_v = size_880 = size_881 = None
    transpose_627 = view_502.transpose(1, 2);  view_502 = None
    transpose_628 = transpose_626.transpose(-2, -1);  transpose_626 = None
    matmul_250 = torch.matmul(transpose_625, transpose_628);  transpose_625 = transpose_628 = None
    mul_199 = matmul_250 * 0.125;  matmul_250 = None
    softmax_125 = torch.softmax(mul_199, dim = -1);  mul_199 = None
    matmul_251 = torch.matmul(softmax_125, transpose_627);  softmax_125 = transpose_627 = None
    transpose_629 = matmul_251.transpose(1, 2);  matmul_251 = None
    contiguous_132 = transpose_629.contiguous();  transpose_629 = None
    view_503 = contiguous_132.view(getitem_556, getitem_557, getitem_558);  contiguous_132 = getitem_556 = getitem_557 = getitem_558 = None
    up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn2.to_out, "0")(view_503);  view_503 = None
    up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_out_0 = None
    add_217 = up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_out_1 + add_216;  up_blocks_0_attentions_2_transformer_blocks_8_attn2_to_out_1 = add_216 = None
    up_blocks_0_attentions_2_transformer_blocks_8_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").norm3(add_217)
    up_blocks_0_attentions_2_transformer_blocks_8_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_8_norm3);  up_blocks_0_attentions_2_transformer_blocks_8_norm3 = None
    chunk_62 = up_blocks_0_attentions_2_transformer_blocks_8_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_8_ff_net_0_proj = None
    getitem_559 = chunk_62[0]
    getitem_560 = chunk_62[1];  chunk_62 = None
    gelu_62 = torch._C._nn.gelu(getitem_560);  getitem_560 = None
    mul_200 = getitem_559 * gelu_62;  getitem_559 = gelu_62 = None
    up_blocks_0_attentions_2_transformer_blocks_8_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").ff.net, "1")(mul_200);  mul_200 = None
    up_blocks_0_attentions_2_transformer_blocks_8_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "8").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_8_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_8_ff_net_1 = None
    add_218 = up_blocks_0_attentions_2_transformer_blocks_8_ff_net_2 + add_217;  up_blocks_0_attentions_2_transformer_blocks_8_ff_net_2 = add_217 = None
    up_blocks_0_attentions_2_transformer_blocks_9_norm1 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").norm1(add_218)
    up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn1.to_q(up_blocks_0_attentions_2_transformer_blocks_9_norm1)
    up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn1.to_k(up_blocks_0_attentions_2_transformer_blocks_9_norm1)
    up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn1.to_v(up_blocks_0_attentions_2_transformer_blocks_9_norm1);  up_blocks_0_attentions_2_transformer_blocks_9_norm1 = None
    size_882 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_q.size()
    getitem_561 = size_882[0]
    getitem_562 = size_882[1]
    getitem_563 = size_882[2];  size_882 = None
    size_883 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_q.size(0)
    size_884 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_q.size(1)
    view_504 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_q.view(size_883, size_884, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_q = size_883 = size_884 = None
    transpose_630 = view_504.transpose(1, 2);  view_504 = None
    size_885 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_k.size(0)
    size_886 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_k.size(1)
    view_505 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_k.view(size_885, size_886, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_k = size_885 = size_886 = None
    transpose_631 = view_505.transpose(1, 2);  view_505 = None
    size_887 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_v.size(0)
    size_888 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_v.size(1)
    view_506 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_v.view(size_887, size_888, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_v = size_887 = size_888 = None
    transpose_632 = view_506.transpose(1, 2);  view_506 = None
    transpose_633 = transpose_631.transpose(-2, -1);  transpose_631 = None
    matmul_252 = torch.matmul(transpose_630, transpose_633);  transpose_630 = transpose_633 = None
    mul_201 = matmul_252 * 0.125;  matmul_252 = None
    softmax_126 = torch.softmax(mul_201, dim = -1);  mul_201 = None
    matmul_253 = torch.matmul(softmax_126, transpose_632);  softmax_126 = transpose_632 = None
    transpose_634 = matmul_253.transpose(1, 2);  matmul_253 = None
    contiguous_133 = transpose_634.contiguous();  transpose_634 = None
    view_507 = contiguous_133.view(getitem_561, getitem_562, getitem_563);  contiguous_133 = getitem_561 = getitem_562 = getitem_563 = None
    up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn1.to_out, "0")(view_507);  view_507 = None
    up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn1.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_out_0 = None
    add_219 = up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_out_1 + add_218;  up_blocks_0_attentions_2_transformer_blocks_9_attn1_to_out_1 = add_218 = None
    up_blocks_0_attentions_2_transformer_blocks_9_norm2 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").norm2(add_219)
    up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn2.to_q(up_blocks_0_attentions_2_transformer_blocks_9_norm2)
    up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn2.to_k(up_blocks_0_attentions_2_transformer_blocks_9_norm2)
    up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn2.to_v(up_blocks_0_attentions_2_transformer_blocks_9_norm2);  up_blocks_0_attentions_2_transformer_blocks_9_norm2 = None
    size_889 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_q.size()
    getitem_564 = size_889[0]
    getitem_565 = size_889[1]
    getitem_566 = size_889[2];  size_889 = None
    size_890 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_q.size(0)
    size_891 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_q.size(1)
    view_508 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_q.view(size_890, size_891, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_q = size_890 = size_891 = None
    transpose_635 = view_508.transpose(1, 2);  view_508 = None
    size_892 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_k.size(0)
    size_893 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_k.size(1)
    view_509 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_k.view(size_892, size_893, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_k = size_892 = size_893 = None
    transpose_636 = view_509.transpose(1, 2);  view_509 = None
    size_894 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_v.size(0)
    size_895 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_v.size(1)
    view_510 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_v.view(size_894, size_895, 20, 64);  up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_v = size_894 = size_895 = None
    transpose_637 = view_510.transpose(1, 2);  view_510 = None
    transpose_638 = transpose_636.transpose(-2, -1);  transpose_636 = None
    matmul_254 = torch.matmul(transpose_635, transpose_638);  transpose_635 = transpose_638 = None
    mul_202 = matmul_254 * 0.125;  matmul_254 = None
    softmax_127 = torch.softmax(mul_202, dim = -1);  mul_202 = None
    matmul_255 = torch.matmul(softmax_127, transpose_637);  softmax_127 = transpose_637 = None
    transpose_639 = matmul_255.transpose(1, 2);  matmul_255 = None
    contiguous_134 = transpose_639.contiguous();  transpose_639 = None
    view_511 = contiguous_134.view(getitem_564, getitem_565, getitem_566);  contiguous_134 = getitem_564 = getitem_565 = getitem_566 = None
    up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn2.to_out, "0")(view_511);  view_511 = None
    up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").attn2.to_out, "1")(up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_out_0);  up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_out_0 = None
    add_220 = up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_out_1 + add_219;  up_blocks_0_attentions_2_transformer_blocks_9_attn2_to_out_1 = add_219 = None
    up_blocks_0_attentions_2_transformer_blocks_9_norm3 = getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").norm3(add_220)
    up_blocks_0_attentions_2_transformer_blocks_9_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").ff.net, "0").proj(up_blocks_0_attentions_2_transformer_blocks_9_norm3);  up_blocks_0_attentions_2_transformer_blocks_9_norm3 = None
    chunk_63 = up_blocks_0_attentions_2_transformer_blocks_9_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_0_attentions_2_transformer_blocks_9_ff_net_0_proj = None
    getitem_567 = chunk_63[0]
    getitem_568 = chunk_63[1];  chunk_63 = None
    gelu_63 = torch._C._nn.gelu(getitem_568);  getitem_568 = None
    mul_203 = getitem_567 * gelu_63;  getitem_567 = gelu_63 = None
    up_blocks_0_attentions_2_transformer_blocks_9_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").ff.net, "1")(mul_203);  mul_203 = None
    up_blocks_0_attentions_2_transformer_blocks_9_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "0").attentions, "2").transformer_blocks, "9").ff.net, "2")(up_blocks_0_attentions_2_transformer_blocks_9_ff_net_1);  up_blocks_0_attentions_2_transformer_blocks_9_ff_net_1 = None
    add_221 = up_blocks_0_attentions_2_transformer_blocks_9_ff_net_2 + add_220;  up_blocks_0_attentions_2_transformer_blocks_9_ff_net_2 = add_220 = None
    up_blocks_0_attentions_2_proj_out = getattr(getattr(self.up_blocks, "0").attentions, "2").proj_out(add_221);  add_221 = None
    reshape_16 = up_blocks_0_attentions_2_proj_out.reshape(getitem_484, getitem_486, getitem_487, getitem_488);  up_blocks_0_attentions_2_proj_out = getitem_484 = getitem_486 = getitem_487 = getitem_488 = None
    permute_15 = reshape_16.permute(0, 3, 1, 2);  reshape_16 = None
    contiguous_135 = permute_15.contiguous();  permute_15 = None
    add_222 = contiguous_135 + add_191;  contiguous_135 = add_191 = None
    interpolate = torch.nn.functional.interpolate(add_222, size = None, scale_factor = 2.0, mode = 'nearest', align_corners = None, recompute_scale_factor = None, antialias = False);  add_222 = None
    up_blocks_0_upsamplers_0_conv = getattr(getattr(self.up_blocks, "0").upsamplers, "0").conv(interpolate);  interpolate = None
    cat_5 = torch.cat([up_blocks_0_upsamplers_0_conv, add_22], dim = 1);  up_blocks_0_upsamplers_0_conv = add_22 = None
    up_blocks_1_resnets_0_norm1 = getattr(getattr(self.up_blocks, "1").resnets, "0").norm1(cat_5)
    up_blocks_1_resnets_0_nonlinearity = getattr(getattr(self.up_blocks, "1").resnets, "0").nonlinearity(up_blocks_1_resnets_0_norm1);  up_blocks_1_resnets_0_norm1 = None
    up_blocks_1_resnets_0_conv1 = getattr(getattr(self.up_blocks, "1").resnets, "0").conv1(up_blocks_1_resnets_0_nonlinearity);  up_blocks_1_resnets_0_nonlinearity = None
    up_blocks_1_resnets_0_nonlinearity_1 = getattr(getattr(self.up_blocks, "1").resnets, "0").nonlinearity(add)
    up_blocks_1_resnets_0_time_emb_proj = getattr(getattr(self.up_blocks, "1").resnets, "0").time_emb_proj(up_blocks_1_resnets_0_nonlinearity_1);  up_blocks_1_resnets_0_nonlinearity_1 = None
    getitem_569 = up_blocks_1_resnets_0_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_1_resnets_0_time_emb_proj = None
    add_223 = up_blocks_1_resnets_0_conv1 + getitem_569;  up_blocks_1_resnets_0_conv1 = getitem_569 = None
    up_blocks_1_resnets_0_norm2 = getattr(getattr(self.up_blocks, "1").resnets, "0").norm2(add_223);  add_223 = None
    up_blocks_1_resnets_0_nonlinearity_2 = getattr(getattr(self.up_blocks, "1").resnets, "0").nonlinearity(up_blocks_1_resnets_0_norm2);  up_blocks_1_resnets_0_norm2 = None
    up_blocks_1_resnets_0_dropout = getattr(getattr(self.up_blocks, "1").resnets, "0").dropout(up_blocks_1_resnets_0_nonlinearity_2);  up_blocks_1_resnets_0_nonlinearity_2 = None
    up_blocks_1_resnets_0_conv2 = getattr(getattr(self.up_blocks, "1").resnets, "0").conv2(up_blocks_1_resnets_0_dropout);  up_blocks_1_resnets_0_dropout = None
    up_blocks_1_resnets_0_conv_shortcut = getattr(getattr(self.up_blocks, "1").resnets, "0").conv_shortcut(cat_5);  cat_5 = None
    add_224 = up_blocks_1_resnets_0_conv_shortcut + up_blocks_1_resnets_0_conv2;  up_blocks_1_resnets_0_conv_shortcut = up_blocks_1_resnets_0_conv2 = None
    getattr_25 = add_224.shape
    getitem_570 = getattr_25[0]
    getitem_571 = getattr_25[1]
    getitem_572 = getattr_25[2]
    getitem_573 = getattr_25[3];  getattr_25 = None
    up_blocks_1_attentions_0_norm = getattr(getattr(self.up_blocks, "1").attentions, "0").norm(add_224)
    getattr_26 = up_blocks_1_attentions_0_norm.shape
    getitem_574 = getattr_26[1];  getattr_26 = None
    permute_16 = up_blocks_1_attentions_0_norm.permute(0, 2, 3, 1);  up_blocks_1_attentions_0_norm = None
    mul_204 = getitem_572 * getitem_573
    reshape_17 = permute_16.reshape(getitem_570, mul_204, getitem_574);  permute_16 = mul_204 = None
    up_blocks_1_attentions_0_proj_in = getattr(getattr(self.up_blocks, "1").attentions, "0").proj_in(reshape_17);  reshape_17 = None
    up_blocks_1_attentions_0_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").norm1(up_blocks_1_attentions_0_proj_in)
    up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_q(up_blocks_1_attentions_0_transformer_blocks_0_norm1)
    up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_k(up_blocks_1_attentions_0_transformer_blocks_0_norm1)
    up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_v(up_blocks_1_attentions_0_transformer_blocks_0_norm1);  up_blocks_1_attentions_0_transformer_blocks_0_norm1 = None
    size_896 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.size()
    getitem_575 = size_896[0]
    getitem_576 = size_896[1]
    getitem_577 = size_896[2];  size_896 = None
    size_897 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.size(0)
    size_898 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.size(1)
    view_512 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.view(size_897, size_898, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q = size_897 = size_898 = None
    transpose_640 = view_512.transpose(1, 2);  view_512 = None
    size_899 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.size(0)
    size_900 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.size(1)
    view_513 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.view(size_899, size_900, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k = size_899 = size_900 = None
    transpose_641 = view_513.transpose(1, 2);  view_513 = None
    size_901 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.size(0)
    size_902 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.size(1)
    view_514 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.view(size_901, size_902, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v = size_901 = size_902 = None
    transpose_642 = view_514.transpose(1, 2);  view_514 = None
    transpose_643 = transpose_641.transpose(-2, -1);  transpose_641 = None
    matmul_256 = torch.matmul(transpose_640, transpose_643);  transpose_640 = transpose_643 = None
    mul_205 = matmul_256 * 0.125;  matmul_256 = None
    softmax_128 = torch.softmax(mul_205, dim = -1);  mul_205 = None
    matmul_257 = torch.matmul(softmax_128, transpose_642);  softmax_128 = transpose_642 = None
    transpose_644 = matmul_257.transpose(1, 2);  matmul_257 = None
    contiguous_136 = transpose_644.contiguous();  transpose_644 = None
    view_515 = contiguous_136.view(getitem_575, getitem_576, getitem_577);  contiguous_136 = getitem_575 = getitem_576 = getitem_577 = None
    up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_out, "0")(view_515);  view_515 = None
    up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn1.to_out, "1")(up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0);  up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 = None
    add_225 = up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_1 + up_blocks_1_attentions_0_proj_in;  up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_1 = up_blocks_1_attentions_0_proj_in = None
    up_blocks_1_attentions_0_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").norm2(add_225)
    up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_q(up_blocks_1_attentions_0_transformer_blocks_0_norm2)
    up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_k(up_blocks_1_attentions_0_transformer_blocks_0_norm2)
    up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_v(up_blocks_1_attentions_0_transformer_blocks_0_norm2);  up_blocks_1_attentions_0_transformer_blocks_0_norm2 = None
    size_903 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.size()
    getitem_578 = size_903[0]
    getitem_579 = size_903[1]
    getitem_580 = size_903[2];  size_903 = None
    size_904 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.size(0)
    size_905 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.size(1)
    view_516 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.view(size_904, size_905, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q = size_904 = size_905 = None
    transpose_645 = view_516.transpose(1, 2);  view_516 = None
    size_906 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.size(0)
    size_907 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.size(1)
    view_517 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.view(size_906, size_907, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k = size_906 = size_907 = None
    transpose_646 = view_517.transpose(1, 2);  view_517 = None
    size_908 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.size(0)
    size_909 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.size(1)
    view_518 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.view(size_908, size_909, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v = size_908 = size_909 = None
    transpose_647 = view_518.transpose(1, 2);  view_518 = None
    transpose_648 = transpose_646.transpose(-2, -1);  transpose_646 = None
    matmul_258 = torch.matmul(transpose_645, transpose_648);  transpose_645 = transpose_648 = None
    mul_206 = matmul_258 * 0.125;  matmul_258 = None
    softmax_129 = torch.softmax(mul_206, dim = -1);  mul_206 = None
    matmul_259 = torch.matmul(softmax_129, transpose_647);  softmax_129 = transpose_647 = None
    transpose_649 = matmul_259.transpose(1, 2);  matmul_259 = None
    contiguous_137 = transpose_649.contiguous();  transpose_649 = None
    view_519 = contiguous_137.view(getitem_578, getitem_579, getitem_580);  contiguous_137 = getitem_578 = getitem_579 = getitem_580 = None
    up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_out, "0")(view_519);  view_519 = None
    up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").attn2.to_out, "1")(up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0);  up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 = None
    add_226 = up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_1 + add_225;  up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_1 = add_225 = None
    up_blocks_1_attentions_0_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").norm3(add_226)
    up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").ff.net, "0").proj(up_blocks_1_attentions_0_transformer_blocks_0_norm3);  up_blocks_1_attentions_0_transformer_blocks_0_norm3 = None
    chunk_64 = up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj = None
    getitem_581 = chunk_64[0]
    getitem_582 = chunk_64[1];  chunk_64 = None
    gelu_64 = torch._C._nn.gelu(getitem_582);  getitem_582 = None
    mul_207 = getitem_581 * gelu_64;  getitem_581 = gelu_64 = None
    up_blocks_1_attentions_0_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").ff.net, "1")(mul_207);  mul_207 = None
    up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "0").ff.net, "2")(up_blocks_1_attentions_0_transformer_blocks_0_ff_net_1);  up_blocks_1_attentions_0_transformer_blocks_0_ff_net_1 = None
    add_227 = up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 + add_226;  up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 = add_226 = None
    up_blocks_1_attentions_0_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").norm1(add_227)
    up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_q(up_blocks_1_attentions_0_transformer_blocks_1_norm1)
    up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_k(up_blocks_1_attentions_0_transformer_blocks_1_norm1)
    up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_v(up_blocks_1_attentions_0_transformer_blocks_1_norm1);  up_blocks_1_attentions_0_transformer_blocks_1_norm1 = None
    size_910 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.size()
    getitem_583 = size_910[0]
    getitem_584 = size_910[1]
    getitem_585 = size_910[2];  size_910 = None
    size_911 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.size(0)
    size_912 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.size(1)
    view_520 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q.view(size_911, size_912, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_q = size_911 = size_912 = None
    transpose_650 = view_520.transpose(1, 2);  view_520 = None
    size_913 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k.size(0)
    size_914 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k.size(1)
    view_521 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k.view(size_913, size_914, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_k = size_913 = size_914 = None
    transpose_651 = view_521.transpose(1, 2);  view_521 = None
    size_915 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v.size(0)
    size_916 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v.size(1)
    view_522 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v.view(size_915, size_916, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_v = size_915 = size_916 = None
    transpose_652 = view_522.transpose(1, 2);  view_522 = None
    transpose_653 = transpose_651.transpose(-2, -1);  transpose_651 = None
    matmul_260 = torch.matmul(transpose_650, transpose_653);  transpose_650 = transpose_653 = None
    mul_208 = matmul_260 * 0.125;  matmul_260 = None
    softmax_130 = torch.softmax(mul_208, dim = -1);  mul_208 = None
    matmul_261 = torch.matmul(softmax_130, transpose_652);  softmax_130 = transpose_652 = None
    transpose_654 = matmul_261.transpose(1, 2);  matmul_261 = None
    contiguous_138 = transpose_654.contiguous();  transpose_654 = None
    view_523 = contiguous_138.view(getitem_583, getitem_584, getitem_585);  contiguous_138 = getitem_583 = getitem_584 = getitem_585 = None
    up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_out, "0")(view_523);  view_523 = None
    up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn1.to_out, "1")(up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_0);  up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_0 = None
    add_228 = up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_1 + add_227;  up_blocks_1_attentions_0_transformer_blocks_1_attn1_to_out_1 = add_227 = None
    up_blocks_1_attentions_0_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").norm2(add_228)
    up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_q(up_blocks_1_attentions_0_transformer_blocks_1_norm2)
    up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_k(up_blocks_1_attentions_0_transformer_blocks_1_norm2)
    up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_v(up_blocks_1_attentions_0_transformer_blocks_1_norm2);  up_blocks_1_attentions_0_transformer_blocks_1_norm2 = None
    size_917 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.size()
    getitem_586 = size_917[0]
    getitem_587 = size_917[1]
    getitem_588 = size_917[2];  size_917 = None
    size_918 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.size(0)
    size_919 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.size(1)
    view_524 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q.view(size_918, size_919, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_q = size_918 = size_919 = None
    transpose_655 = view_524.transpose(1, 2);  view_524 = None
    size_920 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k.size(0)
    size_921 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k.size(1)
    view_525 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k.view(size_920, size_921, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_k = size_920 = size_921 = None
    transpose_656 = view_525.transpose(1, 2);  view_525 = None
    size_922 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v.size(0)
    size_923 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v.size(1)
    view_526 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v.view(size_922, size_923, 10, 64);  up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_v = size_922 = size_923 = None
    transpose_657 = view_526.transpose(1, 2);  view_526 = None
    transpose_658 = transpose_656.transpose(-2, -1);  transpose_656 = None
    matmul_262 = torch.matmul(transpose_655, transpose_658);  transpose_655 = transpose_658 = None
    mul_209 = matmul_262 * 0.125;  matmul_262 = None
    softmax_131 = torch.softmax(mul_209, dim = -1);  mul_209 = None
    matmul_263 = torch.matmul(softmax_131, transpose_657);  softmax_131 = transpose_657 = None
    transpose_659 = matmul_263.transpose(1, 2);  matmul_263 = None
    contiguous_139 = transpose_659.contiguous();  transpose_659 = None
    view_527 = contiguous_139.view(getitem_586, getitem_587, getitem_588);  contiguous_139 = getitem_586 = getitem_587 = getitem_588 = None
    up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_out, "0")(view_527);  view_527 = None
    up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").attn2.to_out, "1")(up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_0);  up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_0 = None
    add_229 = up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_1 + add_228;  up_blocks_1_attentions_0_transformer_blocks_1_attn2_to_out_1 = add_228 = None
    up_blocks_1_attentions_0_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").norm3(add_229)
    up_blocks_1_attentions_0_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").ff.net, "0").proj(up_blocks_1_attentions_0_transformer_blocks_1_norm3);  up_blocks_1_attentions_0_transformer_blocks_1_norm3 = None
    chunk_65 = up_blocks_1_attentions_0_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_1_attentions_0_transformer_blocks_1_ff_net_0_proj = None
    getitem_589 = chunk_65[0]
    getitem_590 = chunk_65[1];  chunk_65 = None
    gelu_65 = torch._C._nn.gelu(getitem_590);  getitem_590 = None
    mul_210 = getitem_589 * gelu_65;  getitem_589 = gelu_65 = None
    up_blocks_1_attentions_0_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").ff.net, "1")(mul_210);  mul_210 = None
    up_blocks_1_attentions_0_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "0").transformer_blocks, "1").ff.net, "2")(up_blocks_1_attentions_0_transformer_blocks_1_ff_net_1);  up_blocks_1_attentions_0_transformer_blocks_1_ff_net_1 = None
    add_230 = up_blocks_1_attentions_0_transformer_blocks_1_ff_net_2 + add_229;  up_blocks_1_attentions_0_transformer_blocks_1_ff_net_2 = add_229 = None
    up_blocks_1_attentions_0_proj_out = getattr(getattr(self.up_blocks, "1").attentions, "0").proj_out(add_230);  add_230 = None
    reshape_18 = up_blocks_1_attentions_0_proj_out.reshape(getitem_570, getitem_572, getitem_573, getitem_574);  up_blocks_1_attentions_0_proj_out = getitem_570 = getitem_572 = getitem_573 = getitem_574 = None
    permute_17 = reshape_18.permute(0, 3, 1, 2);  reshape_18 = None
    contiguous_140 = permute_17.contiguous();  permute_17 = None
    add_231 = contiguous_140 + add_224;  contiguous_140 = add_224 = None
    cat_6 = torch.cat([add_231, add_13], dim = 1);  add_231 = add_13 = None
    up_blocks_1_resnets_1_norm1 = getattr(getattr(self.up_blocks, "1").resnets, "1").norm1(cat_6)
    up_blocks_1_resnets_1_nonlinearity = getattr(getattr(self.up_blocks, "1").resnets, "1").nonlinearity(up_blocks_1_resnets_1_norm1);  up_blocks_1_resnets_1_norm1 = None
    up_blocks_1_resnets_1_conv1 = getattr(getattr(self.up_blocks, "1").resnets, "1").conv1(up_blocks_1_resnets_1_nonlinearity);  up_blocks_1_resnets_1_nonlinearity = None
    up_blocks_1_resnets_1_nonlinearity_1 = getattr(getattr(self.up_blocks, "1").resnets, "1").nonlinearity(add)
    up_blocks_1_resnets_1_time_emb_proj = getattr(getattr(self.up_blocks, "1").resnets, "1").time_emb_proj(up_blocks_1_resnets_1_nonlinearity_1);  up_blocks_1_resnets_1_nonlinearity_1 = None
    getitem_591 = up_blocks_1_resnets_1_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_1_resnets_1_time_emb_proj = None
    add_232 = up_blocks_1_resnets_1_conv1 + getitem_591;  up_blocks_1_resnets_1_conv1 = getitem_591 = None
    up_blocks_1_resnets_1_norm2 = getattr(getattr(self.up_blocks, "1").resnets, "1").norm2(add_232);  add_232 = None
    up_blocks_1_resnets_1_nonlinearity_2 = getattr(getattr(self.up_blocks, "1").resnets, "1").nonlinearity(up_blocks_1_resnets_1_norm2);  up_blocks_1_resnets_1_norm2 = None
    up_blocks_1_resnets_1_dropout = getattr(getattr(self.up_blocks, "1").resnets, "1").dropout(up_blocks_1_resnets_1_nonlinearity_2);  up_blocks_1_resnets_1_nonlinearity_2 = None
    up_blocks_1_resnets_1_conv2 = getattr(getattr(self.up_blocks, "1").resnets, "1").conv2(up_blocks_1_resnets_1_dropout);  up_blocks_1_resnets_1_dropout = None
    up_blocks_1_resnets_1_conv_shortcut = getattr(getattr(self.up_blocks, "1").resnets, "1").conv_shortcut(cat_6);  cat_6 = None
    add_233 = up_blocks_1_resnets_1_conv_shortcut + up_blocks_1_resnets_1_conv2;  up_blocks_1_resnets_1_conv_shortcut = up_blocks_1_resnets_1_conv2 = None
    getattr_27 = add_233.shape
    getitem_592 = getattr_27[0]
    getitem_593 = getattr_27[1]
    getitem_594 = getattr_27[2]
    getitem_595 = getattr_27[3];  getattr_27 = None
    up_blocks_1_attentions_1_norm = getattr(getattr(self.up_blocks, "1").attentions, "1").norm(add_233)
    getattr_28 = up_blocks_1_attentions_1_norm.shape
    getitem_596 = getattr_28[1];  getattr_28 = None
    permute_18 = up_blocks_1_attentions_1_norm.permute(0, 2, 3, 1);  up_blocks_1_attentions_1_norm = None
    mul_211 = getitem_594 * getitem_595
    reshape_19 = permute_18.reshape(getitem_592, mul_211, getitem_596);  permute_18 = mul_211 = None
    up_blocks_1_attentions_1_proj_in = getattr(getattr(self.up_blocks, "1").attentions, "1").proj_in(reshape_19);  reshape_19 = None
    up_blocks_1_attentions_1_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").norm1(up_blocks_1_attentions_1_proj_in)
    up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_q(up_blocks_1_attentions_1_transformer_blocks_0_norm1)
    up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_k(up_blocks_1_attentions_1_transformer_blocks_0_norm1)
    up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_v(up_blocks_1_attentions_1_transformer_blocks_0_norm1);  up_blocks_1_attentions_1_transformer_blocks_0_norm1 = None
    size_924 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.size()
    getitem_597 = size_924[0]
    getitem_598 = size_924[1]
    getitem_599 = size_924[2];  size_924 = None
    size_925 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.size(0)
    size_926 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.size(1)
    view_528 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.view(size_925, size_926, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q = size_925 = size_926 = None
    transpose_660 = view_528.transpose(1, 2);  view_528 = None
    size_927 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.size(0)
    size_928 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.size(1)
    view_529 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.view(size_927, size_928, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k = size_927 = size_928 = None
    transpose_661 = view_529.transpose(1, 2);  view_529 = None
    size_929 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.size(0)
    size_930 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.size(1)
    view_530 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.view(size_929, size_930, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v = size_929 = size_930 = None
    transpose_662 = view_530.transpose(1, 2);  view_530 = None
    transpose_663 = transpose_661.transpose(-2, -1);  transpose_661 = None
    matmul_264 = torch.matmul(transpose_660, transpose_663);  transpose_660 = transpose_663 = None
    mul_212 = matmul_264 * 0.125;  matmul_264 = None
    softmax_132 = torch.softmax(mul_212, dim = -1);  mul_212 = None
    matmul_265 = torch.matmul(softmax_132, transpose_662);  softmax_132 = transpose_662 = None
    transpose_664 = matmul_265.transpose(1, 2);  matmul_265 = None
    contiguous_141 = transpose_664.contiguous();  transpose_664 = None
    view_531 = contiguous_141.view(getitem_597, getitem_598, getitem_599);  contiguous_141 = getitem_597 = getitem_598 = getitem_599 = None
    up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_out, "0")(view_531);  view_531 = None
    up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn1.to_out, "1")(up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0);  up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 = None
    add_234 = up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_1 + up_blocks_1_attentions_1_proj_in;  up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_1 = up_blocks_1_attentions_1_proj_in = None
    up_blocks_1_attentions_1_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").norm2(add_234)
    up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_q(up_blocks_1_attentions_1_transformer_blocks_0_norm2)
    up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_k(up_blocks_1_attentions_1_transformer_blocks_0_norm2)
    up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_v(up_blocks_1_attentions_1_transformer_blocks_0_norm2);  up_blocks_1_attentions_1_transformer_blocks_0_norm2 = None
    size_931 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.size()
    getitem_600 = size_931[0]
    getitem_601 = size_931[1]
    getitem_602 = size_931[2];  size_931 = None
    size_932 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.size(0)
    size_933 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.size(1)
    view_532 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.view(size_932, size_933, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q = size_932 = size_933 = None
    transpose_665 = view_532.transpose(1, 2);  view_532 = None
    size_934 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.size(0)
    size_935 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.size(1)
    view_533 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.view(size_934, size_935, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k = size_934 = size_935 = None
    transpose_666 = view_533.transpose(1, 2);  view_533 = None
    size_936 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.size(0)
    size_937 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.size(1)
    view_534 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.view(size_936, size_937, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v = size_936 = size_937 = None
    transpose_667 = view_534.transpose(1, 2);  view_534 = None
    transpose_668 = transpose_666.transpose(-2, -1);  transpose_666 = None
    matmul_266 = torch.matmul(transpose_665, transpose_668);  transpose_665 = transpose_668 = None
    mul_213 = matmul_266 * 0.125;  matmul_266 = None
    softmax_133 = torch.softmax(mul_213, dim = -1);  mul_213 = None
    matmul_267 = torch.matmul(softmax_133, transpose_667);  softmax_133 = transpose_667 = None
    transpose_669 = matmul_267.transpose(1, 2);  matmul_267 = None
    contiguous_142 = transpose_669.contiguous();  transpose_669 = None
    view_535 = contiguous_142.view(getitem_600, getitem_601, getitem_602);  contiguous_142 = getitem_600 = getitem_601 = getitem_602 = None
    up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_out, "0")(view_535);  view_535 = None
    up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").attn2.to_out, "1")(up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0);  up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 = None
    add_235 = up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_1 + add_234;  up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_1 = add_234 = None
    up_blocks_1_attentions_1_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").norm3(add_235)
    up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").ff.net, "0").proj(up_blocks_1_attentions_1_transformer_blocks_0_norm3);  up_blocks_1_attentions_1_transformer_blocks_0_norm3 = None
    chunk_66 = up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj = None
    getitem_603 = chunk_66[0]
    getitem_604 = chunk_66[1];  chunk_66 = None
    gelu_66 = torch._C._nn.gelu(getitem_604);  getitem_604 = None
    mul_214 = getitem_603 * gelu_66;  getitem_603 = gelu_66 = None
    up_blocks_1_attentions_1_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").ff.net, "1")(mul_214);  mul_214 = None
    up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "0").ff.net, "2")(up_blocks_1_attentions_1_transformer_blocks_0_ff_net_1);  up_blocks_1_attentions_1_transformer_blocks_0_ff_net_1 = None
    add_236 = up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 + add_235;  up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 = add_235 = None
    up_blocks_1_attentions_1_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").norm1(add_236)
    up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_q(up_blocks_1_attentions_1_transformer_blocks_1_norm1)
    up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_k(up_blocks_1_attentions_1_transformer_blocks_1_norm1)
    up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_v(up_blocks_1_attentions_1_transformer_blocks_1_norm1);  up_blocks_1_attentions_1_transformer_blocks_1_norm1 = None
    size_938 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.size()
    getitem_605 = size_938[0]
    getitem_606 = size_938[1]
    getitem_607 = size_938[2];  size_938 = None
    size_939 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.size(0)
    size_940 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.size(1)
    view_536 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q.view(size_939, size_940, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_q = size_939 = size_940 = None
    transpose_670 = view_536.transpose(1, 2);  view_536 = None
    size_941 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k.size(0)
    size_942 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k.size(1)
    view_537 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k.view(size_941, size_942, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_k = size_941 = size_942 = None
    transpose_671 = view_537.transpose(1, 2);  view_537 = None
    size_943 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v.size(0)
    size_944 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v.size(1)
    view_538 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v.view(size_943, size_944, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_v = size_943 = size_944 = None
    transpose_672 = view_538.transpose(1, 2);  view_538 = None
    transpose_673 = transpose_671.transpose(-2, -1);  transpose_671 = None
    matmul_268 = torch.matmul(transpose_670, transpose_673);  transpose_670 = transpose_673 = None
    mul_215 = matmul_268 * 0.125;  matmul_268 = None
    softmax_134 = torch.softmax(mul_215, dim = -1);  mul_215 = None
    matmul_269 = torch.matmul(softmax_134, transpose_672);  softmax_134 = transpose_672 = None
    transpose_674 = matmul_269.transpose(1, 2);  matmul_269 = None
    contiguous_143 = transpose_674.contiguous();  transpose_674 = None
    view_539 = contiguous_143.view(getitem_605, getitem_606, getitem_607);  contiguous_143 = getitem_605 = getitem_606 = getitem_607 = None
    up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_out, "0")(view_539);  view_539 = None
    up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn1.to_out, "1")(up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_0);  up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_0 = None
    add_237 = up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_1 + add_236;  up_blocks_1_attentions_1_transformer_blocks_1_attn1_to_out_1 = add_236 = None
    up_blocks_1_attentions_1_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").norm2(add_237)
    up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_q(up_blocks_1_attentions_1_transformer_blocks_1_norm2)
    up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_k(up_blocks_1_attentions_1_transformer_blocks_1_norm2)
    up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_v(up_blocks_1_attentions_1_transformer_blocks_1_norm2);  up_blocks_1_attentions_1_transformer_blocks_1_norm2 = None
    size_945 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.size()
    getitem_608 = size_945[0]
    getitem_609 = size_945[1]
    getitem_610 = size_945[2];  size_945 = None
    size_946 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.size(0)
    size_947 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.size(1)
    view_540 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q.view(size_946, size_947, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_q = size_946 = size_947 = None
    transpose_675 = view_540.transpose(1, 2);  view_540 = None
    size_948 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k.size(0)
    size_949 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k.size(1)
    view_541 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k.view(size_948, size_949, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_k = size_948 = size_949 = None
    transpose_676 = view_541.transpose(1, 2);  view_541 = None
    size_950 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v.size(0)
    size_951 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v.size(1)
    view_542 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v.view(size_950, size_951, 10, 64);  up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_v = size_950 = size_951 = None
    transpose_677 = view_542.transpose(1, 2);  view_542 = None
    transpose_678 = transpose_676.transpose(-2, -1);  transpose_676 = None
    matmul_270 = torch.matmul(transpose_675, transpose_678);  transpose_675 = transpose_678 = None
    mul_216 = matmul_270 * 0.125;  matmul_270 = None
    softmax_135 = torch.softmax(mul_216, dim = -1);  mul_216 = None
    matmul_271 = torch.matmul(softmax_135, transpose_677);  softmax_135 = transpose_677 = None
    transpose_679 = matmul_271.transpose(1, 2);  matmul_271 = None
    contiguous_144 = transpose_679.contiguous();  transpose_679 = None
    view_543 = contiguous_144.view(getitem_608, getitem_609, getitem_610);  contiguous_144 = getitem_608 = getitem_609 = getitem_610 = None
    up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_out, "0")(view_543);  view_543 = None
    up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").attn2.to_out, "1")(up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_0);  up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_0 = None
    add_238 = up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_1 + add_237;  up_blocks_1_attentions_1_transformer_blocks_1_attn2_to_out_1 = add_237 = None
    up_blocks_1_attentions_1_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").norm3(add_238)
    up_blocks_1_attentions_1_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").ff.net, "0").proj(up_blocks_1_attentions_1_transformer_blocks_1_norm3);  up_blocks_1_attentions_1_transformer_blocks_1_norm3 = None
    chunk_67 = up_blocks_1_attentions_1_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_1_attentions_1_transformer_blocks_1_ff_net_0_proj = None
    getitem_611 = chunk_67[0]
    getitem_612 = chunk_67[1];  chunk_67 = None
    gelu_67 = torch._C._nn.gelu(getitem_612);  getitem_612 = None
    mul_217 = getitem_611 * gelu_67;  getitem_611 = gelu_67 = None
    up_blocks_1_attentions_1_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").ff.net, "1")(mul_217);  mul_217 = None
    up_blocks_1_attentions_1_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "1").transformer_blocks, "1").ff.net, "2")(up_blocks_1_attentions_1_transformer_blocks_1_ff_net_1);  up_blocks_1_attentions_1_transformer_blocks_1_ff_net_1 = None
    add_239 = up_blocks_1_attentions_1_transformer_blocks_1_ff_net_2 + add_238;  up_blocks_1_attentions_1_transformer_blocks_1_ff_net_2 = add_238 = None
    up_blocks_1_attentions_1_proj_out = getattr(getattr(self.up_blocks, "1").attentions, "1").proj_out(add_239);  add_239 = None
    reshape_20 = up_blocks_1_attentions_1_proj_out.reshape(getitem_592, getitem_594, getitem_595, getitem_596);  up_blocks_1_attentions_1_proj_out = getitem_592 = getitem_594 = getitem_595 = getitem_596 = None
    permute_19 = reshape_20.permute(0, 3, 1, 2);  reshape_20 = None
    contiguous_145 = permute_19.contiguous();  permute_19 = None
    add_240 = contiguous_145 + add_233;  contiguous_145 = add_233 = None
    cat_7 = torch.cat([add_240, down_blocks_0_downsamplers_0_conv], dim = 1);  add_240 = down_blocks_0_downsamplers_0_conv = None
    up_blocks_1_resnets_2_norm1 = getattr(getattr(self.up_blocks, "1").resnets, "2").norm1(cat_7)
    up_blocks_1_resnets_2_nonlinearity = getattr(getattr(self.up_blocks, "1").resnets, "2").nonlinearity(up_blocks_1_resnets_2_norm1);  up_blocks_1_resnets_2_norm1 = None
    up_blocks_1_resnets_2_conv1 = getattr(getattr(self.up_blocks, "1").resnets, "2").conv1(up_blocks_1_resnets_2_nonlinearity);  up_blocks_1_resnets_2_nonlinearity = None
    up_blocks_1_resnets_2_nonlinearity_1 = getattr(getattr(self.up_blocks, "1").resnets, "2").nonlinearity(add)
    up_blocks_1_resnets_2_time_emb_proj = getattr(getattr(self.up_blocks, "1").resnets, "2").time_emb_proj(up_blocks_1_resnets_2_nonlinearity_1);  up_blocks_1_resnets_2_nonlinearity_1 = None
    getitem_613 = up_blocks_1_resnets_2_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_1_resnets_2_time_emb_proj = None
    add_241 = up_blocks_1_resnets_2_conv1 + getitem_613;  up_blocks_1_resnets_2_conv1 = getitem_613 = None
    up_blocks_1_resnets_2_norm2 = getattr(getattr(self.up_blocks, "1").resnets, "2").norm2(add_241);  add_241 = None
    up_blocks_1_resnets_2_nonlinearity_2 = getattr(getattr(self.up_blocks, "1").resnets, "2").nonlinearity(up_blocks_1_resnets_2_norm2);  up_blocks_1_resnets_2_norm2 = None
    up_blocks_1_resnets_2_dropout = getattr(getattr(self.up_blocks, "1").resnets, "2").dropout(up_blocks_1_resnets_2_nonlinearity_2);  up_blocks_1_resnets_2_nonlinearity_2 = None
    up_blocks_1_resnets_2_conv2 = getattr(getattr(self.up_blocks, "1").resnets, "2").conv2(up_blocks_1_resnets_2_dropout);  up_blocks_1_resnets_2_dropout = None
    up_blocks_1_resnets_2_conv_shortcut = getattr(getattr(self.up_blocks, "1").resnets, "2").conv_shortcut(cat_7);  cat_7 = None
    add_242 = up_blocks_1_resnets_2_conv_shortcut + up_blocks_1_resnets_2_conv2;  up_blocks_1_resnets_2_conv_shortcut = up_blocks_1_resnets_2_conv2 = None
    getattr_29 = add_242.shape
    getitem_614 = getattr_29[0]
    getitem_615 = getattr_29[1]
    getitem_616 = getattr_29[2]
    getitem_617 = getattr_29[3];  getattr_29 = None
    up_blocks_1_attentions_2_norm = getattr(getattr(self.up_blocks, "1").attentions, "2").norm(add_242)
    getattr_30 = up_blocks_1_attentions_2_norm.shape
    getitem_618 = getattr_30[1];  getattr_30 = None
    permute_20 = up_blocks_1_attentions_2_norm.permute(0, 2, 3, 1);  up_blocks_1_attentions_2_norm = None
    mul_218 = getitem_616 * getitem_617
    reshape_21 = permute_20.reshape(getitem_614, mul_218, getitem_618);  permute_20 = mul_218 = None
    up_blocks_1_attentions_2_proj_in = getattr(getattr(self.up_blocks, "1").attentions, "2").proj_in(reshape_21);  reshape_21 = None
    up_blocks_1_attentions_2_transformer_blocks_0_norm1 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").norm1(up_blocks_1_attentions_2_proj_in)
    up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn1.to_q(up_blocks_1_attentions_2_transformer_blocks_0_norm1)
    up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn1.to_k(up_blocks_1_attentions_2_transformer_blocks_0_norm1)
    up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn1.to_v(up_blocks_1_attentions_2_transformer_blocks_0_norm1);  up_blocks_1_attentions_2_transformer_blocks_0_norm1 = None
    size_952 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.size()
    getitem_619 = size_952[0]
    getitem_620 = size_952[1]
    getitem_621 = size_952[2];  size_952 = None
    size_953 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.size(0)
    size_954 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.size(1)
    view_544 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.view(size_953, size_954, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q = size_953 = size_954 = None
    transpose_680 = view_544.transpose(1, 2);  view_544 = None
    size_955 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.size(0)
    size_956 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.size(1)
    view_545 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.view(size_955, size_956, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k = size_955 = size_956 = None
    transpose_681 = view_545.transpose(1, 2);  view_545 = None
    size_957 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.size(0)
    size_958 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.size(1)
    view_546 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.view(size_957, size_958, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v = size_957 = size_958 = None
    transpose_682 = view_546.transpose(1, 2);  view_546 = None
    transpose_683 = transpose_681.transpose(-2, -1);  transpose_681 = None
    matmul_272 = torch.matmul(transpose_680, transpose_683);  transpose_680 = transpose_683 = None
    mul_219 = matmul_272 * 0.125;  matmul_272 = None
    softmax_136 = torch.softmax(mul_219, dim = -1);  mul_219 = None
    matmul_273 = torch.matmul(softmax_136, transpose_682);  softmax_136 = transpose_682 = None
    transpose_684 = matmul_273.transpose(1, 2);  matmul_273 = None
    contiguous_146 = transpose_684.contiguous();  transpose_684 = None
    view_547 = contiguous_146.view(getitem_619, getitem_620, getitem_621);  contiguous_146 = getitem_619 = getitem_620 = getitem_621 = None
    up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn1.to_out, "0")(view_547);  view_547 = None
    up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn1.to_out, "1")(up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0);  up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0 = None
    add_243 = up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_1 + up_blocks_1_attentions_2_proj_in;  up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_1 = up_blocks_1_attentions_2_proj_in = None
    up_blocks_1_attentions_2_transformer_blocks_0_norm2 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").norm2(add_243)
    up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn2.to_q(up_blocks_1_attentions_2_transformer_blocks_0_norm2)
    up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn2.to_k(up_blocks_1_attentions_2_transformer_blocks_0_norm2)
    up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn2.to_v(up_blocks_1_attentions_2_transformer_blocks_0_norm2);  up_blocks_1_attentions_2_transformer_blocks_0_norm2 = None
    size_959 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.size()
    getitem_622 = size_959[0]
    getitem_623 = size_959[1]
    getitem_624 = size_959[2];  size_959 = None
    size_960 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.size(0)
    size_961 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.size(1)
    view_548 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.view(size_960, size_961, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q = size_960 = size_961 = None
    transpose_685 = view_548.transpose(1, 2);  view_548 = None
    size_962 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.size(0)
    size_963 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.size(1)
    view_549 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.view(size_962, size_963, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k = size_962 = size_963 = None
    transpose_686 = view_549.transpose(1, 2);  view_549 = None
    size_964 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.size(0)
    size_965 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.size(1)
    view_550 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.view(size_964, size_965, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v = size_964 = size_965 = None
    transpose_687 = view_550.transpose(1, 2);  view_550 = None
    transpose_688 = transpose_686.transpose(-2, -1);  transpose_686 = None
    matmul_274 = torch.matmul(transpose_685, transpose_688);  transpose_685 = transpose_688 = None
    mul_220 = matmul_274 * 0.125;  matmul_274 = None
    softmax_137 = torch.softmax(mul_220, dim = -1);  mul_220 = None
    matmul_275 = torch.matmul(softmax_137, transpose_687);  softmax_137 = transpose_687 = None
    transpose_689 = matmul_275.transpose(1, 2);  matmul_275 = None
    contiguous_147 = transpose_689.contiguous();  transpose_689 = None
    view_551 = contiguous_147.view(getitem_622, getitem_623, getitem_624);  contiguous_147 = getitem_622 = getitem_623 = getitem_624 = None
    up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn2.to_out, "0")(view_551);  view_551 = None
    up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").attn2.to_out, "1")(up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0);  up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0 = None
    add_244 = up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_1 + add_243;  up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_1 = add_243 = None
    up_blocks_1_attentions_2_transformer_blocks_0_norm3 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").norm3(add_244)
    up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").ff.net, "0").proj(up_blocks_1_attentions_2_transformer_blocks_0_norm3);  up_blocks_1_attentions_2_transformer_blocks_0_norm3 = None
    chunk_68 = up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj = None
    getitem_625 = chunk_68[0]
    getitem_626 = chunk_68[1];  chunk_68 = None
    gelu_68 = torch._C._nn.gelu(getitem_626);  getitem_626 = None
    mul_221 = getitem_625 * gelu_68;  getitem_625 = gelu_68 = None
    up_blocks_1_attentions_2_transformer_blocks_0_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").ff.net, "1")(mul_221);  mul_221 = None
    up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "0").ff.net, "2")(up_blocks_1_attentions_2_transformer_blocks_0_ff_net_1);  up_blocks_1_attentions_2_transformer_blocks_0_ff_net_1 = None
    add_245 = up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2 + add_244;  up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2 = add_244 = None
    up_blocks_1_attentions_2_transformer_blocks_1_norm1 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").norm1(add_245)
    up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn1.to_q(up_blocks_1_attentions_2_transformer_blocks_1_norm1)
    up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn1.to_k(up_blocks_1_attentions_2_transformer_blocks_1_norm1)
    up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn1.to_v(up_blocks_1_attentions_2_transformer_blocks_1_norm1);  up_blocks_1_attentions_2_transformer_blocks_1_norm1 = None
    size_966 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_q.size()
    getitem_627 = size_966[0]
    getitem_628 = size_966[1]
    getitem_629 = size_966[2];  size_966 = None
    size_967 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_q.size(0)
    size_968 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_q.size(1)
    view_552 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_q.view(size_967, size_968, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_q = size_967 = size_968 = None
    transpose_690 = view_552.transpose(1, 2);  view_552 = None
    size_969 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_k.size(0)
    size_970 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_k.size(1)
    view_553 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_k.view(size_969, size_970, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_k = size_969 = size_970 = None
    transpose_691 = view_553.transpose(1, 2);  view_553 = None
    size_971 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_v.size(0)
    size_972 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_v.size(1)
    view_554 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_v.view(size_971, size_972, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_v = size_971 = size_972 = None
    transpose_692 = view_554.transpose(1, 2);  view_554 = None
    transpose_693 = transpose_691.transpose(-2, -1);  transpose_691 = None
    matmul_276 = torch.matmul(transpose_690, transpose_693);  transpose_690 = transpose_693 = None
    mul_222 = matmul_276 * 0.125;  matmul_276 = None
    softmax_138 = torch.softmax(mul_222, dim = -1);  mul_222 = None
    matmul_277 = torch.matmul(softmax_138, transpose_692);  softmax_138 = transpose_692 = None
    transpose_694 = matmul_277.transpose(1, 2);  matmul_277 = None
    contiguous_148 = transpose_694.contiguous();  transpose_694 = None
    view_555 = contiguous_148.view(getitem_627, getitem_628, getitem_629);  contiguous_148 = getitem_627 = getitem_628 = getitem_629 = None
    up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn1.to_out, "0")(view_555);  view_555 = None
    up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn1.to_out, "1")(up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_out_0);  up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_out_0 = None
    add_246 = up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_out_1 + add_245;  up_blocks_1_attentions_2_transformer_blocks_1_attn1_to_out_1 = add_245 = None
    up_blocks_1_attentions_2_transformer_blocks_1_norm2 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").norm2(add_246)
    up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_q = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn2.to_q(up_blocks_1_attentions_2_transformer_blocks_1_norm2)
    up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_k = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn2.to_k(up_blocks_1_attentions_2_transformer_blocks_1_norm2)
    up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_v = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn2.to_v(up_blocks_1_attentions_2_transformer_blocks_1_norm2);  up_blocks_1_attentions_2_transformer_blocks_1_norm2 = None
    size_973 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_q.size()
    getitem_630 = size_973[0]
    getitem_631 = size_973[1]
    getitem_632 = size_973[2];  size_973 = None
    size_974 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_q.size(0)
    size_975 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_q.size(1)
    view_556 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_q.view(size_974, size_975, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_q = size_974 = size_975 = None
    transpose_695 = view_556.transpose(1, 2);  view_556 = None
    size_976 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_k.size(0)
    size_977 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_k.size(1)
    view_557 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_k.view(size_976, size_977, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_k = size_976 = size_977 = None
    transpose_696 = view_557.transpose(1, 2);  view_557 = None
    size_978 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_v.size(0)
    size_979 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_v.size(1)
    view_558 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_v.view(size_978, size_979, 10, 64);  up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_v = size_978 = size_979 = None
    transpose_697 = view_558.transpose(1, 2);  view_558 = None
    transpose_698 = transpose_696.transpose(-2, -1);  transpose_696 = None
    matmul_278 = torch.matmul(transpose_695, transpose_698);  transpose_695 = transpose_698 = None
    mul_223 = matmul_278 * 0.125;  matmul_278 = None
    softmax_139 = torch.softmax(mul_223, dim = -1);  mul_223 = None
    matmul_279 = torch.matmul(softmax_139, transpose_697);  softmax_139 = transpose_697 = None
    transpose_699 = matmul_279.transpose(1, 2);  matmul_279 = None
    contiguous_149 = transpose_699.contiguous();  transpose_699 = None
    view_559 = contiguous_149.view(getitem_630, getitem_631, getitem_632);  contiguous_149 = getitem_630 = getitem_631 = getitem_632 = None
    up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_out_0 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn2.to_out, "0")(view_559);  view_559 = None
    up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_out_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").attn2.to_out, "1")(up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_out_0);  up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_out_0 = None
    add_247 = up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_out_1 + add_246;  up_blocks_1_attentions_2_transformer_blocks_1_attn2_to_out_1 = add_246 = None
    up_blocks_1_attentions_2_transformer_blocks_1_norm3 = getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").norm3(add_247)
    up_blocks_1_attentions_2_transformer_blocks_1_ff_net_0_proj = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").ff.net, "0").proj(up_blocks_1_attentions_2_transformer_blocks_1_norm3);  up_blocks_1_attentions_2_transformer_blocks_1_norm3 = None
    chunk_69 = up_blocks_1_attentions_2_transformer_blocks_1_ff_net_0_proj.chunk(2, dim = -1);  up_blocks_1_attentions_2_transformer_blocks_1_ff_net_0_proj = None
    getitem_633 = chunk_69[0]
    getitem_634 = chunk_69[1];  chunk_69 = None
    gelu_69 = torch._C._nn.gelu(getitem_634);  getitem_634 = None
    mul_224 = getitem_633 * gelu_69;  getitem_633 = gelu_69 = None
    up_blocks_1_attentions_2_transformer_blocks_1_ff_net_1 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").ff.net, "1")(mul_224);  mul_224 = None
    up_blocks_1_attentions_2_transformer_blocks_1_ff_net_2 = getattr(getattr(getattr(getattr(self.up_blocks, "1").attentions, "2").transformer_blocks, "1").ff.net, "2")(up_blocks_1_attentions_2_transformer_blocks_1_ff_net_1);  up_blocks_1_attentions_2_transformer_blocks_1_ff_net_1 = None
    add_248 = up_blocks_1_attentions_2_transformer_blocks_1_ff_net_2 + add_247;  up_blocks_1_attentions_2_transformer_blocks_1_ff_net_2 = add_247 = None
    up_blocks_1_attentions_2_proj_out = getattr(getattr(self.up_blocks, "1").attentions, "2").proj_out(add_248);  add_248 = None
    reshape_22 = up_blocks_1_attentions_2_proj_out.reshape(getitem_614, getitem_616, getitem_617, getitem_618);  up_blocks_1_attentions_2_proj_out = getitem_614 = getitem_616 = getitem_617 = getitem_618 = None
    permute_21 = reshape_22.permute(0, 3, 1, 2);  reshape_22 = None
    contiguous_150 = permute_21.contiguous();  permute_21 = None
    add_249 = contiguous_150 + add_242;  contiguous_150 = add_242 = None
    interpolate_1 = torch.nn.functional.interpolate(add_249, size = None, scale_factor = 2.0, mode = 'nearest', align_corners = None, recompute_scale_factor = None, antialias = False);  add_249 = None
    up_blocks_1_upsamplers_0_conv = getattr(getattr(self.up_blocks, "1").upsamplers, "0").conv(interpolate_1);  interpolate_1 = None
    cat_8 = torch.cat([up_blocks_1_upsamplers_0_conv, add_4], dim = 1);  up_blocks_1_upsamplers_0_conv = add_4 = None
    up_blocks_2_resnets_0_norm1 = getattr(getattr(self.up_blocks, "2").resnets, "0").norm1(cat_8)
    up_blocks_2_resnets_0_nonlinearity = getattr(getattr(self.up_blocks, "2").resnets, "0").nonlinearity(up_blocks_2_resnets_0_norm1);  up_blocks_2_resnets_0_norm1 = None
    up_blocks_2_resnets_0_conv1 = getattr(getattr(self.up_blocks, "2").resnets, "0").conv1(up_blocks_2_resnets_0_nonlinearity);  up_blocks_2_resnets_0_nonlinearity = None
    up_blocks_2_resnets_0_nonlinearity_1 = getattr(getattr(self.up_blocks, "2").resnets, "0").nonlinearity(add)
    up_blocks_2_resnets_0_time_emb_proj = getattr(getattr(self.up_blocks, "2").resnets, "0").time_emb_proj(up_blocks_2_resnets_0_nonlinearity_1);  up_blocks_2_resnets_0_nonlinearity_1 = None
    getitem_635 = up_blocks_2_resnets_0_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_2_resnets_0_time_emb_proj = None
    add_250 = up_blocks_2_resnets_0_conv1 + getitem_635;  up_blocks_2_resnets_0_conv1 = getitem_635 = None
    up_blocks_2_resnets_0_norm2 = getattr(getattr(self.up_blocks, "2").resnets, "0").norm2(add_250);  add_250 = None
    up_blocks_2_resnets_0_nonlinearity_2 = getattr(getattr(self.up_blocks, "2").resnets, "0").nonlinearity(up_blocks_2_resnets_0_norm2);  up_blocks_2_resnets_0_norm2 = None
    up_blocks_2_resnets_0_dropout = getattr(getattr(self.up_blocks, "2").resnets, "0").dropout(up_blocks_2_resnets_0_nonlinearity_2);  up_blocks_2_resnets_0_nonlinearity_2 = None
    up_blocks_2_resnets_0_conv2 = getattr(getattr(self.up_blocks, "2").resnets, "0").conv2(up_blocks_2_resnets_0_dropout);  up_blocks_2_resnets_0_dropout = None
    up_blocks_2_resnets_0_conv_shortcut = getattr(getattr(self.up_blocks, "2").resnets, "0").conv_shortcut(cat_8);  cat_8 = None
    add_251 = up_blocks_2_resnets_0_conv_shortcut + up_blocks_2_resnets_0_conv2;  up_blocks_2_resnets_0_conv_shortcut = up_blocks_2_resnets_0_conv2 = None
    cat_9 = torch.cat([add_251, add_2], dim = 1);  add_251 = add_2 = None
    up_blocks_2_resnets_1_norm1 = getattr(getattr(self.up_blocks, "2").resnets, "1").norm1(cat_9)
    up_blocks_2_resnets_1_nonlinearity = getattr(getattr(self.up_blocks, "2").resnets, "1").nonlinearity(up_blocks_2_resnets_1_norm1);  up_blocks_2_resnets_1_norm1 = None
    up_blocks_2_resnets_1_conv1 = getattr(getattr(self.up_blocks, "2").resnets, "1").conv1(up_blocks_2_resnets_1_nonlinearity);  up_blocks_2_resnets_1_nonlinearity = None
    up_blocks_2_resnets_1_nonlinearity_1 = getattr(getattr(self.up_blocks, "2").resnets, "1").nonlinearity(add)
    up_blocks_2_resnets_1_time_emb_proj = getattr(getattr(self.up_blocks, "2").resnets, "1").time_emb_proj(up_blocks_2_resnets_1_nonlinearity_1);  up_blocks_2_resnets_1_nonlinearity_1 = None
    getitem_636 = up_blocks_2_resnets_1_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_2_resnets_1_time_emb_proj = None
    add_252 = up_blocks_2_resnets_1_conv1 + getitem_636;  up_blocks_2_resnets_1_conv1 = getitem_636 = None
    up_blocks_2_resnets_1_norm2 = getattr(getattr(self.up_blocks, "2").resnets, "1").norm2(add_252);  add_252 = None
    up_blocks_2_resnets_1_nonlinearity_2 = getattr(getattr(self.up_blocks, "2").resnets, "1").nonlinearity(up_blocks_2_resnets_1_norm2);  up_blocks_2_resnets_1_norm2 = None
    up_blocks_2_resnets_1_dropout = getattr(getattr(self.up_blocks, "2").resnets, "1").dropout(up_blocks_2_resnets_1_nonlinearity_2);  up_blocks_2_resnets_1_nonlinearity_2 = None
    up_blocks_2_resnets_1_conv2 = getattr(getattr(self.up_blocks, "2").resnets, "1").conv2(up_blocks_2_resnets_1_dropout);  up_blocks_2_resnets_1_dropout = None
    up_blocks_2_resnets_1_conv_shortcut = getattr(getattr(self.up_blocks, "2").resnets, "1").conv_shortcut(cat_9);  cat_9 = None
    add_253 = up_blocks_2_resnets_1_conv_shortcut + up_blocks_2_resnets_1_conv2;  up_blocks_2_resnets_1_conv_shortcut = up_blocks_2_resnets_1_conv2 = None
    cat_10 = torch.cat([add_253, conv_in], dim = 1);  add_253 = conv_in = None
    up_blocks_2_resnets_2_norm1 = getattr(getattr(self.up_blocks, "2").resnets, "2").norm1(cat_10)
    up_blocks_2_resnets_2_nonlinearity = getattr(getattr(self.up_blocks, "2").resnets, "2").nonlinearity(up_blocks_2_resnets_2_norm1);  up_blocks_2_resnets_2_norm1 = None
    up_blocks_2_resnets_2_conv1 = getattr(getattr(self.up_blocks, "2").resnets, "2").conv1(up_blocks_2_resnets_2_nonlinearity);  up_blocks_2_resnets_2_nonlinearity = None
    up_blocks_2_resnets_2_nonlinearity_1 = getattr(getattr(self.up_blocks, "2").resnets, "2").nonlinearity(add);  add = None
    up_blocks_2_resnets_2_time_emb_proj = getattr(getattr(self.up_blocks, "2").resnets, "2").time_emb_proj(up_blocks_2_resnets_2_nonlinearity_1);  up_blocks_2_resnets_2_nonlinearity_1 = None
    getitem_637 = up_blocks_2_resnets_2_time_emb_proj[(slice(None, None, None), slice(None, None, None), None, None)];  up_blocks_2_resnets_2_time_emb_proj = None
    add_254 = up_blocks_2_resnets_2_conv1 + getitem_637;  up_blocks_2_resnets_2_conv1 = getitem_637 = None
    up_blocks_2_resnets_2_norm2 = getattr(getattr(self.up_blocks, "2").resnets, "2").norm2(add_254);  add_254 = None
    up_blocks_2_resnets_2_nonlinearity_2 = getattr(getattr(self.up_blocks, "2").resnets, "2").nonlinearity(up_blocks_2_resnets_2_norm2);  up_blocks_2_resnets_2_norm2 = None
    up_blocks_2_resnets_2_dropout = getattr(getattr(self.up_blocks, "2").resnets, "2").dropout(up_blocks_2_resnets_2_nonlinearity_2);  up_blocks_2_resnets_2_nonlinearity_2 = None
    up_blocks_2_resnets_2_conv2 = getattr(getattr(self.up_blocks, "2").resnets, "2").conv2(up_blocks_2_resnets_2_dropout);  up_blocks_2_resnets_2_dropout = None
    up_blocks_2_resnets_2_conv_shortcut = getattr(getattr(self.up_blocks, "2").resnets, "2").conv_shortcut(cat_10);  cat_10 = None
    add_255 = up_blocks_2_resnets_2_conv_shortcut + up_blocks_2_resnets_2_conv2;  up_blocks_2_resnets_2_conv_shortcut = up_blocks_2_resnets_2_conv2 = None
    conv_norm_out = self.conv_norm_out(add_255);  add_255 = None
    conv_act = self.conv_act(conv_norm_out);  conv_norm_out = None
    conv_out = self.conv_out(conv_act);  conv_act = None
    return [conv_out]
    
# To see more debug info, please use `graph_module.print_readable()`
Module(
  (linear_1): Linear(in_features=320, out_features=1280, bias=True)
  (act): SiLU()
  (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
)
Linear(in_features=320, out_features=1280, bias=True)
SiLU()
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (linear_1): Linear(in_features=2816, out_features=1280, bias=True)
  (act): SiLU()
  (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
)
Linear(in_features=2816, out_features=1280, bias=True)
SiLU()
Linear(in_features=1280, out_features=1280, bias=True)
Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (0): Module(
    (resnets): Module(
      (0): Module(
        (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): Module(
        (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (downsamplers): Module(
      (0): Module(
        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  (1): Module(
    (resnets): Module(
      (0): Module(
        (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Module(
        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (attentions): Module(
      (0): Module(
        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=640, out_features=640, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=640, out_features=640, bias=True)
      )
      (1): Module(
        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=640, out_features=640, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=640, out_features=640, bias=True)
      )
    )
    (downsamplers): Module(
      (0): Module(
        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  (2): Module(
    (resnets): Module(
      (0): Module(
        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Module(
        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (attentions): Module(
      (0): Module(
        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (2): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (3): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (4): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (5): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (6): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (7): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (8): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (9): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (1): Module(
        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (2): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (3): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (4): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (5): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (6): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (7): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (8): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (9): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
      )
    )
  )
)
Module(
  (resnets): Module(
    (0): Module(
      (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): Module(
      (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (downsamplers): Module(
    (0): Module(
      (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
)
Module(
  (0): Module(
    (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (1): Module(
    (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
Module(
  (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
GroupNorm(32, 320, eps=1e-05, affine=True)
SiLU()
Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=320, bias=True)
GroupNorm(32, 320, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
GroupNorm(32, 320, eps=1e-05, affine=True)
SiLU()
Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=320, bias=True)
GroupNorm(32, 320, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (0): Module(
    (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
)
Module(
  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
Module(
  (resnets): Module(
    (0): Module(
      (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): Module(
      (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (attentions): Module(
    (0): Module(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (1): Module(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (downsamplers): Module(
    (0): Module(
      (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
)
Module(
  (0): Module(
    (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
  )
  (1): Module(
    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
Module(
  (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 320, eps=1e-05, affine=True)
SiLU()
Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=640, bias=True)
GroupNorm(32, 640, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
GroupNorm(32, 640, eps=1e-05, affine=True)
SiLU()
Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=640, bias=True)
GroupNorm(32, 640, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (0): Module(
    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=640, out_features=640, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=640, out_features=640, bias=True)
  )
  (1): Module(
    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=640, out_features=640, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=640, out_features=640, bias=True)
  )
)
Module(
  (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=640, out_features=640, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=640, out_features=640, bias=True)
)
GroupNorm(32, 640, eps=1e-06, affine=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=640, out_features=640, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=640, out_features=640, bias=True)
)
GroupNorm(32, 640, eps=1e-06, affine=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (0): Module(
    (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
)
Module(
  (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
Module(
  (resnets): Module(
    (0): Module(
      (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): Module(
      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (attentions): Module(
    (0): Module(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (2): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (3): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (4): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (5): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (6): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (7): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (8): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (9): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (1): Module(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (2): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (3): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (4): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (5): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (6): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (7): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (8): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (9): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
)
Module(
  (0): Module(
    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
  )
  (1): Module(
    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
Module(
  (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 640, eps=1e-05, affine=True)
SiLU()
Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=1280, bias=True)
GroupNorm(32, 1280, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
GroupNorm(32, 1280, eps=1e-05, affine=True)
SiLU()
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=1280, bias=True)
GroupNorm(32, 1280, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (0): Module(
    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (2): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (3): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (4): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (5): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (6): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (7): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (8): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (9): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (1): Module(
    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (2): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (3): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (4): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (5): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (6): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (7): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (8): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (9): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
  )
)
Module(
  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (2): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (3): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (4): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (5): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (6): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (7): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (8): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (9): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
)
GroupNorm(32, 1280, eps=1e-06, affine=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (2): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (3): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (4): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (5): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (6): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (7): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (8): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (9): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (2): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (3): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (4): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (5): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (6): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (7): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (8): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (9): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
)
GroupNorm(32, 1280, eps=1e-06, affine=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (2): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (3): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (4): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (5): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (6): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (7): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (8): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (9): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (resnets): Module(
    (0): Module(
      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): Module(
      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (attentions): Module(
    (0): Module(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (2): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (3): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (4): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (5): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (6): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (7): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (8): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (9): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
)
Module(
  (0): Module(
    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (1): Module(
    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
Module(
  (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
GroupNorm(32, 1280, eps=1e-05, affine=True)
SiLU()
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=1280, bias=True)
GroupNorm(32, 1280, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
GroupNorm(32, 1280, eps=1e-05, affine=True)
SiLU()
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=1280, bias=True)
GroupNorm(32, 1280, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (0): Module(
    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (2): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (3): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (4): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (5): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (6): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (7): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (8): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (9): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
  )
)
Module(
  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (2): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (3): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (4): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (5): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (6): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (7): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (8): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (9): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
)
GroupNorm(32, 1280, eps=1e-06, affine=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (2): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (3): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (4): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (5): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (6): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (7): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (8): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (9): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (resnets): Module(
      (0): Module(
        (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Module(
        (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): Module(
        (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (attentions): Module(
      (0): Module(
        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (2): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (3): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (4): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (5): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (6): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (7): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (8): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (9): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (1): Module(
        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (2): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (3): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (4): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (5): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (6): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (7): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (8): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (9): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (2): Module(
        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (2): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (3): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (4): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (5): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (6): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (7): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (8): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
          (9): Module(
            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=1280, out_features=1280, bias=False)
              (to_v): Linear(in_features=1280, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=1280, out_features=1280, bias=False)
              (to_k): Linear(in_features=2048, out_features=1280, bias=False)
              (to_v): Linear(in_features=2048, out_features=1280, bias=False)
              (to_out): Module(
                (0): Linear(in_features=1280, out_features=1280, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=1280, out_features=10240, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=5120, out_features=1280, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
      )
    )
    (upsamplers): Module(
      (0): Module(
        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (1): Module(
    (resnets): Module(
      (0): Module(
        (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Module(
        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): Module(
        (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (attentions): Module(
      (0): Module(
        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=640, out_features=640, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=640, out_features=640, bias=True)
      )
      (1): Module(
        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=640, out_features=640, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=640, out_features=640, bias=True)
      )
      (2): Module(
        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
        (proj_in): Linear(in_features=640, out_features=640, bias=True)
        (transformer_blocks): Module(
          (0): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
          (1): Module(
            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn1): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=640, out_features=640, bias=False)
              (to_v): Linear(in_features=640, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (attn2): Module(
              (to_q): Linear(in_features=640, out_features=640, bias=False)
              (to_k): Linear(in_features=2048, out_features=640, bias=False)
              (to_v): Linear(in_features=2048, out_features=640, bias=False)
              (to_out): Module(
                (0): Linear(in_features=640, out_features=640, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
            (ff): Module(
              (net): Module(
                (0): Module(
                  (proj): Linear(in_features=640, out_features=5120, bias=True)
                )
                (1): Dropout(p=0.0, inplace=False)
                (2): Linear(in_features=2560, out_features=640, bias=True)
              )
            )
          )
        )
        (proj_out): Linear(in_features=640, out_features=640, bias=True)
      )
    )
    (upsamplers): Module(
      (0): Module(
        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (2): Module(
    (resnets): Module(
      (0): Module(
        (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Module(
        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): Module(
        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
        (nonlinearity): SiLU()
        (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
Module(
  (resnets): Module(
    (0): Module(
      (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): Module(
      (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): Module(
      (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (attentions): Module(
    (0): Module(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (2): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (3): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (4): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (5): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (6): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (7): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (8): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (9): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (1): Module(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (2): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (3): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (4): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (5): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (6): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (7): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (8): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (9): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (2): Module(
      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (2): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (3): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (4): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (5): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (6): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (7): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (8): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (9): Module(
          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=1280, out_features=1280, bias=False)
            (to_v): Linear(in_features=1280, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=1280, out_features=1280, bias=False)
            (to_k): Linear(in_features=2048, out_features=1280, bias=False)
            (to_v): Linear(in_features=2048, out_features=1280, bias=False)
            (to_out): Module(
              (0): Linear(in_features=1280, out_features=1280, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=1280, out_features=10240, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
    )
  )
  (upsamplers): Module(
    (0): Module(
      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
Module(
  (0): Module(
    (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
  )
  (1): Module(
    (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
  )
  (2): Module(
    (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
  )
)
Module(
  (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 2560, eps=1e-05, affine=True)
SiLU()
Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=1280, bias=True)
GroupNorm(32, 1280, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 2560, eps=1e-05, affine=True)
SiLU()
Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=1280, bias=True)
GroupNorm(32, 1280, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 1920, eps=1e-05, affine=True)
SiLU()
Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=1280, bias=True)
GroupNorm(32, 1280, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
Module(
  (0): Module(
    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (2): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (3): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (4): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (5): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (6): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (7): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (8): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (9): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (1): Module(
    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (2): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (3): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (4): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (5): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (6): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (7): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (8): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (9): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (2): Module(
    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (2): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (3): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (4): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (5): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (6): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (7): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (8): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (9): Module(
        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=1280, out_features=1280, bias=False)
          (to_v): Linear(in_features=1280, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=1280, out_features=1280, bias=False)
          (to_k): Linear(in_features=2048, out_features=1280, bias=False)
          (to_v): Linear(in_features=2048, out_features=1280, bias=False)
          (to_out): Module(
            (0): Linear(in_features=1280, out_features=1280, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=1280, out_features=10240, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
  )
)
Module(
  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (2): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (3): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (4): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (5): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (6): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (7): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (8): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (9): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
)
GroupNorm(32, 1280, eps=1e-06, affine=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (2): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (3): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (4): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (5): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (6): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (7): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (8): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (9): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (2): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (3): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (4): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (5): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (6): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (7): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (8): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (9): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
)
GroupNorm(32, 1280, eps=1e-06, affine=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (2): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (3): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (4): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (5): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (6): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (7): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (8): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (9): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=1280, out_features=1280, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (2): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (3): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (4): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (5): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (6): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (7): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (8): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (9): Module(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
        (to_k): Linear(in_features=2048, out_features=1280, bias=False)
        (to_v): Linear(in_features=2048, out_features=1280, bias=False)
        (to_out): Module(
          (0): Linear(in_features=1280, out_features=1280, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=1280, out_features=10240, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=1280, out_features=1280, bias=True)
)
GroupNorm(32, 1280, eps=1e-06, affine=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (2): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (3): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (4): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (5): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (6): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (7): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (8): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
  (9): Module(
    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
      (to_k): Linear(in_features=2048, out_features=1280, bias=False)
      (to_v): Linear(in_features=2048, out_features=1280, bias=False)
      (to_out): Module(
        (0): Linear(in_features=1280, out_features=1280, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=1280, out_features=10240, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=5120, out_features=1280, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Module(
  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=1280, out_features=1280, bias=False)
    (to_v): Linear(in_features=1280, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=1280, out_features=1280, bias=False)
    (to_k): Linear(in_features=2048, out_features=1280, bias=False)
    (to_v): Linear(in_features=2048, out_features=1280, bias=False)
    (to_out): Module(
      (0): Linear(in_features=1280, out_features=1280, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=1280, out_features=10240, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=5120, out_features=1280, bias=True)
    )
  )
)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=1280, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=2048, out_features=1280, bias=False)
  (to_v): Linear(in_features=2048, out_features=1280, bias=False)
  (to_out): Module(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=1280, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Linear(in_features=2048, out_features=1280, bias=False)
Module(
  (0): Linear(in_features=1280, out_features=1280, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=1280, out_features=1280, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=1280, out_features=10240, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=5120, out_features=1280, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=1280, out_features=10240, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=5120, out_features=1280, bias=True)
)
Module(
  (proj): Linear(in_features=1280, out_features=10240, bias=True)
)
Linear(in_features=1280, out_features=10240, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=5120, out_features=1280, bias=True)
Linear(in_features=1280, out_features=1280, bias=True)
Module(
  (0): Module(
    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
Module(
  (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (resnets): Module(
    (0): Module(
      (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): Module(
      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): Module(
      (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (attentions): Module(
    (0): Module(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (1): Module(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
    (2): Module(
      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
      (proj_in): Linear(in_features=640, out_features=640, bias=True)
      (transformer_blocks): Module(
        (0): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
        (1): Module(
          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn1): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=640, out_features=640, bias=False)
            (to_v): Linear(in_features=640, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (attn2): Module(
            (to_q): Linear(in_features=640, out_features=640, bias=False)
            (to_k): Linear(in_features=2048, out_features=640, bias=False)
            (to_v): Linear(in_features=2048, out_features=640, bias=False)
            (to_out): Module(
              (0): Linear(in_features=640, out_features=640, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
          (ff): Module(
            (net): Module(
              (0): Module(
                (proj): Linear(in_features=640, out_features=5120, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): Linear(in_features=2560, out_features=640, bias=True)
            )
          )
        )
      )
      (proj_out): Linear(in_features=640, out_features=640, bias=True)
    )
  )
  (upsamplers): Module(
    (0): Module(
      (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
Module(
  (0): Module(
    (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
  )
  (1): Module(
    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
  )
  (2): Module(
    (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
  )
)
Module(
  (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 1920, eps=1e-05, affine=True)
SiLU()
Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=640, bias=True)
GroupNorm(32, 640, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 1280, eps=1e-05, affine=True)
SiLU()
Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=640, bias=True)
GroupNorm(32, 640, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 960, eps=1e-05, affine=True)
SiLU()
Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=640, bias=True)
GroupNorm(32, 640, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
Module(
  (0): Module(
    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=640, out_features=640, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=640, out_features=640, bias=True)
  )
  (1): Module(
    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=640, out_features=640, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=640, out_features=640, bias=True)
  )
  (2): Module(
    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
    (proj_in): Linear(in_features=640, out_features=640, bias=True)
    (transformer_blocks): Module(
      (0): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
      (1): Module(
        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn1): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=640, out_features=640, bias=False)
          (to_v): Linear(in_features=640, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (attn2): Module(
          (to_q): Linear(in_features=640, out_features=640, bias=False)
          (to_k): Linear(in_features=2048, out_features=640, bias=False)
          (to_v): Linear(in_features=2048, out_features=640, bias=False)
          (to_out): Module(
            (0): Linear(in_features=640, out_features=640, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
        (ff): Module(
          (net): Module(
            (0): Module(
              (proj): Linear(in_features=640, out_features=5120, bias=True)
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=2560, out_features=640, bias=True)
          )
        )
      )
    )
    (proj_out): Linear(in_features=640, out_features=640, bias=True)
  )
)
Module(
  (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=640, out_features=640, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=640, out_features=640, bias=True)
)
GroupNorm(32, 640, eps=1e-06, affine=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=640, out_features=640, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=640, out_features=640, bias=True)
)
GroupNorm(32, 640, eps=1e-06, affine=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
  (proj_in): Linear(in_features=640, out_features=640, bias=True)
  (transformer_blocks): Module(
    (0): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
    (1): Module(
      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn1): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=640, out_features=640, bias=False)
        (to_v): Linear(in_features=640, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (attn2): Module(
        (to_q): Linear(in_features=640, out_features=640, bias=False)
        (to_k): Linear(in_features=2048, out_features=640, bias=False)
        (to_v): Linear(in_features=2048, out_features=640, bias=False)
        (to_out): Module(
          (0): Linear(in_features=640, out_features=640, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
      (ff): Module(
        (net): Module(
          (0): Module(
            (proj): Linear(in_features=640, out_features=5120, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=2560, out_features=640, bias=True)
        )
      )
    )
  )
  (proj_out): Linear(in_features=640, out_features=640, bias=True)
)
GroupNorm(32, 640, eps=1e-06, affine=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (0): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
  (1): Module(
    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn1): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=640, out_features=640, bias=False)
      (to_v): Linear(in_features=640, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (attn2): Module(
      (to_q): Linear(in_features=640, out_features=640, bias=False)
      (to_k): Linear(in_features=2048, out_features=640, bias=False)
      (to_v): Linear(in_features=2048, out_features=640, bias=False)
      (to_out): Module(
        (0): Linear(in_features=640, out_features=640, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
    (ff): Module(
      (net): Module(
        (0): Module(
          (proj): Linear(in_features=640, out_features=5120, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=2560, out_features=640, bias=True)
      )
    )
  )
)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Module(
  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn1): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=640, out_features=640, bias=False)
    (to_v): Linear(in_features=640, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (attn2): Module(
    (to_q): Linear(in_features=640, out_features=640, bias=False)
    (to_k): Linear(in_features=2048, out_features=640, bias=False)
    (to_v): Linear(in_features=2048, out_features=640, bias=False)
    (to_out): Module(
      (0): Linear(in_features=640, out_features=640, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
  (ff): Module(
    (net): Module(
      (0): Module(
        (proj): Linear(in_features=640, out_features=5120, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=2560, out_features=640, bias=True)
    )
  )
)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=640, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=2048, out_features=640, bias=False)
  (to_v): Linear(in_features=2048, out_features=640, bias=False)
  (to_out): Module(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
Linear(in_features=640, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Linear(in_features=2048, out_features=640, bias=False)
Module(
  (0): Linear(in_features=640, out_features=640, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
Linear(in_features=640, out_features=640, bias=True)
Dropout(p=0.0, inplace=False)
LayerNorm((640,), eps=1e-05, elementwise_affine=True)
Module(
  (net): Module(
    (0): Module(
      (proj): Linear(in_features=640, out_features=5120, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=2560, out_features=640, bias=True)
  )
)
Module(
  (0): Module(
    (proj): Linear(in_features=640, out_features=5120, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=2560, out_features=640, bias=True)
)
Module(
  (proj): Linear(in_features=640, out_features=5120, bias=True)
)
Linear(in_features=640, out_features=5120, bias=True)
Dropout(p=0.0, inplace=False)
Linear(in_features=2560, out_features=640, bias=True)
Linear(in_features=640, out_features=640, bias=True)
Module(
  (0): Module(
    (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
Module(
  (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Module(
  (resnets): Module(
    (0): Module(
      (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): Module(
      (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): Module(
      (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
      (nonlinearity): SiLU()
      (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Module(
  (0): Module(
    (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
  )
  (1): Module(
    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
  )
  (2): Module(
    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
    (nonlinearity): SiLU()
    (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
    (dropout): Dropout(p=0.0, inplace=False)
    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
  )
)
Module(
  (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 960, eps=1e-05, affine=True)
SiLU()
Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=320, bias=True)
GroupNorm(32, 320, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 640, eps=1e-05, affine=True)
SiLU()
Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=320, bias=True)
GroupNorm(32, 320, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
Module(
  (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
  (nonlinearity): SiLU()
  (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
)
GroupNorm(32, 640, eps=1e-05, affine=True)
SiLU()
Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Linear(in_features=1280, out_features=320, bias=True)
GroupNorm(32, 320, eps=1e-05, affine=True)
Dropout(p=0.0, inplace=False)
Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
GroupNorm(32, 320, eps=1e-05, affine=True)
SiLU()
Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
